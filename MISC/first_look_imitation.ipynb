{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Wf5dmRrrBvMJ","otF25OWCBy8i","yfbMSp7iW73z","8qgcKIbY8TBh","GoWnkl0LzJLU","O6qQ2sRIYioW","efe4TOVv5vwj"],"gpuType":"T4","authorship_tag":"ABX9TyNROtWAJrGjVpK0EVnCwOW0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8c5f9758f2d24d7b892ae25b32bfb229":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9c0d6633be94b0dafa8e1f27850f157","IPY_MODEL_465e014d41084c61827a769efce628ed","IPY_MODEL_bcf811c9829d41d79b7887d7d243bf16"],"layout":"IPY_MODEL_b05f9a9d5c6a4914bcd67118b2631b5d"}},"a9c0d6633be94b0dafa8e1f27850f157":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c4c25b219e346eab995c67a8c614e67","placeholder":"​","style":"IPY_MODEL_9879171c171f4fc6a90d8db6d8b1a27a","value":"ppo-CartPole-v1.zip: 100%"}},"465e014d41084c61827a769efce628ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4b4b155a70846c78747fc9bbf02390a","max":139331,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d36dfd26a26411784a68860a3843469","value":139331}},"bcf811c9829d41d79b7887d7d243bf16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b242638ffdef49bb9de336c7342924c6","placeholder":"​","style":"IPY_MODEL_eedbfc0c0e544160950e35829e5003cf","value":" 139k/139k [00:00&lt;00:00, 1.93MB/s]"}},"b05f9a9d5c6a4914bcd67118b2631b5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c4c25b219e346eab995c67a8c614e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9879171c171f4fc6a90d8db6d8b1a27a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4b4b155a70846c78747fc9bbf02390a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d36dfd26a26411784a68860a3843469":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b242638ffdef49bb9de336c7342924c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eedbfc0c0e544160950e35829e5003cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d4a0870d1d147b48120ed98f13ee9cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0013b516b31e489facf05d7b3fbe5f28","IPY_MODEL_d75cd4fbd67d4d8daa5a0ac66e0119d5","IPY_MODEL_3a5fc6693eae4936a824e85ffb324c6e"],"layout":"IPY_MODEL_92592cc827bb4ef782cfb57f1b4708e1"}},"0013b516b31e489facf05d7b3fbe5f28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8219e279d3624abcb2ab94b76997eb67","placeholder":"​","style":"IPY_MODEL_a7913eaec1994021bbdc507a2ce6544d","value":""}},"d75cd4fbd67d4d8daa5a0ac66e0119d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f109c148daf4d9aba198ec99021dbf3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a5b76f3e5d64581a5d3537683784701","value":0}},"3a5fc6693eae4936a824e85ffb324c6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c990af05e706431280ac8f8e3d86c7ba","placeholder":"​","style":"IPY_MODEL_be139469c4224fa4977faab55de0c11d","value":" 0/0 [00:00&lt;?, ?it/s]"}},"92592cc827bb4ef782cfb57f1b4708e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8219e279d3624abcb2ab94b76997eb67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7913eaec1994021bbdc507a2ce6544d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f109c148daf4d9aba198ec99021dbf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9a5b76f3e5d64581a5d3537683784701":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c990af05e706431280ac8f8e3d86c7ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be139469c4224fa4977faab55de0c11d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3820a9432ff4455cb352e1473e388c61":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3caffd7dcbd4d4daaa75ff936435d83","IPY_MODEL_06fb7e28ba2746d3940a94368fc903cc","IPY_MODEL_c06505ff0ec74e8390a941724e49a8bd"],"layout":"IPY_MODEL_52a6721e56b44183b6fe6bdc504e53b3"}},"e3caffd7dcbd4d4daaa75ff936435d83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d28436dc5d4e41ca92253e52255f1b86","placeholder":"​","style":"IPY_MODEL_4bc4c0ab5a9b4072ae7a1d663bccabaa","value":"ppo-CartPole-v1.zip: 100%"}},"06fb7e28ba2746d3940a94368fc903cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da27747dc4e4b2ba79e0cf20309c8d7","max":139331,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a28c09f4dbda4c1aadf4dcbeefd70f12","value":139331}},"c06505ff0ec74e8390a941724e49a8bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed14d805a0464773bee83dc2c5af0d92","placeholder":"​","style":"IPY_MODEL_24dc4bdbf5a64b55b36936e92b3795ac","value":" 139k/139k [00:00&lt;00:00, 1.88MB/s]"}},"52a6721e56b44183b6fe6bdc504e53b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d28436dc5d4e41ca92253e52255f1b86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc4c0ab5a9b4072ae7a1d663bccabaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4da27747dc4e4b2ba79e0cf20309c8d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a28c09f4dbda4c1aadf4dcbeefd70f12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed14d805a0464773bee83dc2c5af0d92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24dc4bdbf5a64b55b36936e92b3795ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b069967c4dd248ef97924a9b03a31126":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aae235807f0e4cb6be811c64d288525b","IPY_MODEL_d083c4339a354ab689722b4f2c683191","IPY_MODEL_6a2211150d7c430885d46d7c2efc6ef5"],"layout":"IPY_MODEL_8cd960b4c56540fa943c62877fd8ad8c"}},"aae235807f0e4cb6be811c64d288525b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_958d3f10b3f140ee92a41b3f98971282","placeholder":"​","style":"IPY_MODEL_5798c021dad74e52b994cc71876beda3","value":""}},"d083c4339a354ab689722b4f2c683191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae7d9a67659f4750acafa5a537dc8193","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0659c4a881c4ad2bb078f1ee9297cee","value":0}},"6a2211150d7c430885d46d7c2efc6ef5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c77ddbb886e14d1c99bd4236964415a8","placeholder":"​","style":"IPY_MODEL_518c61cc94a34d1c9325de643a29e1ba","value":" 0/0 [00:00&lt;?, ?it/s]"}},"8cd960b4c56540fa943c62877fd8ad8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"958d3f10b3f140ee92a41b3f98971282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5798c021dad74e52b994cc71876beda3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae7d9a67659f4750acafa5a537dc8193":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d0659c4a881c4ad2bb078f1ee9297cee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c77ddbb886e14d1c99bd4236964415a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"518c61cc94a34d1c9325de643a29e1ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d6cfca918504a29888b312bd9f17ca4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f343274531264e559776a4f6f412466a","IPY_MODEL_516b7de559774808a8850a1e5ede1c06","IPY_MODEL_84a42027c1724615940cdc4504a50183"],"layout":"IPY_MODEL_e044f0fe858f4622b617dd21e750b735"}},"f343274531264e559776a4f6f412466a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2851baacd2a0444399bda8cc07480ba7","placeholder":"​","style":"IPY_MODEL_1fd7ad8da8c74a7aa52a97b61243e9f8","value":"ppo-seals-CartPole-v0.zip: 100%"}},"516b7de559774808a8850a1e5ede1c06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d5af323c57249cd9f33c3df03c1b1f7","max":139005,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14d9c2d9df104c14ac3ad2835ab9cf58","value":139005}},"84a42027c1724615940cdc4504a50183":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a2ae27113bc4fff88a07982101c56a9","placeholder":"​","style":"IPY_MODEL_f8f65a1bcc944e57baf6f6ee77b164ee","value":" 139k/139k [00:00&lt;00:00, 1.86MB/s]"}},"e044f0fe858f4622b617dd21e750b735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2851baacd2a0444399bda8cc07480ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd7ad8da8c74a7aa52a97b61243e9f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d5af323c57249cd9f33c3df03c1b1f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14d9c2d9df104c14ac3ad2835ab9cf58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a2ae27113bc4fff88a07982101c56a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8f65a1bcc944e57baf6f6ee77b164ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f586dd0cec994c4d946ce8d77d9fa63f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50528fd2199141ad90a4153385384611","IPY_MODEL_88a4e41198654677aca577ec3554704b","IPY_MODEL_5b1327f61f93423d831fbe9e8dc3ca57"],"layout":"IPY_MODEL_cd9f7da2a9c34640ba3a312af36e3882"}},"50528fd2199141ad90a4153385384611":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f876d03b259a4452b48b571e132adcee","placeholder":"​","style":"IPY_MODEL_98c27a16b6564d968f36ea231e7b3585","value":"ppo-seals-HalfCheetah-v1.zip: 100%"}},"88a4e41198654677aca577ec3554704b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6eaeb4382834a07810eda9cc5bd3115","max":167933,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75fa600c0e104b67b25ba41b23256e74","value":167933}},"5b1327f61f93423d831fbe9e8dc3ca57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53bc7473ca014b3b8c3fefc940214510","placeholder":"​","style":"IPY_MODEL_608234fcdf904115b011fce305da3535","value":" 168k/168k [00:00&lt;00:00, 920kB/s]"}},"cd9f7da2a9c34640ba3a312af36e3882":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f876d03b259a4452b48b571e132adcee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c27a16b6564d968f36ea231e7b3585":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6eaeb4382834a07810eda9cc5bd3115":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75fa600c0e104b67b25ba41b23256e74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53bc7473ca014b3b8c3fefc940214510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"608234fcdf904115b011fce305da3535":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28b5c21733bc4cc2934cd86c90c3a153":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75cd4aac4fe047b1b23e8a90f03c45e0","IPY_MODEL_2e2c28e6c88a4c709202ac55378598d4","IPY_MODEL_066a46748dcb443b860bc83e4b6030a5"],"layout":"IPY_MODEL_b55ddc6148bb44f58b0f7361bb79dfcf"}},"75cd4aac4fe047b1b23e8a90f03c45e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e633c8ab6fba4dbb89a832b4c4126a2d","placeholder":"​","style":"IPY_MODEL_cc7bd4111ee149b3a6a707dec1cb2032","value":""}},"2e2c28e6c88a4c709202ac55378598d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1754314bb21a419c837863dc8e0027fe","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cd43964c46349309c3c65f884628636","value":0}},"066a46748dcb443b860bc83e4b6030a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b0cbe3886fa4658810458753859bcab","placeholder":"​","style":"IPY_MODEL_1607010718454da0a5e1cd7afc432b76","value":" 0/0 [00:00&lt;?, ?it/s]"}},"b55ddc6148bb44f58b0f7361bb79dfcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e633c8ab6fba4dbb89a832b4c4126a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc7bd4111ee149b3a6a707dec1cb2032":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1754314bb21a419c837863dc8e0027fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1cd43964c46349309c3c65f884628636":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b0cbe3886fa4658810458753859bcab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1607010718454da0a5e1cd7afc432b76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["plan\n","\n","1. check differences between bc, gail, airl in ppo and trpo.\n","\n","2. for gail, check different network structure"],"metadata":{"id":"yqS6f79T_Ycw"}},{"cell_type":"markdown","source":["# dependencies\n","connect to cpu if want to use behaviour cloning"],"metadata":{"id":"ixn1sAM3Bqny"}},{"cell_type":"code","source":["#!git clone http://github.com/HumanCompatibleAI/imitation\n","#!pip install -e \"/content/imitation[dev]\""],"metadata":{"id":"IFne9nIxhmcJ","executionInfo":{"status":"ok","timestamp":1709634530981,"user_tz":0,"elapsed":3,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!pip install gymnasium[mujoco]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdPCJmj1K-rc","executionInfo":{"status":"ok","timestamp":1709634541243,"user_tz":0,"elapsed":10265,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"7d84ca0f-a02f-4bc5-df34-b8589e843f1b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium[mujoco]\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.10.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium[mujoco])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n","  Downloading mujoco-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.31.6)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n","Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n","  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.1.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.17.0)\n","Installing collected packages: glfw, farama-notifications, gymnasium, mujoco\n","Successfully installed farama-notifications-0.0.4 glfw-2.7.0 gymnasium-0.29.1 mujoco-3.1.2\n"]}]},{"cell_type":"code","source":["!pip install imitation\n","!pip install shimmy\n","!pip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIzQfSL8zRGZ","executionInfo":{"status":"ok","timestamp":1709634578056,"user_tz":0,"elapsed":36817,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"dd8f8561-0130-4748-a7f7-da71da88c811"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imitation\n","  Downloading imitation-1.0.0-py3-none-any.whl (216 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/216.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/216.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium[classic-control]~=0.29 in /usr/local/lib/python3.10/dist-packages (from imitation) (0.29.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imitation) (3.7.1)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from imitation) (1.25.2)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from imitation) (2.1.0+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from imitation) (4.66.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from imitation) (13.7.1)\n","Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from imitation) (1.2.2)\n","Collecting seals~=0.2.1 (from imitation)\n","  Downloading seals-0.2.1-py3-none-any.whl (35 kB)\n","Collecting stable-baselines3~=2.0 (from imitation)\n","  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacred>=0.8.4 (from imitation)\n","  Downloading sacred-0.8.5-py2.py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.10/dist-packages (from imitation) (2.15.2)\n","Collecting huggingface-sb3~=3.0 (from imitation)\n","  Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n","Collecting optuna>=3.0.1 (from imitation)\n","  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets>=2.8.0 (from imitation)\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.8.0->imitation)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (2.31.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (3.4.1)\n","Collecting multiprocess (from datasets>=2.8.0->imitation)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.8.0->imitation) (6.0.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]~=0.29->imitation) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]~=0.29->imitation) (4.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]~=0.29->imitation) (0.0.4)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]~=0.29->imitation) (2.5.2)\n","Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3~=3.0->imitation) (1.1.2)\n","Collecting alembic>=1.5.0 (from optuna>=3.0.1->imitation)\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna>=3.0.1->imitation)\n","  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.0.1->imitation) (2.0.27)\n","Collecting docopt<1.0,>=0.3 (from sacred>=0.8.4->imitation)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: jsonpickle>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from sacred>=0.8.4->imitation) (3.0.3)\n","Collecting munch<5.0,>=2.5 (from sacred>=0.8.4->imitation)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from sacred>=0.8.4->imitation) (1.14.1)\n","Requirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.10/dist-packages (from sacred>=0.8.4->imitation) (9.0.0)\n","Collecting colorama>=0.4 (from sacred>=0.8.4->imitation)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting GitPython (from sacred>=0.8.4->imitation)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->imitation) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->imitation) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.2->imitation) (3.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (1.62.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (3.5.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.14->imitation) (3.0.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->imitation) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->imitation) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->imitation) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->imitation) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imitation) (2.8.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->imitation) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->imitation) (2.16.1)\n","Collecting Mako (from alembic>=1.5.0->optuna>=3.0.1->imitation)\n","  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.8.0->imitation) (4.0.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->imitation) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->imitation) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->imitation) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=1.14->imitation) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->imitation) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->imitation) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->imitation) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->imitation) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.8.0->imitation) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.0.1->imitation) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=1.14->imitation) (2.1.5)\n","Collecting gitdb<5,>=4.0.1 (from GitPython->sacred>=0.8.4->imitation)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.8.0->imitation) (2023.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->imitation) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->sacred>=0.8.4->imitation)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->imitation) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=1.14->imitation) (3.2.2)\n","Building wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=5b7089ca31609c166ca6a4bb50731be70d4e60e7d2ad54c64353419f2fb570c6\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built docopt\n","Installing collected packages: docopt, smmap, munch, Mako, dill, colorlog, colorama, seals, multiprocess, gitdb, alembic, stable-baselines3, optuna, huggingface-sb3, GitPython, sacred, datasets, imitation\n","Successfully installed GitPython-3.1.42 Mako-1.3.2 alembic-1.13.1 colorama-0.4.6 colorlog-6.8.2 datasets-2.18.0 dill-0.3.8 docopt-0.6.2 gitdb-4.0.11 huggingface-sb3-3.0 imitation-1.0.0 multiprocess-0.70.16 munch-4.0.0 optuna-3.5.0 sacred-0.8.5 seals-0.2.1 smmap-5.0.1 stable-baselines3-2.2.1\n","Collecting shimmy\n","  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.25.2)\n","Requirement already satisfied: gymnasium>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (0.29.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (4.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.27.0->shimmy) (0.0.4)\n","Installing collected packages: shimmy\n","Successfully installed shimmy-1.3.0\n","Collecting git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n","  Cloning https://github.com/Stable-Baselines-Team/stable-baselines3-contrib to /tmp/pip-req-build-t8074qxx\n","  Running command git clone --filter=blob:none --quiet https://github.com/Stable-Baselines-Team/stable-baselines3-contrib /tmp/pip-req-build-t8074qxx\n","  Resolved https://github.com/Stable-Baselines-Team/stable-baselines3-contrib to commit 8dca5d5e22315fb01c11ed24ef6a4546aa4c508e\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting stable-baselines3<3.0,>=2.3.0a0 (from sb3_contrib==2.3.0a1)\n","  Downloading stable_baselines3-2.3.0a2-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (0.29.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.25.2)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2.1.0+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (3.7.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (4.10.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3<3.0,>=2.3.0a0->sb3_contrib==2.3.0a1) (1.3.0)\n","Building wheels for collected packages: sb3_contrib\n","  Building wheel for sb3_contrib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sb3_contrib: filename=sb3_contrib-2.3.0a1-py3-none-any.whl size=80744 sha256=1cc18ea1624396f5bce37491ee47b526bd0e80971363aea946fa82e1cc6a80cd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-va9ymikz/wheels/f8/e1/97/7e886248901c65ffe4ff7a9c16bccfbde992692ac10d99ee41\n","Successfully built sb3_contrib\n","Installing collected packages: stable-baselines3, sb3_contrib\n","  Attempting uninstall: stable-baselines3\n","    Found existing installation: stable-baselines3 2.2.1\n","    Uninstalling stable-baselines3-2.2.1:\n","      Successfully uninstalled stable-baselines3-2.2.1\n","Successfully installed sb3_contrib-2.3.0a1 stable-baselines3-2.3.0a2\n"]}]},{"cell_type":"markdown","source":["# GAIL-PPO"],"metadata":{"id":"Wf5dmRrrBvMJ"}},{"cell_type":"code","source":["import gymnasium as gym\n","import imitation"],"metadata":{"id":"-ZotCu2u8JbW","executionInfo":{"status":"ok","timestamp":1709634580201,"user_tz":0,"elapsed":1449,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from imitation.policies.serialize import load_policy\n","from imitation.util.util import make_vec_env\n","from imitation.data.wrappers import RolloutInfoWrapper"],"metadata":{"id":"wzyfbCln0vtB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709634596847,"user_tz":0,"elapsed":16649,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"60a03f85-7e2b-4149-b00f-9dd5a30a298b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["SEED = 42\n","\n","env = make_vec_env(\n","    \"seals:seals/HalfCheetah-v1\", # seals:seals/CartPole-v0\n","    rng=np.random.default_rng(SEED),\n","    n_envs=8,\n","    post_wrappers=[\n","        lambda env, _: RolloutInfoWrapper(env)\n","    ],  # needed for computing rollouts later\n",")\n","expert = load_policy(\n","    \"ppo-huggingface\",\n","    organization=\"HumanCompatibleAI\",\n","    env_name=\"seals/HalfCheetah-v1\",\n","    venv=env,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["f586dd0cec994c4d946ce8d77d9fa63f","50528fd2199141ad90a4153385384611","88a4e41198654677aca577ec3554704b","5b1327f61f93423d831fbe9e8dc3ca57","cd9f7da2a9c34640ba3a312af36e3882","f876d03b259a4452b48b571e132adcee","98c27a16b6564d968f36ea231e7b3585","a6eaeb4382834a07810eda9cc5bd3115","75fa600c0e104b67b25ba41b23256e74","53bc7473ca014b3b8c3fefc940214510","608234fcdf904115b011fce305da3535","28b5c21733bc4cc2934cd86c90c3a153","75cd4aac4fe047b1b23e8a90f03c45e0","2e2c28e6c88a4c709202ac55378598d4","066a46748dcb443b860bc83e4b6030a5","b55ddc6148bb44f58b0f7361bb79dfcf","e633c8ab6fba4dbb89a832b4c4126a2d","cc7bd4111ee149b3a6a707dec1cb2032","1754314bb21a419c837863dc8e0027fe","1cd43964c46349309c3c65f884628636","4b0cbe3886fa4658810458753859bcab","1607010718454da0a5e1cd7afc432b76"]},"id":"tIC0ip21MYHo","executionInfo":{"status":"ok","timestamp":1709634599736,"user_tz":0,"elapsed":2902,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"ad70b5b7-82ca-46c2-ba9e-0f3690712845"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["ppo-seals-HalfCheetah-v1.zip:   0%|          | 0.00/168k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f586dd0cec994c4d946ce8d77d9fa63f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b5c21733bc4cc2934cd86c90c3a153"}},"metadata":{}}]},{"cell_type":"code","source":["from imitation.data import rollout\n","\n","rollouts = rollout.rollout(\n","    expert,\n","    env,\n","    rollout.make_sample_until(min_timesteps=1000000, min_episodes=4), # config: https://huggingface.co/HumanCompatibleAI/ppo-seals-HalfCheetah-v1\n","    rng=np.random.default_rng(SEED), # 随机数？ The random state to use for sampling trajectories.\n",")"],"metadata":{"id":"A9Ra6iKD0xu5","executionInfo":{"status":"ok","timestamp":1709634673723,"user_tz":0,"elapsed":73991,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from imitation.algorithms.adversarial.gail import GAIL\n","from imitation.rewards.reward_nets import BasicRewardNet\n","from imitation.util.networks import RunningNorm\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import MlpPolicy\n","from stable_baselines3.common.evaluation import evaluate_policy\n","import sb3_contrib\n","from sb3_contrib import TRPO\n","\n","learner = PPO(\n","    env=env,\n","    policy=MlpPolicy,\n","    batch_size=64,\n","    ent_coef=0.0,\n","    learning_rate=0.0004,\n","    gamma=0.95,\n","    n_epochs=100,\n","    seed=SEED,\n",")\n","\n","reward_net = BasicRewardNet(\n","    observation_space=env.observation_space,\n","    action_space=env.action_space,\n","    normalize_input_layer=RunningNorm,\n",")\n","\n","gail_trainer = GAIL(\n","    demonstrations=rollouts,\n","    demo_batch_size=1024,\n","    gen_replay_buffer_capacity=512,\n","    n_disc_updates_per_round=8,\n","    venv=env,\n","    gen_algo=learner,\n","    reward_net=reward_net,\n","    allow_variable_horizon=True\n",")"],"metadata":{"id":"98QGgy8n0zki","executionInfo":{"status":"ok","timestamp":1709635096830,"user_tz":0,"elapsed":319,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dc181c0c-04ae-497c-dae1-2ee98716b7df"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Running with `allow_variable_horizon` set to True. Some algorithms are biased towards shorter or longer episodes, which may significantly confound results. Additionally, even unbiased algorithms can exploit the information leak from the termination condition, producing spuriously high performance. See https://imitation.readthedocs.io/en/latest/getting-started/variable-horizon.html for more information.\n"]}]},{"cell_type":"code","source":["env.seed(SEED)\n","learner_rewards_before_training, _ = evaluate_policy(\n","    learner, env, 20, return_episode_rewards=True\n",")"],"metadata":{"id":"YCfobJHS01vh","executionInfo":{"status":"ok","timestamp":1709635105798,"user_tz":0,"elapsed":5704,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["gail_trainer.train(300000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7eT6gqt03Qy","executionInfo":{"status":"ok","timestamp":1709637189046,"user_tz":0,"elapsed":2081414,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"e80dd7b3-fd33-49d6-db83-f1baab13c197"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["\rround:   0%|          | 0/18 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------\n","| raw/                        |          |\n","|    gen/rollout/ep_len_mean  | 1e+03    |\n","|    gen/rollout/ep_rew_mean  | -370     |\n","|    gen/time/fps             | 2691     |\n","|    gen/time/iterations      | 1        |\n","|    gen/time/time_elapsed    | 6        |\n","|    gen/time/total_timesteps | 16384    |\n","------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.61     |\n","|    disc/disc_acc_expert             | 0.356    |\n","|    disc/disc_acc_gen                | 0.864    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.685    |\n","|    disc/disc_proportion_expert_pred | 0.246    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.646    |\n","|    disc/disc_acc_expert             | 0.423    |\n","|    disc/disc_acc_gen                | 0.868    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.682    |\n","|    disc/disc_proportion_expert_pred | 0.277    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.691    |\n","|    disc/disc_acc_expert             | 0.491    |\n","|    disc/disc_acc_gen                | 0.891    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.679    |\n","|    disc/disc_proportion_expert_pred | 0.3      |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.75     |\n","|    disc/disc_acc_expert             | 0.602    |\n","|    disc/disc_acc_gen                | 0.897    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.674    |\n","|    disc/disc_proportion_expert_pred | 0.352    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.782    |\n","|    disc/disc_acc_expert             | 0.664    |\n","|    disc/disc_acc_gen                | 0.9      |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.671    |\n","|    disc/disc_proportion_expert_pred | 0.382    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.822    |\n","|    disc/disc_acc_expert             | 0.725    |\n","|    disc/disc_acc_gen                | 0.919    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.668    |\n","|    disc/disc_proportion_expert_pred | 0.403    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.86     |\n","|    disc/disc_acc_expert             | 0.786    |\n","|    disc/disc_acc_gen                | 0.935    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.663    |\n","|    disc/disc_proportion_expert_pred | 0.426    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.879    |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.932    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.659    |\n","|    disc/disc_proportion_expert_pred | 0.447    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.755    |\n","|    disc/disc_acc_expert             | 0.609    |\n","|    disc/disc_acc_gen                | 0.901    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.673    |\n","|    disc/disc_proportion_expert_pred | 0.354    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -370     |\n","|    gen/time/fps                     | 2.69e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.64e+04 |\n","|    gen/train/approx_kl              | 0.0513   |\n","|    gen/train/clip_fraction          | 0.385    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.52    |\n","|    gen/train/explained_variance     | -0.626   |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | 0.508    |\n","|    gen/train/n_updates              | 100      |\n","|    gen/train/policy_gradient_loss   | -0.0484  |\n","|    gen/train/std                    | 0.997    |\n","|    gen/train/value_loss             | 0.97     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:   6%|▌         | 1/18 [01:54<32:26, 114.50s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -377       |\n","|    gen/rollout/ep_rew_wrapped_mean | 756        |\n","|    gen/time/fps                    | 2555       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 32768      |\n","|    gen/train/approx_kl             | 0.05130582 |\n","|    gen/train/clip_fraction         | 0.385      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8.52      |\n","|    gen/train/explained_variance    | -0.626     |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | 0.508      |\n","|    gen/train/n_updates             | 100        |\n","|    gen/train/policy_gradient_loss  | -0.0484    |\n","|    gen/train/std                   | 0.997      |\n","|    gen/train/value_loss            | 0.97       |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.902    |\n","|    disc/disc_acc_expert             | 0.845    |\n","|    disc/disc_acc_gen                | 0.959    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.652    |\n","|    disc/disc_proportion_expert_pred | 0.443    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.928    |\n","|    disc/disc_acc_expert             | 0.889    |\n","|    disc/disc_acc_gen                | 0.967    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.646    |\n","|    disc/disc_proportion_expert_pred | 0.461    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.931    |\n","|    disc/disc_acc_expert             | 0.896    |\n","|    disc/disc_acc_gen                | 0.966    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.643    |\n","|    disc/disc_proportion_expert_pred | 0.465    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.933    |\n","|    disc/disc_acc_expert             | 0.901    |\n","|    disc/disc_acc_gen                | 0.964    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.639    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.954    |\n","|    disc/disc_acc_expert             | 0.932    |\n","|    disc/disc_acc_gen                | 0.977    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.634    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.949    |\n","|    disc/disc_acc_expert             | 0.931    |\n","|    disc/disc_acc_gen                | 0.968    |\n","|    disc/disc_entropy                | 0.69     |\n","|    disc/disc_loss                   | 0.629    |\n","|    disc/disc_proportion_expert_pred | 0.481    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.952    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.967    |\n","|    disc/disc_entropy                | 0.69     |\n","|    disc/disc_loss                   | 0.624    |\n","|    disc/disc_proportion_expert_pred | 0.485    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.961    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.97     |\n","|    disc/disc_entropy                | 0.689    |\n","|    disc/disc_loss                   | 0.618    |\n","|    disc/disc_proportion_expert_pred | 0.492    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.939    |\n","|    disc/disc_acc_expert             | 0.911    |\n","|    disc/disc_acc_gen                | 0.967    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.636    |\n","|    disc/disc_proportion_expert_pred | 0.472    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -377     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 756      |\n","|    gen/time/fps                     | 2.56e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 3.28e+04 |\n","|    gen/train/approx_kl              | 0.0563   |\n","|    gen/train/clip_fraction          | 0.428    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.46    |\n","|    gen/train/explained_variance     | -0.399   |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | 0.0231   |\n","|    gen/train/n_updates              | 200      |\n","|    gen/train/policy_gradient_loss   | -0.0584  |\n","|    gen/train/std                    | 0.988    |\n","|    gen/train/value_loss             | 0.0567   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  11%|█         | 2/18 [03:49<30:34, 114.68s/it]"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------\n","| raw/                               |             |\n","|    gen/rollout/ep_len_mean         | 1e+03       |\n","|    gen/rollout/ep_rew_mean         | -363        |\n","|    gen/rollout/ep_rew_wrapped_mean | 710         |\n","|    gen/time/fps                    | 2541        |\n","|    gen/time/iterations             | 1           |\n","|    gen/time/time_elapsed           | 6           |\n","|    gen/time/total_timesteps        | 49152       |\n","|    gen/train/approx_kl             | 0.056286022 |\n","|    gen/train/clip_fraction         | 0.428       |\n","|    gen/train/clip_range            | 0.2         |\n","|    gen/train/entropy_loss          | -8.46       |\n","|    gen/train/explained_variance    | -0.399      |\n","|    gen/train/learning_rate         | 0.0004      |\n","|    gen/train/loss                  | 0.0231      |\n","|    gen/train/n_updates             | 200         |\n","|    gen/train/policy_gradient_loss  | -0.0584     |\n","|    gen/train/std                   | 0.988       |\n","|    gen/train/value_loss            | 0.0567      |\n","----------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.932    |\n","|    disc/disc_acc_expert             | 0.935    |\n","|    disc/disc_acc_gen                | 0.929    |\n","|    disc/disc_entropy                | 0.689    |\n","|    disc/disc_loss                   | 0.622    |\n","|    disc/disc_proportion_expert_pred | 0.503    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.934    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.921    |\n","|    disc/disc_entropy                | 0.688    |\n","|    disc/disc_loss                   | 0.616    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.944    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 0.938    |\n","|    disc/disc_entropy                | 0.688    |\n","|    disc/disc_loss                   | 0.611    |\n","|    disc/disc_proportion_expert_pred | 0.505    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.941    |\n","|    disc/disc_acc_expert             | 0.957    |\n","|    disc/disc_acc_gen                | 0.926    |\n","|    disc/disc_entropy                | 0.687    |\n","|    disc/disc_loss                   | 0.602    |\n","|    disc/disc_proportion_expert_pred | 0.516    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.937    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.927    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.6      |\n","|    disc/disc_proportion_expert_pred | 0.51     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.944    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.935    |\n","|    disc/disc_entropy                | 0.685    |\n","|    disc/disc_loss                   | 0.594    |\n","|    disc/disc_proportion_expert_pred | 0.509    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.939    |\n","|    disc/disc_acc_expert             | 0.954    |\n","|    disc/disc_acc_gen                | 0.924    |\n","|    disc/disc_entropy                | 0.684    |\n","|    disc/disc_loss                   | 0.588    |\n","|    disc/disc_proportion_expert_pred | 0.515    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.935    |\n","|    disc/disc_acc_expert             | 0.956    |\n","|    disc/disc_acc_gen                | 0.913    |\n","|    disc/disc_entropy                | 0.683    |\n","|    disc/disc_loss                   | 0.581    |\n","|    disc/disc_proportion_expert_pred | 0.521    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.95     |\n","|    disc/disc_acc_gen                | 0.927    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.602    |\n","|    disc/disc_proportion_expert_pred | 0.512    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -363     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 710      |\n","|    gen/time/fps                     | 2.54e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 4.92e+04 |\n","|    gen/train/approx_kl              | 0.0599   |\n","|    gen/train/clip_fraction          | 0.437    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.41    |\n","|    gen/train/explained_variance     | 0.584    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0574  |\n","|    gen/train/n_updates              | 300      |\n","|    gen/train/policy_gradient_loss   | -0.0626  |\n","|    gen/train/std                    | 0.979    |\n","|    gen/train/value_loss             | 0.0188   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  17%|█▋        | 3/18 [05:44<28:45, 115.05s/it]"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------\n","| raw/                               |             |\n","|    gen/rollout/ep_len_mean         | 1e+03       |\n","|    gen/rollout/ep_rew_mean         | -361        |\n","|    gen/rollout/ep_rew_wrapped_mean | 694         |\n","|    gen/time/fps                    | 2608        |\n","|    gen/time/iterations             | 1           |\n","|    gen/time/time_elapsed           | 6           |\n","|    gen/time/total_timesteps        | 65536       |\n","|    gen/train/approx_kl             | 0.059927896 |\n","|    gen/train/clip_fraction         | 0.437       |\n","|    gen/train/clip_range            | 0.2         |\n","|    gen/train/entropy_loss          | -8.41       |\n","|    gen/train/explained_variance    | 0.584       |\n","|    gen/train/learning_rate         | 0.0004      |\n","|    gen/train/loss                  | -0.0574     |\n","|    gen/train/n_updates             | 300         |\n","|    gen/train/policy_gradient_loss  | -0.0626     |\n","|    gen/train/std                   | 0.979       |\n","|    gen/train/value_loss            | 0.0188      |\n","----------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.949    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.955    |\n","|    disc/disc_entropy                | 0.681    |\n","|    disc/disc_loss                   | 0.572    |\n","|    disc/disc_proportion_expert_pred | 0.494    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.953    |\n","|    disc/disc_acc_expert             | 0.954    |\n","|    disc/disc_acc_gen                | 0.951    |\n","|    disc/disc_entropy                | 0.68     |\n","|    disc/disc_loss                   | 0.564    |\n","|    disc/disc_proportion_expert_pred | 0.501    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.95     |\n","|    disc/disc_acc_expert             | 0.962    |\n","|    disc/disc_acc_gen                | 0.938    |\n","|    disc/disc_entropy                | 0.679    |\n","|    disc/disc_loss                   | 0.559    |\n","|    disc/disc_proportion_expert_pred | 0.512    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.945    |\n","|    disc/disc_acc_expert             | 0.951    |\n","|    disc/disc_acc_gen                | 0.938    |\n","|    disc/disc_entropy                | 0.677    |\n","|    disc/disc_loss                   | 0.552    |\n","|    disc/disc_proportion_expert_pred | 0.506    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.948    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.949    |\n","|    disc/disc_entropy                | 0.676    |\n","|    disc/disc_loss                   | 0.547    |\n","|    disc/disc_proportion_expert_pred | 0.499    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.944    |\n","|    disc/disc_acc_expert             | 0.957    |\n","|    disc/disc_acc_gen                | 0.932    |\n","|    disc/disc_entropy                | 0.672    |\n","|    disc/disc_loss                   | 0.535    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.947    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.947    |\n","|    disc/disc_entropy                | 0.67     |\n","|    disc/disc_loss                   | 0.528    |\n","|    disc/disc_proportion_expert_pred | 0.5      |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.952    |\n","|    disc/disc_acc_expert             | 0.954    |\n","|    disc/disc_acc_gen                | 0.95     |\n","|    disc/disc_entropy                | 0.668    |\n","|    disc/disc_loss                   | 0.519    |\n","|    disc/disc_proportion_expert_pred | 0.502    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.949    |\n","|    disc/disc_acc_expert             | 0.952    |\n","|    disc/disc_acc_gen                | 0.945    |\n","|    disc/disc_entropy                | 0.675    |\n","|    disc/disc_loss                   | 0.547    |\n","|    disc/disc_proportion_expert_pred | 0.503    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -361     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 694      |\n","|    gen/time/fps                     | 2.61e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 6.55e+04 |\n","|    gen/train/approx_kl              | 0.0625   |\n","|    gen/train/clip_fraction          | 0.452    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.36    |\n","|    gen/train/explained_variance     | 0.782    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0699  |\n","|    gen/train/n_updates              | 400      |\n","|    gen/train/policy_gradient_loss   | -0.0686  |\n","|    gen/train/std                    | 0.972    |\n","|    gen/train/value_loss             | 0.0167   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  22%|██▏       | 4/18 [07:40<26:53, 115.28s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -366       |\n","|    gen/rollout/ep_rew_wrapped_mean | 682        |\n","|    gen/time/fps                    | 2478       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 81920      |\n","|    gen/train/approx_kl             | 0.06254129 |\n","|    gen/train/clip_fraction         | 0.452      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8.36      |\n","|    gen/train/explained_variance    | 0.782      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0699    |\n","|    gen/train/n_updates             | 400        |\n","|    gen/train/policy_gradient_loss  | -0.0686    |\n","|    gen/train/std                   | 0.972      |\n","|    gen/train/value_loss            | 0.0167     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.935    |\n","|    disc/disc_acc_expert             | 0.955    |\n","|    disc/disc_acc_gen                | 0.914    |\n","|    disc/disc_entropy                | 0.665    |\n","|    disc/disc_loss                   | 0.515    |\n","|    disc/disc_proportion_expert_pred | 0.521    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.934    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.921    |\n","|    disc/disc_entropy                | 0.663    |\n","|    disc/disc_loss                   | 0.511    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.945    |\n","|    disc/disc_acc_expert             | 0.959    |\n","|    disc/disc_acc_gen                | 0.931    |\n","|    disc/disc_entropy                | 0.66     |\n","|    disc/disc_loss                   | 0.498    |\n","|    disc/disc_proportion_expert_pred | 0.514    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.937    |\n","|    disc/disc_acc_expert             | 0.961    |\n","|    disc/disc_acc_gen                | 0.912    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.492    |\n","|    disc/disc_proportion_expert_pred | 0.524    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.946    |\n","|    disc/disc_acc_expert             | 0.961    |\n","|    disc/disc_acc_gen                | 0.932    |\n","|    disc/disc_entropy                | 0.652    |\n","|    disc/disc_loss                   | 0.479    |\n","|    disc/disc_proportion_expert_pred | 0.515    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.929    |\n","|    disc/disc_entropy                | 0.649    |\n","|    disc/disc_loss                   | 0.476    |\n","|    disc/disc_proportion_expert_pred | 0.51     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.937    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.93     |\n","|    disc/disc_entropy                | 0.643    |\n","|    disc/disc_loss                   | 0.464    |\n","|    disc/disc_proportion_expert_pred | 0.507    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.936    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.925    |\n","|    disc/disc_entropy                | 0.64     |\n","|    disc/disc_loss                   | 0.458    |\n","|    disc/disc_proportion_expert_pred | 0.511    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.924    |\n","|    disc/disc_entropy                | 0.654    |\n","|    disc/disc_loss                   | 0.487    |\n","|    disc/disc_proportion_expert_pred | 0.514    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -366     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 682      |\n","|    gen/time/fps                     | 2.48e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 8.19e+04 |\n","|    gen/train/approx_kl              | 0.0773   |\n","|    gen/train/clip_fraction          | 0.481    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.31    |\n","|    gen/train/explained_variance     | 0.836    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0329  |\n","|    gen/train/n_updates              | 500      |\n","|    gen/train/policy_gradient_loss   | -0.0701  |\n","|    gen/train/std                    | 0.964    |\n","|    gen/train/value_loss             | 0.0312   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  28%|██▊       | 5/18 [09:36<25:02, 115.57s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -367       |\n","|    gen/rollout/ep_rew_wrapped_mean | 672        |\n","|    gen/time/fps                    | 2501       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 98304      |\n","|    gen/train/approx_kl             | 0.07730751 |\n","|    gen/train/clip_fraction         | 0.481      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8.31      |\n","|    gen/train/explained_variance    | 0.836      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0329    |\n","|    gen/train/n_updates             | 500        |\n","|    gen/train/policy_gradient_loss  | -0.0701    |\n","|    gen/train/std                   | 0.964      |\n","|    gen/train/value_loss            | 0.0312     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.9      |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.853    |\n","|    disc/disc_entropy                | 0.639    |\n","|    disc/disc_loss                   | 0.468    |\n","|    disc/disc_proportion_expert_pred | 0.548    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.898    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.853    |\n","|    disc/disc_entropy                | 0.634    |\n","|    disc/disc_loss                   | 0.458    |\n","|    disc/disc_proportion_expert_pred | 0.545    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.894    |\n","|    disc/disc_acc_expert             | 0.96     |\n","|    disc/disc_acc_gen                | 0.828    |\n","|    disc/disc_entropy                | 0.629    |\n","|    disc/disc_loss                   | 0.451    |\n","|    disc/disc_proportion_expert_pred | 0.566    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.89     |\n","|    disc/disc_acc_expert             | 0.936    |\n","|    disc/disc_acc_gen                | 0.844    |\n","|    disc/disc_entropy                | 0.624    |\n","|    disc/disc_loss                   | 0.445    |\n","|    disc/disc_proportion_expert_pred | 0.546    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.887    |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.832    |\n","|    disc/disc_entropy                | 0.62     |\n","|    disc/disc_loss                   | 0.442    |\n","|    disc/disc_proportion_expert_pred | 0.555    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.954    |\n","|    disc/disc_acc_gen                | 0.829    |\n","|    disc/disc_entropy                | 0.612    |\n","|    disc/disc_loss                   | 0.428    |\n","|    disc/disc_proportion_expert_pred | 0.562    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.893    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.841    |\n","|    disc/disc_entropy                | 0.608    |\n","|    disc/disc_loss                   | 0.421    |\n","|    disc/disc_proportion_expert_pred | 0.552    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.912    |\n","|    disc/disc_acc_expert             | 0.952    |\n","|    disc/disc_acc_gen                | 0.872    |\n","|    disc/disc_entropy                | 0.6      |\n","|    disc/disc_loss                   | 0.404    |\n","|    disc/disc_proportion_expert_pred | 0.54     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.844    |\n","|    disc/disc_entropy                | 0.621    |\n","|    disc/disc_loss                   | 0.44     |\n","|    disc/disc_proportion_expert_pred | 0.552    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -367     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 672      |\n","|    gen/time/fps                     | 2.5e+03  |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 9.83e+04 |\n","|    gen/train/approx_kl              | 0.0774   |\n","|    gen/train/clip_fraction          | 0.481    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.26    |\n","|    gen/train/explained_variance     | 0.864    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0616  |\n","|    gen/train/n_updates              | 600      |\n","|    gen/train/policy_gradient_loss   | -0.0683  |\n","|    gen/train/std                    | 0.956    |\n","|    gen/train/value_loss             | 0.0582   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  33%|███▎      | 6/18 [11:32<23:06, 115.57s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -365       |\n","|    gen/rollout/ep_rew_wrapped_mean | 661        |\n","|    gen/time/fps                    | 2725       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 114688     |\n","|    gen/train/approx_kl             | 0.07736354 |\n","|    gen/train/clip_fraction         | 0.481      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8.26      |\n","|    gen/train/explained_variance    | 0.864      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0616    |\n","|    gen/train/n_updates             | 600        |\n","|    gen/train/policy_gradient_loss  | -0.0683    |\n","|    gen/train/std                   | 0.956      |\n","|    gen/train/value_loss            | 0.0582     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.956    |\n","|    disc/disc_acc_expert             | 0.955    |\n","|    disc/disc_acc_gen                | 0.956    |\n","|    disc/disc_entropy                | 0.587    |\n","|    disc/disc_loss                   | 0.367    |\n","|    disc/disc_proportion_expert_pred | 0.5      |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.955    |\n","|    disc/disc_acc_expert             | 0.95     |\n","|    disc/disc_acc_gen                | 0.96     |\n","|    disc/disc_entropy                | 0.583    |\n","|    disc/disc_loss                   | 0.364    |\n","|    disc/disc_proportion_expert_pred | 0.495    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.955    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.957    |\n","|    disc/disc_entropy                | 0.573    |\n","|    disc/disc_loss                   | 0.352    |\n","|    disc/disc_proportion_expert_pred | 0.498    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.953    |\n","|    disc/disc_acc_expert             | 0.944    |\n","|    disc/disc_acc_gen                | 0.961    |\n","|    disc/disc_entropy                | 0.569    |\n","|    disc/disc_loss                   | 0.353    |\n","|    disc/disc_proportion_expert_pred | 0.492    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.957    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.97     |\n","|    disc/disc_entropy                | 0.558    |\n","|    disc/disc_loss                   | 0.336    |\n","|    disc/disc_proportion_expert_pred | 0.487    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.956    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.964    |\n","|    disc/disc_entropy                | 0.552    |\n","|    disc/disc_loss                   | 0.33     |\n","|    disc/disc_proportion_expert_pred | 0.492    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.948    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.957    |\n","|    disc/disc_entropy                | 0.544    |\n","|    disc/disc_loss                   | 0.325    |\n","|    disc/disc_proportion_expert_pred | 0.491    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.947    |\n","|    disc/disc_acc_expert             | 0.927    |\n","|    disc/disc_acc_gen                | 0.968    |\n","|    disc/disc_entropy                | 0.536    |\n","|    disc/disc_loss                   | 0.32     |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.953    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.962    |\n","|    disc/disc_entropy                | 0.563    |\n","|    disc/disc_loss                   | 0.344    |\n","|    disc/disc_proportion_expert_pred | 0.492    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -365     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 661      |\n","|    gen/time/fps                     | 2.72e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.15e+05 |\n","|    gen/train/approx_kl              | 0.0891   |\n","|    gen/train/clip_fraction          | 0.509    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.18    |\n","|    gen/train/explained_variance     | 0.903    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0756  |\n","|    gen/train/n_updates              | 700      |\n","|    gen/train/policy_gradient_loss   | -0.0685  |\n","|    gen/train/std                    | 0.943    |\n","|    gen/train/value_loss             | 0.0826   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  39%|███▉      | 7/18 [13:26<21:08, 115.31s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -368       |\n","|    gen/rollout/ep_rew_wrapped_mean | 628        |\n","|    gen/time/fps                    | 2428       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 131072     |\n","|    gen/train/approx_kl             | 0.08908844 |\n","|    gen/train/clip_fraction         | 0.509      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8.18      |\n","|    gen/train/explained_variance    | 0.903      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0756    |\n","|    gen/train/n_updates             | 700        |\n","|    gen/train/policy_gradient_loss  | -0.0685    |\n","|    gen/train/std                   | 0.943      |\n","|    gen/train/value_loss            | 0.0826     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.92     |\n","|    disc/disc_acc_expert             | 0.933    |\n","|    disc/disc_acc_gen                | 0.907    |\n","|    disc/disc_entropy                | 0.534    |\n","|    disc/disc_loss                   | 0.336    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.939    |\n","|    disc/disc_acc_expert             | 0.941    |\n","|    disc/disc_acc_gen                | 0.937    |\n","|    disc/disc_entropy                | 0.526    |\n","|    disc/disc_loss                   | 0.321    |\n","|    disc/disc_proportion_expert_pred | 0.502    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.93     |\n","|    disc/disc_acc_expert             | 0.95     |\n","|    disc/disc_acc_gen                | 0.91     |\n","|    disc/disc_entropy                | 0.513    |\n","|    disc/disc_loss                   | 0.312    |\n","|    disc/disc_proportion_expert_pred | 0.52     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.936    |\n","|    disc/disc_acc_gen                | 0.94     |\n","|    disc/disc_entropy                | 0.505    |\n","|    disc/disc_loss                   | 0.303    |\n","|    disc/disc_proportion_expert_pred | 0.498    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.96     |\n","|    disc/disc_acc_gen                | 0.916    |\n","|    disc/disc_entropy                | 0.497    |\n","|    disc/disc_loss                   | 0.292    |\n","|    disc/disc_proportion_expert_pred | 0.522    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.932    |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 0.925    |\n","|    disc/disc_entropy                | 0.49     |\n","|    disc/disc_loss                   | 0.295    |\n","|    disc/disc_proportion_expert_pred | 0.507    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.936    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.929    |\n","|    disc/disc_entropy                | 0.479    |\n","|    disc/disc_loss                   | 0.285    |\n","|    disc/disc_proportion_expert_pred | 0.507    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.943    |\n","|    disc/disc_acc_expert             | 0.951    |\n","|    disc/disc_acc_gen                | 0.935    |\n","|    disc/disc_entropy                | 0.467    |\n","|    disc/disc_loss                   | 0.271    |\n","|    disc/disc_proportion_expert_pred | 0.508    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.935    |\n","|    disc/disc_acc_expert             | 0.944    |\n","|    disc/disc_acc_gen                | 0.925    |\n","|    disc/disc_entropy                | 0.502    |\n","|    disc/disc_loss                   | 0.302    |\n","|    disc/disc_proportion_expert_pred | 0.51     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -368     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 628      |\n","|    gen/time/fps                     | 2.43e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.31e+05 |\n","|    gen/train/approx_kl              | 0.0977   |\n","|    gen/train/clip_fraction          | 0.526    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8.12    |\n","|    gen/train/explained_variance     | 0.928    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0331  |\n","|    gen/train/n_updates              | 800      |\n","|    gen/train/policy_gradient_loss   | -0.0722  |\n","|    gen/train/std                    | 0.931    |\n","|    gen/train/value_loss             | 0.142    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  44%|████▍     | 8/18 [15:23<19:16, 115.61s/it]"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------\n","| raw/                               |             |\n","|    gen/rollout/ep_len_mean         | 1e+03       |\n","|    gen/rollout/ep_rew_mean         | -370        |\n","|    gen/rollout/ep_rew_wrapped_mean | 601         |\n","|    gen/time/fps                    | 2472        |\n","|    gen/time/iterations             | 1           |\n","|    gen/time/time_elapsed           | 6           |\n","|    gen/time/total_timesteps        | 147456      |\n","|    gen/train/approx_kl             | 0.097731166 |\n","|    gen/train/clip_fraction         | 0.526       |\n","|    gen/train/clip_range            | 0.2         |\n","|    gen/train/entropy_loss          | -8.12       |\n","|    gen/train/explained_variance    | 0.928       |\n","|    gen/train/learning_rate         | 0.0004      |\n","|    gen/train/loss                  | -0.0331     |\n","|    gen/train/n_updates             | 800         |\n","|    gen/train/policy_gradient_loss  | -0.0722     |\n","|    gen/train/std                   | 0.931       |\n","|    gen/train/value_loss            | 0.142       |\n","----------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.848    |\n","|    disc/disc_entropy                | 0.47     |\n","|    disc/disc_loss                   | 0.306    |\n","|    disc/disc_proportion_expert_pred | 0.549    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.877    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.809    |\n","|    disc/disc_entropy                | 0.467    |\n","|    disc/disc_loss                   | 0.312    |\n","|    disc/disc_proportion_expert_pred | 0.569    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.898    |\n","|    disc/disc_acc_expert             | 0.951    |\n","|    disc/disc_acc_gen                | 0.845    |\n","|    disc/disc_entropy                | 0.455    |\n","|    disc/disc_loss                   | 0.288    |\n","|    disc/disc_proportion_expert_pred | 0.553    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 0.853    |\n","|    disc/disc_entropy                | 0.455    |\n","|    disc/disc_loss                   | 0.294    |\n","|    disc/disc_proportion_expert_pred | 0.543    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.901    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 0.853    |\n","|    disc/disc_entropy                | 0.442    |\n","|    disc/disc_loss                   | 0.28     |\n","|    disc/disc_proportion_expert_pred | 0.548    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.9      |\n","|    disc/disc_acc_expert             | 0.931    |\n","|    disc/disc_acc_gen                | 0.87     |\n","|    disc/disc_entropy                | 0.436    |\n","|    disc/disc_loss                   | 0.286    |\n","|    disc/disc_proportion_expert_pred | 0.53     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.854    |\n","|    disc/disc_entropy                | 0.433    |\n","|    disc/disc_loss                   | 0.283    |\n","|    disc/disc_proportion_expert_pred | 0.542    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.923    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.897    |\n","|    disc/disc_entropy                | 0.419    |\n","|    disc/disc_loss                   | 0.258    |\n","|    disc/disc_proportion_expert_pred | 0.525    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.898    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.854    |\n","|    disc/disc_entropy                | 0.447    |\n","|    disc/disc_loss                   | 0.288    |\n","|    disc/disc_proportion_expert_pred | 0.545    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -370     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 601      |\n","|    gen/time/fps                     | 2.47e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.47e+05 |\n","|    gen/train/approx_kl              | 0.111    |\n","|    gen/train/clip_fraction          | 0.542    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -8       |\n","|    gen/train/explained_variance     | 0.944    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0271  |\n","|    gen/train/n_updates              | 900      |\n","|    gen/train/policy_gradient_loss   | -0.0689  |\n","|    gen/train/std                    | 0.914    |\n","|    gen/train/value_loss             | 0.193    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  50%|█████     | 9/18 [17:19<17:21, 115.71s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -381       |\n","|    gen/rollout/ep_rew_wrapped_mean | 564        |\n","|    gen/time/fps                    | 2721       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 163840     |\n","|    gen/train/approx_kl             | 0.11138485 |\n","|    gen/train/clip_fraction         | 0.542      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -8         |\n","|    gen/train/explained_variance    | 0.944      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0271    |\n","|    gen/train/n_updates             | 900        |\n","|    gen/train/policy_gradient_loss  | -0.0689    |\n","|    gen/train/std                   | 0.914      |\n","|    gen/train/value_loss            | 0.193      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.837    |\n","|    disc/disc_entropy                | 0.448    |\n","|    disc/disc_loss                   | 0.303    |\n","|    disc/disc_proportion_expert_pred | 0.555    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.944    |\n","|    disc/disc_acc_gen                | 0.84     |\n","|    disc/disc_entropy                | 0.445    |\n","|    disc/disc_loss                   | 0.299    |\n","|    disc/disc_proportion_expert_pred | 0.552    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.912    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.877    |\n","|    disc/disc_entropy                | 0.438    |\n","|    disc/disc_loss                   | 0.282    |\n","|    disc/disc_proportion_expert_pred | 0.535    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.921    |\n","|    disc/disc_acc_expert             | 0.941    |\n","|    disc/disc_acc_gen                | 0.9      |\n","|    disc/disc_entropy                | 0.437    |\n","|    disc/disc_loss                   | 0.29     |\n","|    disc/disc_proportion_expert_pred | 0.521    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.926    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.906    |\n","|    disc/disc_entropy                | 0.428    |\n","|    disc/disc_loss                   | 0.274    |\n","|    disc/disc_proportion_expert_pred | 0.52     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.931    |\n","|    disc/disc_entropy                | 0.43     |\n","|    disc/disc_loss                   | 0.269    |\n","|    disc/disc_proportion_expert_pred | 0.508    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.939    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.933    |\n","|    disc/disc_entropy                | 0.423    |\n","|    disc/disc_loss                   | 0.262    |\n","|    disc/disc_proportion_expert_pred | 0.506    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.951    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.949    |\n","|    disc/disc_entropy                | 0.419    |\n","|    disc/disc_loss                   | 0.256    |\n","|    disc/disc_proportion_expert_pred | 0.502    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.921    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 0.897    |\n","|    disc/disc_entropy                | 0.434    |\n","|    disc/disc_loss                   | 0.279    |\n","|    disc/disc_proportion_expert_pred | 0.525    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -381     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 564      |\n","|    gen/time/fps                     | 2.72e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.64e+05 |\n","|    gen/train/approx_kl              | 0.125    |\n","|    gen/train/clip_fraction          | 0.574    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.9     |\n","|    gen/train/explained_variance     | 0.966    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | 0.0141   |\n","|    gen/train/n_updates              | 1e+03    |\n","|    gen/train/policy_gradient_loss   | -0.0713  |\n","|    gen/train/std                    | 0.901    |\n","|    gen/train/value_loss             | 0.255    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  56%|█████▌    | 10/18 [19:14<15:24, 115.52s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -373       |\n","|    gen/rollout/ep_rew_wrapped_mean | 525        |\n","|    gen/time/fps                    | 2484       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 180224     |\n","|    gen/train/approx_kl             | 0.12542373 |\n","|    gen/train/clip_fraction         | 0.574      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.9       |\n","|    gen/train/explained_variance    | 0.966      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | 0.0141     |\n","|    gen/train/n_updates             | 1000       |\n","|    gen/train/policy_gradient_loss  | -0.0713    |\n","|    gen/train/std                   | 0.901      |\n","|    gen/train/value_loss            | 0.255      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.902    |\n","|    disc/disc_acc_expert             | 0.929    |\n","|    disc/disc_acc_gen                | 0.876    |\n","|    disc/disc_entropy                | 0.452    |\n","|    disc/disc_loss                   | 0.324    |\n","|    disc/disc_proportion_expert_pred | 0.526    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.922    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.907    |\n","|    disc/disc_entropy                | 0.442    |\n","|    disc/disc_loss                   | 0.303    |\n","|    disc/disc_proportion_expert_pred | 0.515    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.935    |\n","|    disc/disc_acc_expert             | 0.951    |\n","|    disc/disc_acc_gen                | 0.919    |\n","|    disc/disc_entropy                | 0.441    |\n","|    disc/disc_loss                   | 0.285    |\n","|    disc/disc_proportion_expert_pred | 0.516    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.944    |\n","|    disc/disc_acc_expert             | 0.94     |\n","|    disc/disc_acc_gen                | 0.948    |\n","|    disc/disc_entropy                | 0.438    |\n","|    disc/disc_loss                   | 0.287    |\n","|    disc/disc_proportion_expert_pred | 0.496    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.944    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.949    |\n","|    disc/disc_entropy                | 0.432    |\n","|    disc/disc_loss                   | 0.279    |\n","|    disc/disc_proportion_expert_pred | 0.495    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.945    |\n","|    disc/disc_acc_expert             | 0.933    |\n","|    disc/disc_acc_gen                | 0.957    |\n","|    disc/disc_entropy                | 0.429    |\n","|    disc/disc_loss                   | 0.274    |\n","|    disc/disc_proportion_expert_pred | 0.488    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.954    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.971    |\n","|    disc/disc_entropy                | 0.425    |\n","|    disc/disc_loss                   | 0.266    |\n","|    disc/disc_proportion_expert_pred | 0.483    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.953    |\n","|    disc/disc_acc_expert             | 0.93     |\n","|    disc/disc_acc_gen                | 0.977    |\n","|    disc/disc_entropy                | 0.423    |\n","|    disc/disc_loss                   | 0.259    |\n","|    disc/disc_proportion_expert_pred | 0.477    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.938    |\n","|    disc/disc_acc_expert             | 0.937    |\n","|    disc/disc_acc_gen                | 0.938    |\n","|    disc/disc_entropy                | 0.435    |\n","|    disc/disc_loss                   | 0.285    |\n","|    disc/disc_proportion_expert_pred | 0.5      |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -373     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 525      |\n","|    gen/time/fps                     | 2.48e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.8e+05  |\n","|    gen/train/approx_kl              | 0.11     |\n","|    gen/train/clip_fraction          | 0.563    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.83    |\n","|    gen/train/explained_variance     | 0.963    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.00291 |\n","|    gen/train/n_updates              | 1.1e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0683  |\n","|    gen/train/std                    | 0.888    |\n","|    gen/train/value_loss             | 0.195    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  61%|██████    | 11/18 [21:10<13:29, 115.64s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                               |           |\n","|    gen/rollout/ep_len_mean         | 1e+03     |\n","|    gen/rollout/ep_rew_mean         | -365      |\n","|    gen/rollout/ep_rew_wrapped_mean | 473       |\n","|    gen/time/fps                    | 2535      |\n","|    gen/time/iterations             | 1         |\n","|    gen/time/time_elapsed           | 6         |\n","|    gen/time/total_timesteps        | 196608    |\n","|    gen/train/approx_kl             | 0.1098777 |\n","|    gen/train/clip_fraction         | 0.563     |\n","|    gen/train/clip_range            | 0.2       |\n","|    gen/train/entropy_loss          | -7.83     |\n","|    gen/train/explained_variance    | 0.963     |\n","|    gen/train/learning_rate         | 0.0004    |\n","|    gen/train/loss                  | -0.00291  |\n","|    gen/train/n_updates             | 1100      |\n","|    gen/train/policy_gradient_loss  | -0.0683   |\n","|    gen/train/std                   | 0.888     |\n","|    gen/train/value_loss            | 0.195     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.936    |\n","|    disc/disc_acc_gen                | 0.999    |\n","|    disc/disc_entropy                | 0.32     |\n","|    disc/disc_loss                   | 0.177    |\n","|    disc/disc_proportion_expert_pred | 0.468    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.964    |\n","|    disc/disc_acc_expert             | 0.928    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.32     |\n","|    disc/disc_loss                   | 0.182    |\n","|    disc/disc_proportion_expert_pred | 0.464    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.314    |\n","|    disc/disc_loss                   | 0.163    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.969    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.305    |\n","|    disc/disc_loss                   | 0.165    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.97     |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.302    |\n","|    disc/disc_loss                   | 0.164    |\n","|    disc/disc_proportion_expert_pred | 0.47     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.963    |\n","|    disc/disc_acc_expert             | 0.927    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.293    |\n","|    disc/disc_loss                   | 0.175    |\n","|    disc/disc_proportion_expert_pred | 0.463    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.969    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.282    |\n","|    disc/disc_loss                   | 0.158    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.281    |\n","|    disc/disc_loss                   | 0.151    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.937    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.302    |\n","|    disc/disc_loss                   | 0.167    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -365     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 473      |\n","|    gen/time/fps                     | 2.54e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 1.97e+05 |\n","|    gen/train/approx_kl              | 0.104    |\n","|    gen/train/clip_fraction          | 0.535    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.76    |\n","|    gen/train/explained_variance     | 0.909    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0346  |\n","|    gen/train/n_updates              | 1.2e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0666  |\n","|    gen/train/std                    | 0.879    |\n","|    gen/train/value_loss             | 0.156    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  67%|██████▋   | 12/18 [23:06<11:34, 115.78s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -358       |\n","|    gen/rollout/ep_rew_wrapped_mean | 411        |\n","|    gen/time/fps                    | 2696       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 212992     |\n","|    gen/train/approx_kl             | 0.10364252 |\n","|    gen/train/clip_fraction         | 0.535      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.76      |\n","|    gen/train/explained_variance    | 0.909      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0346    |\n","|    gen/train/n_updates             | 1200       |\n","|    gen/train/policy_gradient_loss  | -0.0666    |\n","|    gen/train/std                   | 0.879      |\n","|    gen/train/value_loss            | 0.156      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.961    |\n","|    disc/disc_acc_expert             | 0.931    |\n","|    disc/disc_acc_gen                | 0.992    |\n","|    disc/disc_entropy                | 0.35     |\n","|    disc/disc_loss                   | 0.214    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.339    |\n","|    disc/disc_loss                   | 0.204    |\n","|    disc/disc_proportion_expert_pred | 0.471    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.994    |\n","|    disc/disc_entropy                | 0.335    |\n","|    disc/disc_loss                   | 0.191    |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.97     |\n","|    disc/disc_acc_expert             | 0.944    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.331    |\n","|    disc/disc_loss                   | 0.187    |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.962    |\n","|    disc/disc_acc_expert             | 0.932    |\n","|    disc/disc_acc_gen                | 0.992    |\n","|    disc/disc_entropy                | 0.329    |\n","|    disc/disc_loss                   | 0.203    |\n","|    disc/disc_proportion_expert_pred | 0.47     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.319    |\n","|    disc/disc_loss                   | 0.192    |\n","|    disc/disc_proportion_expert_pred | 0.472    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.317    |\n","|    disc/disc_loss                   | 0.189    |\n","|    disc/disc_proportion_expert_pred | 0.471    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.978    |\n","|    disc/disc_acc_expert             | 0.956    |\n","|    disc/disc_acc_gen                | 0.999    |\n","|    disc/disc_entropy                | 0.308    |\n","|    disc/disc_loss                   | 0.165    |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.94     |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.329    |\n","|    disc/disc_loss                   | 0.193    |\n","|    disc/disc_proportion_expert_pred | 0.472    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 13       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -358     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 411      |\n","|    gen/time/fps                     | 2.7e+03  |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 2.13e+05 |\n","|    gen/train/approx_kl              | 0.115    |\n","|    gen/train/clip_fraction          | 0.551    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.68    |\n","|    gen/train/explained_variance     | 0.916    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0254  |\n","|    gen/train/n_updates              | 1.3e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0666  |\n","|    gen/train/std                    | 0.867    |\n","|    gen/train/value_loss             | 0.114    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  72%|███████▏  | 13/18 [25:01<09:38, 115.77s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -345       |\n","|    gen/rollout/ep_rew_wrapped_mean | 354        |\n","|    gen/time/fps                    | 2456       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 229376     |\n","|    gen/train/approx_kl             | 0.11469969 |\n","|    gen/train/clip_fraction         | 0.551      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.68      |\n","|    gen/train/explained_variance    | 0.916      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0254    |\n","|    gen/train/n_updates             | 1300       |\n","|    gen/train/policy_gradient_loss  | -0.0666    |\n","|    gen/train/std                   | 0.867      |\n","|    gen/train/value_loss            | 0.114      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.986    |\n","|    disc/disc_entropy                | 0.324    |\n","|    disc/disc_loss                   | 0.193    |\n","|    disc/disc_proportion_expert_pred | 0.481    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.965    |\n","|    disc/disc_acc_expert             | 0.944    |\n","|    disc/disc_acc_gen                | 0.986    |\n","|    disc/disc_entropy                | 0.324    |\n","|    disc/disc_loss                   | 0.193    |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.966    |\n","|    disc/disc_acc_expert             | 0.941    |\n","|    disc/disc_acc_gen                | 0.99     |\n","|    disc/disc_entropy                | 0.316    |\n","|    disc/disc_loss                   | 0.187    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.989    |\n","|    disc/disc_entropy                | 0.314    |\n","|    disc/disc_loss                   | 0.187    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.306    |\n","|    disc/disc_loss                   | 0.186    |\n","|    disc/disc_proportion_expert_pred | 0.471    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.966    |\n","|    disc/disc_acc_expert             | 0.937    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.305    |\n","|    disc/disc_loss                   | 0.189    |\n","|    disc/disc_proportion_expert_pred | 0.471    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.966    |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.99     |\n","|    disc/disc_entropy                | 0.296    |\n","|    disc/disc_loss                   | 0.174    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.94     |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.293    |\n","|    disc/disc_loss                   | 0.174    |\n","|    disc/disc_proportion_expert_pred | 0.472    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.991    |\n","|    disc/disc_entropy                | 0.31     |\n","|    disc/disc_loss                   | 0.185    |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 14       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -345     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 354      |\n","|    gen/time/fps                     | 2.46e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 2.29e+05 |\n","|    gen/train/approx_kl              | 0.115    |\n","|    gen/train/clip_fraction          | 0.555    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.58    |\n","|    gen/train/explained_variance     | 0.924    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0335  |\n","|    gen/train/n_updates              | 1.4e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0654  |\n","|    gen/train/std                    | 0.852    |\n","|    gen/train/value_loss             | 0.121    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  78%|███████▊  | 14/18 [26:58<07:43, 115.97s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -331       |\n","|    gen/rollout/ep_rew_wrapped_mean | 300        |\n","|    gen/time/fps                    | 2378       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 245760     |\n","|    gen/train/approx_kl             | 0.11547151 |\n","|    gen/train/clip_fraction         | 0.555      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.58      |\n","|    gen/train/explained_variance    | 0.924      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0335    |\n","|    gen/train/n_updates             | 1400       |\n","|    gen/train/policy_gradient_loss  | -0.0654    |\n","|    gen/train/std                   | 0.852      |\n","|    gen/train/value_loss            | 0.121      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.97     |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.3      |\n","|    disc/disc_loss                   | 0.178    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.941    |\n","|    disc/disc_acc_gen                | 0.992    |\n","|    disc/disc_entropy                | 0.304    |\n","|    disc/disc_loss                   | 0.185    |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.964    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 0.991    |\n","|    disc/disc_entropy                | 0.294    |\n","|    disc/disc_loss                   | 0.177    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.969    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.289    |\n","|    disc/disc_loss                   | 0.173    |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.974    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.282    |\n","|    disc/disc_loss                   | 0.16     |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.279    |\n","|    disc/disc_loss                   | 0.162    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.271    |\n","|    disc/disc_loss                   | 0.156    |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.274    |\n","|    disc/disc_loss                   | 0.153    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.97     |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.287    |\n","|    disc/disc_loss                   | 0.168    |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 15       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -331     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 300      |\n","|    gen/time/fps                     | 2.38e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 2.46e+05 |\n","|    gen/train/approx_kl              | 0.119    |\n","|    gen/train/clip_fraction          | 0.564    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.49    |\n","|    gen/train/explained_variance     | 0.912    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0227  |\n","|    gen/train/n_updates              | 1.5e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0644  |\n","|    gen/train/std                    | 0.839    |\n","|    gen/train/value_loss             | 0.0802   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  83%|████████▎ | 15/18 [28:54<05:48, 116.01s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -307       |\n","|    gen/rollout/ep_rew_wrapped_mean | 244        |\n","|    gen/time/fps                    | 2750       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 5          |\n","|    gen/time/total_timesteps        | 262144     |\n","|    gen/train/approx_kl             | 0.11865258 |\n","|    gen/train/clip_fraction         | 0.564      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.49      |\n","|    gen/train/explained_variance    | 0.912      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0227    |\n","|    gen/train/n_updates             | 1500       |\n","|    gen/train/policy_gradient_loss  | -0.0644    |\n","|    gen/train/std                   | 0.839      |\n","|    gen/train/value_loss            | 0.0802     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.974    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.164    |\n","|    disc/disc_loss                   | 0.104    |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.969    |\n","|    disc/disc_acc_expert             | 0.938    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.161    |\n","|    disc/disc_loss                   | 0.105    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.966    |\n","|    disc/disc_acc_expert             | 0.932    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.161    |\n","|    disc/disc_loss                   | 0.114    |\n","|    disc/disc_proportion_expert_pred | 0.466    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.974    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.152    |\n","|    disc/disc_loss                   | 0.0954   |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.975    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.151    |\n","|    disc/disc_loss                   | 0.0941   |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.952    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.153    |\n","|    disc/disc_loss                   | 0.0851   |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.974    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.148    |\n","|    disc/disc_loss                   | 0.0931   |\n","|    disc/disc_proportion_expert_pred | 0.474    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.946    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.149    |\n","|    disc/disc_loss                   | 0.0882   |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.973    |\n","|    disc/disc_acc_expert             | 0.945    |\n","|    disc/disc_acc_gen                | 1        |\n","|    disc/disc_entropy                | 0.155    |\n","|    disc/disc_loss                   | 0.0974   |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 16       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -307     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 244      |\n","|    gen/time/fps                     | 2.75e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 2.62e+05 |\n","|    gen/train/approx_kl              | 0.12     |\n","|    gen/train/clip_fraction          | 0.56     |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.4     |\n","|    gen/train/explained_variance     | 0.841    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0616  |\n","|    gen/train/n_updates              | 1.6e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0644  |\n","|    gen/train/std                    | 0.828    |\n","|    gen/train/value_loss             | 0.0741   |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  89%|████████▉ | 16/18 [30:50<03:51, 115.95s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -298       |\n","|    gen/rollout/ep_rew_wrapped_mean | 200        |\n","|    gen/time/fps                    | 2584       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 278528     |\n","|    gen/train/approx_kl             | 0.12011823 |\n","|    gen/train/clip_fraction         | 0.56       |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.4       |\n","|    gen/train/explained_variance    | 0.841      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0616    |\n","|    gen/train/n_updates             | 1600       |\n","|    gen/train/policy_gradient_loss  | -0.0644    |\n","|    gen/train/std                   | 0.828      |\n","|    gen/train/value_loss            | 0.0741     |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.939    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.24     |\n","|    disc/disc_loss                   | 0.149    |\n","|    disc/disc_proportion_expert_pred | 0.471    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.966    |\n","|    disc/disc_acc_expert             | 0.937    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.235    |\n","|    disc/disc_loss                   | 0.149    |\n","|    disc/disc_proportion_expert_pred | 0.47     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.968    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.993    |\n","|    disc/disc_entropy                | 0.232    |\n","|    disc/disc_loss                   | 0.138    |\n","|    disc/disc_proportion_expert_pred | 0.475    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.972    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 0.994    |\n","|    disc/disc_entropy                | 0.225    |\n","|    disc/disc_loss                   | 0.126    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.967    |\n","|    disc/disc_acc_expert             | 0.94     |\n","|    disc/disc_acc_gen                | 0.994    |\n","|    disc/disc_entropy                | 0.224    |\n","|    disc/disc_loss                   | 0.134    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.957    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.216    |\n","|    disc/disc_loss                   | 0.123    |\n","|    disc/disc_proportion_expert_pred | 0.481    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.955    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.216    |\n","|    disc/disc_loss                   | 0.119    |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.975    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.997    |\n","|    disc/disc_entropy                | 0.219    |\n","|    disc/disc_loss                   | 0.123    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.971    |\n","|    disc/disc_acc_expert             | 0.947    |\n","|    disc/disc_acc_gen                | 0.995    |\n","|    disc/disc_entropy                | 0.226    |\n","|    disc/disc_loss                   | 0.133    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 17       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -298     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 200      |\n","|    gen/time/fps                     | 2.58e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 2.79e+05 |\n","|    gen/train/approx_kl              | 0.136    |\n","|    gen/train/clip_fraction          | 0.583    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.32    |\n","|    gen/train/explained_variance     | 0.828    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0245  |\n","|    gen/train/n_updates              | 1.7e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0663  |\n","|    gen/train/std                    | 0.818    |\n","|    gen/train/value_loss             | 0.105    |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  94%|█████████▍| 17/18 [32:45<01:55, 115.84s/it]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------------------------\n","| raw/                               |            |\n","|    gen/rollout/ep_len_mean         | 1e+03      |\n","|    gen/rollout/ep_rew_mean         | -282       |\n","|    gen/rollout/ep_rew_wrapped_mean | 161        |\n","|    gen/time/fps                    | 2502       |\n","|    gen/time/iterations             | 1          |\n","|    gen/time/time_elapsed           | 6          |\n","|    gen/time/total_timesteps        | 294912     |\n","|    gen/train/approx_kl             | 0.13630891 |\n","|    gen/train/clip_fraction         | 0.583      |\n","|    gen/train/clip_range            | 0.2        |\n","|    gen/train/entropy_loss          | -7.32      |\n","|    gen/train/explained_variance    | 0.828      |\n","|    gen/train/learning_rate         | 0.0004     |\n","|    gen/train/loss                  | -0.0245    |\n","|    gen/train/n_updates             | 1700       |\n","|    gen/train/policy_gradient_loss  | -0.0663    |\n","|    gen/train/std                   | 0.818      |\n","|    gen/train/value_loss            | 0.105      |\n","---------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.2      |\n","|    disc/disc_loss                   | 0.114    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.979    |\n","|    disc/disc_acc_expert             | 0.958    |\n","|    disc/disc_acc_gen                | 0.999    |\n","|    disc/disc_entropy                | 0.201    |\n","|    disc/disc_loss                   | 0.106    |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.199    |\n","|    disc/disc_loss                   | 0.112    |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.969    |\n","|    disc/disc_acc_expert             | 0.942    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.203    |\n","|    disc/disc_loss                   | 0.125    |\n","|    disc/disc_proportion_expert_pred | 0.473    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.974    |\n","|    disc/disc_acc_expert             | 0.949    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.192    |\n","|    disc/disc_loss                   | 0.106    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.976    |\n","|    disc/disc_acc_expert             | 0.953    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.186    |\n","|    disc/disc_loss                   | 0.11     |\n","|    disc/disc_proportion_expert_pred | 0.478    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.975    |\n","|    disc/disc_acc_expert             | 0.95     |\n","|    disc/disc_acc_gen                | 0.999    |\n","|    disc/disc_entropy                | 0.189    |\n","|    disc/disc_loss                   | 0.111    |\n","|    disc/disc_proportion_expert_pred | 0.476    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.975    |\n","|    disc/disc_acc_expert             | 0.954    |\n","|    disc/disc_acc_gen                | 0.996    |\n","|    disc/disc_entropy                | 0.179    |\n","|    disc/disc_loss                   | 0.105    |\n","|    disc/disc_proportion_expert_pred | 0.479    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.975    |\n","|    disc/disc_acc_expert             | 0.952    |\n","|    disc/disc_acc_gen                | 0.998    |\n","|    disc/disc_entropy                | 0.193    |\n","|    disc/disc_loss                   | 0.111    |\n","|    disc/disc_proportion_expert_pred | 0.477    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 18       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 1e+03    |\n","|    gen/rollout/ep_rew_mean          | -282     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 161      |\n","|    gen/time/fps                     | 2.5e+03  |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 6        |\n","|    gen/time/total_timesteps         | 2.95e+05 |\n","|    gen/train/approx_kl              | 0.162    |\n","|    gen/train/clip_fraction          | 0.611    |\n","|    gen/train/clip_range             | 0.2      |\n","|    gen/train/entropy_loss           | -7.22    |\n","|    gen/train/explained_variance     | 0.772    |\n","|    gen/train/learning_rate          | 0.0004   |\n","|    gen/train/loss                   | -0.0332  |\n","|    gen/train/n_updates              | 1.8e+03  |\n","|    gen/train/policy_gradient_loss   | -0.0664  |\n","|    gen/train/std                    | 0.802    |\n","|    gen/train/value_loss             | 0.16     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["round: 100%|██████████| 18/18 [34:41<00:00, 115.62s/it]\n"]}]},{"cell_type":"code","source":["env.seed(SEED)\n","learner_rewards_after_training, _ = evaluate_policy(\n","    learner, env, 20, return_episode_rewards=True\n",")"],"metadata":{"id":"haYV1jHz06QA","executionInfo":{"status":"ok","timestamp":1709637194042,"user_tz":0,"elapsed":5006,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(\n","    \"Rewards before training:\",\n","    np.mean(learner_rewards_before_training),\n","    \"+/-\",\n","    np.std(learner_rewards_before_training),\n",")\n","print(\n","    \"Rewards after training:\",\n","    np.mean(learner_rewards_after_training),\n","    \"+/-\",\n","    np.std(learner_rewards_after_training),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4EYIO-Eo24Ht","executionInfo":{"status":"ok","timestamp":1709637194043,"user_tz":0,"elapsed":13,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"fa337ddc-cb33-41f7-bca4-cb02f9102486"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Rewards before training: -0.30919664999999996 +/- 0.5729603050948011\n","Rewards after training: 94.83383855 +/- 71.99733160835072\n"]}]},{"cell_type":"code","source":["expert_reward, _ = evaluate_policy(\n","    expert, env, 8, return_episode_rewards=True\n",")"],"metadata":{"id":"c1OvkK4lXZno","executionInfo":{"status":"ok","timestamp":1709634888053,"user_tz":0,"elapsed":1582,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(\n","    \"expert rewards after training:\",\n","    np.mean(expert_reward),\n","    \"+/-\",\n","    np.std(expert_reward),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN5085MhXhvA","executionInfo":{"status":"ok","timestamp":1709634888053,"user_tz":0,"elapsed":4,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"ffc493e1-5b5d-47e5-e220-4aa6417e175b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["expert rewards after training: 1680.73820975 +/- 27.078724068659696\n"]}]},{"cell_type":"markdown","source":["Cartpole\n","\n","Rewards before training: 102.6 +/- 24.11514047232568\n","Rewards after training: 347.56 +/- 142.09801687567634\n","\n","HaleCheetah\n","\n","Rewards before training: -0.29786967999999997 +/- 0.6720326541318641\n","Rewards after training: 867.3461561099998 +/- 478.52644840867737\n","expert平均1669，看看是步数的问题还是别的参数。参考GAIL里面expert的结果，4000-5000分最终，运行300000到1000分，600000到1500，,2000000到2500"],"metadata":{"id":"aEBB_6Fcpll6"}},{"cell_type":"code","source":[],"metadata":{"id":"bwLkNhHNO_Y-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GAIL-TRPO"],"metadata":{"id":"otF25OWCBy8i"}},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","from imitation.policies.serialize import load_policy\n","from imitation.util.util import make_vec_env\n","from imitation.data.wrappers import RolloutInfoWrapper"],"metadata":{"id":"30aP9Y8wB0c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED=42\n","env_TRPO = make_vec_env(\n","    \"CartPole-v1\",\n","    rng=np.random.default_rng(SEED),\n","    n_envs=8,\n","    post_wrappers=[\n","        lambda env, _: RolloutInfoWrapper(env)\n","    ],  # needed for computing rollouts later\n",")"],"metadata":{"id":"cUW_MjJhCTxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# seal cartpole v0: 别用它那个，要么从huggingface上下个看看，现在这个有ValueError: Observation spaces do not match\n","# 现在换了 cartpole v1\n","expert_TRPO = load_policy(\n","    \"ppo-huggingface\",\n","    organization=\"HumanCompatibleAI\",\n","    env_name=\"CartPole-v1\",\n","    venv=env_TRPO,\n",")"],"metadata":{"id":"orZsgnRPFjzy","executionInfo":{"status":"ok","timestamp":1709119162680,"user_tz":0,"elapsed":3185,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["8c5f9758f2d24d7b892ae25b32bfb229","a9c0d6633be94b0dafa8e1f27850f157","465e014d41084c61827a769efce628ed","bcf811c9829d41d79b7887d7d243bf16","b05f9a9d5c6a4914bcd67118b2631b5d","8c4c25b219e346eab995c67a8c614e67","9879171c171f4fc6a90d8db6d8b1a27a","d4b4b155a70846c78747fc9bbf02390a","6d36dfd26a26411784a68860a3843469","b242638ffdef49bb9de336c7342924c6","eedbfc0c0e544160950e35829e5003cf","8d4a0870d1d147b48120ed98f13ee9cd","0013b516b31e489facf05d7b3fbe5f28","d75cd4fbd67d4d8daa5a0ac66e0119d5","3a5fc6693eae4936a824e85ffb324c6e","92592cc827bb4ef782cfb57f1b4708e1","8219e279d3624abcb2ab94b76997eb67","a7913eaec1994021bbdc507a2ce6544d","1f109c148daf4d9aba198ec99021dbf3","9a5b76f3e5d64581a5d3537683784701","c990af05e706431280ac8f8e3d86c7ba","be139469c4224fa4977faab55de0c11d"]},"outputId":"9158cdff-011d-4228-f0ac-25e6b1a6a78b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["ppo-CartPole-v1.zip:   0%|          | 0.00/139k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5f9758f2d24d7b892ae25b32bfb229"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4a0870d1d147b48120ed98f13ee9cd"}},"metadata":{}}]},{"cell_type":"code","source":["from imitation.data import rollout\n","\n","rollouts_TRPO = rollout.rollout(\n","    expert_TRPO,\n","    env_TRPO,\n","    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n","    rng=np.random.default_rng(SEED), # 随机数？ The random state to use for sampling trajectories.\n",")"],"metadata":{"id":"IjKQUV2eO0uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from imitation.algorithms.adversarial.gail import GAIL\n","from imitation.rewards.reward_nets import BasicRewardNet\n","from imitation.util.networks import RunningNorm\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import MlpPolicy\n","from stable_baselines3.common.evaluation import evaluate_policy\n","import sb3_contrib\n","from sb3_contrib import TRPO, RecurrentPPO\n","\n","learner_TRPO = TRPO(\"MlpPolicy\", env_TRPO, verbose=1) # RecurrentPPO(\"MlpLstmPolicy\", env_TRPO, verbose=1)\n","\n","reward_net = BasicRewardNet(\n","    observation_space=env_TRPO.observation_space,\n","    action_space=env_TRPO.action_space,\n","    normalize_input_layer=RunningNorm,\n",")\n","\n","gail_trainer_TRPO = GAIL(\n","    demonstrations=rollouts_TRPO,\n","    demo_batch_size=1024,\n","    gen_replay_buffer_capacity=512,\n","    n_disc_updates_per_round=8,\n","    venv=env_TRPO,\n","    gen_algo=learner_TRPO,\n","    reward_net=reward_net,\n","    allow_variable_horizon=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_A73HAlAHXWd","executionInfo":{"status":"ok","timestamp":1709124034362,"user_tz":0,"elapsed":208,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"273dfde8-b5ef-49bf-80c1-a2b266d13c35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Running with `allow_variable_horizon` set to True. Some algorithms are biased towards shorter or longer episodes, which may significantly confound results. Additionally, even unbiased algorithms can exploit the information leak from the termination condition, producing spuriously high performance. See https://imitation.readthedocs.io/en/latest/getting-started/variable-horizon.html for more information.\n"]}]},{"cell_type":"code","source":["env_TRPO.seed(SEED)\n","learner_rewards_before_training, _ = evaluate_policy(\n","    learner_TRPO, env_TRPO, 100, return_episode_rewards=True\n",")"],"metadata":{"id":"1Y9kS5J5Pov7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gail_trainer_TRPO.train(200_000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vauFBxgPov7","executionInfo":{"status":"ok","timestamp":1709124148410,"user_tz":0,"elapsed":107472,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"06a45ca5-cb95-431d-9bb7-c0d3caa90d81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rround:   0%|          | 0/12 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------\n","| raw/                        |          |\n","|    gen/rollout/ep_len_mean  | 23.8     |\n","|    gen/rollout/ep_rew_mean  | 23.8     |\n","|    gen/time/fps             | 3700     |\n","|    gen/time/iterations      | 1        |\n","|    gen/time/time_elapsed    | 4        |\n","|    gen/time/total_timesteps | 16384    |\n","------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.621    |\n","|    disc/disc_acc_expert             | 0.993    |\n","|    disc/disc_acc_gen                | 0.248    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.687    |\n","|    disc/disc_proportion_expert_pred | 0.873    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.653    |\n","|    disc/disc_acc_expert             | 0.982    |\n","|    disc/disc_acc_gen                | 0.323    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.686    |\n","|    disc/disc_proportion_expert_pred | 0.83     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.711    |\n","|    disc/disc_acc_expert             | 0.979    |\n","|    disc/disc_acc_gen                | 0.443    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.682    |\n","|    disc/disc_proportion_expert_pred | 0.768    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.751    |\n","|    disc/disc_acc_expert             | 0.957    |\n","|    disc/disc_acc_gen                | 0.546    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.68     |\n","|    disc/disc_proportion_expert_pred | 0.706    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.804    |\n","|    disc/disc_acc_expert             | 0.943    |\n","|    disc/disc_acc_gen                | 0.665    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.677    |\n","|    disc/disc_proportion_expert_pred | 0.639    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.845    |\n","|    disc/disc_acc_expert             | 0.924    |\n","|    disc/disc_acc_gen                | 0.766    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.674    |\n","|    disc/disc_proportion_expert_pred | 0.579    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.85     |\n","|    disc/disc_acc_expert             | 0.904    |\n","|    disc/disc_acc_gen                | 0.796    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.671    |\n","|    disc/disc_proportion_expert_pred | 0.554    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.872    |\n","|    disc/disc_acc_expert             | 0.9      |\n","|    disc/disc_acc_gen                | 0.843    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.669    |\n","|    disc/disc_proportion_expert_pred | 0.529    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.763    |\n","|    disc/disc_acc_expert             | 0.948    |\n","|    disc/disc_acc_gen                | 0.579    |\n","|    disc/disc_entropy                | 0.693    |\n","|    disc/disc_loss                   | 0.678    |\n","|    disc/disc_proportion_expert_pred | 0.685    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 1        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 23.8     |\n","|    gen/rollout/ep_rew_mean          | 23.8     |\n","|    gen/time/fps                     | 3.7e+03  |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 1.64e+04 |\n","|    gen/train/explained_variance     | -0.0111  |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.0084   |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 1        |\n","|    gen/train/policy_objective       | 0.0204   |\n","|    gen/train/value_loss             | 5.23     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:   8%|▊         | 1/12 [00:08<01:29,  8.18s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 29.2     |\n","|    gen/rollout/ep_rew_mean          | 29.2     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 16.6     |\n","|    gen/time/fps                     | 2964     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 32768    |\n","|    gen/train/explained_variance     | -0.0111  |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.0084   |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 1        |\n","|    gen/train/policy_objective       | 0.0204   |\n","|    gen/train/value_loss             | 5.23     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.881    |\n","|    disc/disc_acc_expert             | 0.865    |\n","|    disc/disc_acc_gen                | 0.897    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.661    |\n","|    disc/disc_proportion_expert_pred | 0.484    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.86     |\n","|    disc/disc_acc_gen                | 0.923    |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.658    |\n","|    disc/disc_proportion_expert_pred | 0.469    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.886    |\n","|    disc/disc_acc_expert             | 0.853    |\n","|    disc/disc_acc_gen                | 0.92     |\n","|    disc/disc_entropy                | 0.692    |\n","|    disc/disc_loss                   | 0.656    |\n","|    disc/disc_proportion_expert_pred | 0.466    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.838    |\n","|    disc/disc_acc_gen                | 0.945    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.653    |\n","|    disc/disc_proportion_expert_pred | 0.446    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.828    |\n","|    disc/disc_acc_gen                | 0.963    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.649    |\n","|    disc/disc_proportion_expert_pred | 0.433    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.897    |\n","|    disc/disc_acc_expert             | 0.834    |\n","|    disc/disc_acc_gen                | 0.96     |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.647    |\n","|    disc/disc_proportion_expert_pred | 0.437    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.895    |\n","|    disc/disc_acc_expert             | 0.812    |\n","|    disc/disc_acc_gen                | 0.978    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.645    |\n","|    disc/disc_proportion_expert_pred | 0.417    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.907    |\n","|    disc/disc_acc_expert             | 0.831    |\n","|    disc/disc_acc_gen                | 0.982    |\n","|    disc/disc_entropy                | 0.69     |\n","|    disc/disc_loss                   | 0.639    |\n","|    disc/disc_proportion_expert_pred | 0.424    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.893    |\n","|    disc/disc_acc_expert             | 0.84     |\n","|    disc/disc_acc_gen                | 0.946    |\n","|    disc/disc_entropy                | 0.691    |\n","|    disc/disc_loss                   | 0.651    |\n","|    disc/disc_proportion_expert_pred | 0.447    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 2        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 29.2     |\n","|    gen/rollout/ep_rew_mean          | 29.2     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 16.6     |\n","|    gen/time/fps                     | 2.96e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 3.28e+04 |\n","|    gen/train/explained_variance     | 0.47     |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00819  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 2        |\n","|    gen/train/policy_objective       | 0.0307   |\n","|    gen/train/value_loss             | 9.41     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  17%|█▋        | 2/12 [00:17<01:28,  8.81s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 40.6     |\n","|    gen/rollout/ep_rew_mean          | 40.6     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 18.6     |\n","|    gen/time/fps                     | 3376     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 49152    |\n","|    gen/train/explained_variance     | 0.47     |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00819  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 2        |\n","|    gen/train/policy_objective       | 0.0307   |\n","|    gen/train/value_loss             | 9.41     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.901    |\n","|    disc/disc_acc_expert             | 0.831    |\n","|    disc/disc_acc_gen                | 0.971    |\n","|    disc/disc_entropy                | 0.689    |\n","|    disc/disc_loss                   | 0.637    |\n","|    disc/disc_proportion_expert_pred | 0.43     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.809    |\n","|    disc/disc_acc_gen                | 0.976    |\n","|    disc/disc_entropy                | 0.689    |\n","|    disc/disc_loss                   | 0.634    |\n","|    disc/disc_proportion_expert_pred | 0.417    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.893    |\n","|    disc/disc_acc_expert             | 0.814    |\n","|    disc/disc_acc_gen                | 0.972    |\n","|    disc/disc_entropy                | 0.689    |\n","|    disc/disc_loss                   | 0.631    |\n","|    disc/disc_proportion_expert_pred | 0.421    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.898    |\n","|    disc/disc_acc_expert             | 0.815    |\n","|    disc/disc_acc_gen                | 0.98     |\n","|    disc/disc_entropy                | 0.688    |\n","|    disc/disc_loss                   | 0.629    |\n","|    disc/disc_proportion_expert_pred | 0.417    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.899    |\n","|    disc/disc_acc_expert             | 0.816    |\n","|    disc/disc_acc_gen                | 0.981    |\n","|    disc/disc_entropy                | 0.688    |\n","|    disc/disc_loss                   | 0.625    |\n","|    disc/disc_proportion_expert_pred | 0.417    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.903    |\n","|    disc/disc_acc_expert             | 0.82     |\n","|    disc/disc_acc_gen                | 0.985    |\n","|    disc/disc_entropy                | 0.687    |\n","|    disc/disc_loss                   | 0.621    |\n","|    disc/disc_proportion_expert_pred | 0.417    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.805    |\n","|    disc/disc_acc_gen                | 0.987    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.618    |\n","|    disc/disc_proportion_expert_pred | 0.409    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.9      |\n","|    disc/disc_acc_expert             | 0.816    |\n","|    disc/disc_acc_gen                | 0.984    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.614    |\n","|    disc/disc_proportion_expert_pred | 0.416    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.898    |\n","|    disc/disc_acc_expert             | 0.816    |\n","|    disc/disc_acc_gen                | 0.98     |\n","|    disc/disc_entropy                | 0.688    |\n","|    disc/disc_loss                   | 0.626    |\n","|    disc/disc_proportion_expert_pred | 0.418    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 3        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 40.6     |\n","|    gen/rollout/ep_rew_mean          | 40.6     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 18.6     |\n","|    gen/time/fps                     | 3.38e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 4.92e+04 |\n","|    gen/train/explained_variance     | 0.5      |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00766  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 3        |\n","|    gen/train/policy_objective       | 0.0336   |\n","|    gen/train/value_loss             | 15       |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  25%|██▌       | 3/12 [00:26<01:19,  8.78s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 54.4     |\n","|    gen/rollout/ep_rew_mean          | 54.4     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 24.6     |\n","|    gen/time/fps                     | 3689     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 65536    |\n","|    gen/train/explained_variance     | 0.5      |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00766  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 3        |\n","|    gen/train/policy_objective       | 0.0336   |\n","|    gen/train/value_loss             | 15       |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.89     |\n","|    disc/disc_acc_expert             | 0.809    |\n","|    disc/disc_acc_gen                | 0.971    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.613    |\n","|    disc/disc_proportion_expert_pred | 0.419    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.882    |\n","|    disc/disc_acc_expert             | 0.803    |\n","|    disc/disc_acc_gen                | 0.961    |\n","|    disc/disc_entropy                | 0.686    |\n","|    disc/disc_loss                   | 0.612    |\n","|    disc/disc_proportion_expert_pred | 0.421    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.896    |\n","|    disc/disc_acc_expert             | 0.815    |\n","|    disc/disc_acc_gen                | 0.977    |\n","|    disc/disc_entropy                | 0.685    |\n","|    disc/disc_loss                   | 0.605    |\n","|    disc/disc_proportion_expert_pred | 0.419    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.892    |\n","|    disc/disc_acc_expert             | 0.822    |\n","|    disc/disc_acc_gen                | 0.962    |\n","|    disc/disc_entropy                | 0.685    |\n","|    disc/disc_loss                   | 0.603    |\n","|    disc/disc_proportion_expert_pred | 0.43     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.901    |\n","|    disc/disc_acc_expert             | 0.832    |\n","|    disc/disc_acc_gen                | 0.971    |\n","|    disc/disc_entropy                | 0.683    |\n","|    disc/disc_loss                   | 0.596    |\n","|    disc/disc_proportion_expert_pred | 0.431    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.869    |\n","|    disc/disc_acc_expert             | 0.788    |\n","|    disc/disc_acc_gen                | 0.949    |\n","|    disc/disc_entropy                | 0.683    |\n","|    disc/disc_loss                   | 0.6      |\n","|    disc/disc_proportion_expert_pred | 0.419    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.883    |\n","|    disc/disc_acc_expert             | 0.816    |\n","|    disc/disc_acc_gen                | 0.949    |\n","|    disc/disc_entropy                | 0.682    |\n","|    disc/disc_loss                   | 0.591    |\n","|    disc/disc_proportion_expert_pred | 0.434    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.888    |\n","|    disc/disc_acc_expert             | 0.824    |\n","|    disc/disc_acc_gen                | 0.952    |\n","|    disc/disc_entropy                | 0.682    |\n","|    disc/disc_loss                   | 0.59     |\n","|    disc/disc_proportion_expert_pred | 0.436    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.888    |\n","|    disc/disc_acc_expert             | 0.814    |\n","|    disc/disc_acc_gen                | 0.961    |\n","|    disc/disc_entropy                | 0.684    |\n","|    disc/disc_loss                   | 0.601    |\n","|    disc/disc_proportion_expert_pred | 0.426    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 4        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 54.4     |\n","|    gen/rollout/ep_rew_mean          | 54.4     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 24.6     |\n","|    gen/time/fps                     | 3.69e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 6.55e+04 |\n","|    gen/train/explained_variance     | 0.577    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00727  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 4        |\n","|    gen/train/policy_objective       | 0.0328   |\n","|    gen/train/value_loss             | 15.9     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  33%|███▎      | 4/12 [00:34<01:09,  8.73s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 99.1     |\n","|    gen/rollout/ep_rew_mean          | 99.1     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 30.3     |\n","|    gen/time/fps                     | 3338     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 81920    |\n","|    gen/train/explained_variance     | 0.577    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00727  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 4        |\n","|    gen/train/policy_objective       | 0.0328   |\n","|    gen/train/value_loss             | 15.9     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.924    |\n","|    disc/disc_acc_expert             | 0.854    |\n","|    disc/disc_acc_gen                | 0.993    |\n","|    disc/disc_entropy                | 0.638    |\n","|    disc/disc_loss                   | 0.528    |\n","|    disc/disc_proportion_expert_pred | 0.431    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.912    |\n","|    disc/disc_acc_expert             | 0.845    |\n","|    disc/disc_acc_gen                | 0.979    |\n","|    disc/disc_entropy                | 0.641    |\n","|    disc/disc_loss                   | 0.531    |\n","|    disc/disc_proportion_expert_pred | 0.433    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.915    |\n","|    disc/disc_acc_expert             | 0.859    |\n","|    disc/disc_acc_gen                | 0.97     |\n","|    disc/disc_entropy                | 0.644    |\n","|    disc/disc_loss                   | 0.532    |\n","|    disc/disc_proportion_expert_pred | 0.445    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.919    |\n","|    disc/disc_acc_expert             | 0.88     |\n","|    disc/disc_acc_gen                | 0.958    |\n","|    disc/disc_entropy                | 0.645    |\n","|    disc/disc_loss                   | 0.536    |\n","|    disc/disc_proportion_expert_pred | 0.461    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.922    |\n","|    disc/disc_acc_expert             | 0.887    |\n","|    disc/disc_acc_gen                | 0.958    |\n","|    disc/disc_entropy                | 0.641    |\n","|    disc/disc_loss                   | 0.525    |\n","|    disc/disc_proportion_expert_pred | 0.464    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.913    |\n","|    disc/disc_acc_expert             | 0.902    |\n","|    disc/disc_acc_gen                | 0.923    |\n","|    disc/disc_entropy                | 0.645    |\n","|    disc/disc_loss                   | 0.534    |\n","|    disc/disc_proportion_expert_pred | 0.49     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.908    |\n","|    disc/disc_acc_expert             | 0.902    |\n","|    disc/disc_acc_gen                | 0.913    |\n","|    disc/disc_entropy                | 0.642    |\n","|    disc/disc_loss                   | 0.528    |\n","|    disc/disc_proportion_expert_pred | 0.495    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.91     |\n","|    disc/disc_acc_expert             | 0.906    |\n","|    disc/disc_acc_gen                | 0.913    |\n","|    disc/disc_entropy                | 0.642    |\n","|    disc/disc_loss                   | 0.526    |\n","|    disc/disc_proportion_expert_pred | 0.497    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.915    |\n","|    disc/disc_acc_expert             | 0.88     |\n","|    disc/disc_acc_gen                | 0.951    |\n","|    disc/disc_entropy                | 0.642    |\n","|    disc/disc_loss                   | 0.53     |\n","|    disc/disc_proportion_expert_pred | 0.464    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 5        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 99.1     |\n","|    gen/rollout/ep_rew_mean          | 99.1     |\n","|    gen/rollout/ep_rew_wrapped_mean  | 30.3     |\n","|    gen/time/fps                     | 3.34e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 8.19e+04 |\n","|    gen/train/explained_variance     | 0.687    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.007    |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 5        |\n","|    gen/train/policy_objective       | 0.0262   |\n","|    gen/train/value_loss             | 13.4     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  42%|████▏     | 5/12 [00:43<01:00,  8.71s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 142      |\n","|    gen/rollout/ep_rew_mean          | 142      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 48.4     |\n","|    gen/time/fps                     | 2939     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 98304    |\n","|    gen/train/explained_variance     | 0.687    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.007    |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 5        |\n","|    gen/train/policy_objective       | 0.0262   |\n","|    gen/train/value_loss             | 13.4     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.783    |\n","|    disc/disc_acc_expert             | 0.906    |\n","|    disc/disc_acc_gen                | 0.66     |\n","|    disc/disc_entropy                | 0.682    |\n","|    disc/disc_loss                   | 0.609    |\n","|    disc/disc_proportion_expert_pred | 0.623    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.78     |\n","|    disc/disc_acc_expert             | 0.904    |\n","|    disc/disc_acc_gen                | 0.656    |\n","|    disc/disc_entropy                | 0.681    |\n","|    disc/disc_loss                   | 0.608    |\n","|    disc/disc_proportion_expert_pred | 0.624    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.806    |\n","|    disc/disc_acc_expert             | 0.917    |\n","|    disc/disc_acc_gen                | 0.694    |\n","|    disc/disc_entropy                | 0.68     |\n","|    disc/disc_loss                   | 0.599    |\n","|    disc/disc_proportion_expert_pred | 0.611    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.789    |\n","|    disc/disc_acc_expert             | 0.911    |\n","|    disc/disc_acc_gen                | 0.666    |\n","|    disc/disc_entropy                | 0.678    |\n","|    disc/disc_loss                   | 0.597    |\n","|    disc/disc_proportion_expert_pred | 0.623    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.794    |\n","|    disc/disc_acc_expert             | 0.904    |\n","|    disc/disc_acc_gen                | 0.684    |\n","|    disc/disc_entropy                | 0.679    |\n","|    disc/disc_loss                   | 0.599    |\n","|    disc/disc_proportion_expert_pred | 0.61     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.819    |\n","|    disc/disc_acc_expert             | 0.902    |\n","|    disc/disc_acc_gen                | 0.736    |\n","|    disc/disc_entropy                | 0.676    |\n","|    disc/disc_loss                   | 0.585    |\n","|    disc/disc_proportion_expert_pred | 0.583    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.809    |\n","|    disc/disc_acc_expert             | 0.911    |\n","|    disc/disc_acc_gen                | 0.707    |\n","|    disc/disc_entropy                | 0.676    |\n","|    disc/disc_loss                   | 0.587    |\n","|    disc/disc_proportion_expert_pred | 0.602    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.817    |\n","|    disc/disc_acc_expert             | 0.911    |\n","|    disc/disc_acc_gen                | 0.724    |\n","|    disc/disc_entropy                | 0.675    |\n","|    disc/disc_loss                   | 0.586    |\n","|    disc/disc_proportion_expert_pred | 0.594    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.8      |\n","|    disc/disc_acc_expert             | 0.908    |\n","|    disc/disc_acc_gen                | 0.691    |\n","|    disc/disc_entropy                | 0.678    |\n","|    disc/disc_loss                   | 0.596    |\n","|    disc/disc_proportion_expert_pred | 0.609    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 6        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 142      |\n","|    gen/rollout/ep_rew_mean          | 142      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 48.4     |\n","|    gen/time/fps                     | 2.94e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 9.83e+04 |\n","|    gen/train/explained_variance     | 0.813    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00669  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 6        |\n","|    gen/train/policy_objective       | 0.0202   |\n","|    gen/train/value_loss             | 11.7     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  50%|█████     | 6/12 [00:52<00:53,  8.90s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 194      |\n","|    gen/rollout/ep_rew_mean          | 194      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 78.9     |\n","|    gen/time/fps                     | 3896     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 114688   |\n","|    gen/train/explained_variance     | 0.813    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00669  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 6        |\n","|    gen/train/policy_objective       | 0.0202   |\n","|    gen/train/value_loss             | 11.7     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.816    |\n","|    disc/disc_acc_expert             | 0.895    |\n","|    disc/disc_acc_gen                | 0.738    |\n","|    disc/disc_entropy                | 0.666    |\n","|    disc/disc_loss                   | 0.577    |\n","|    disc/disc_proportion_expert_pred | 0.578    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.803    |\n","|    disc/disc_acc_expert             | 0.889    |\n","|    disc/disc_acc_gen                | 0.718    |\n","|    disc/disc_entropy                | 0.664    |\n","|    disc/disc_loss                   | 0.581    |\n","|    disc/disc_proportion_expert_pred | 0.585    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.806    |\n","|    disc/disc_acc_expert             | 0.882    |\n","|    disc/disc_acc_gen                | 0.729    |\n","|    disc/disc_entropy                | 0.662    |\n","|    disc/disc_loss                   | 0.57     |\n","|    disc/disc_proportion_expert_pred | 0.576    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.8      |\n","|    disc/disc_acc_expert             | 0.871    |\n","|    disc/disc_acc_gen                | 0.729    |\n","|    disc/disc_entropy                | 0.662    |\n","|    disc/disc_loss                   | 0.576    |\n","|    disc/disc_proportion_expert_pred | 0.571    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.81     |\n","|    disc/disc_acc_expert             | 0.849    |\n","|    disc/disc_acc_gen                | 0.771    |\n","|    disc/disc_entropy                | 0.662    |\n","|    disc/disc_loss                   | 0.567    |\n","|    disc/disc_proportion_expert_pred | 0.539    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.821    |\n","|    disc/disc_acc_expert             | 0.861    |\n","|    disc/disc_acc_gen                | 0.781    |\n","|    disc/disc_entropy                | 0.659    |\n","|    disc/disc_loss                   | 0.56     |\n","|    disc/disc_proportion_expert_pred | 0.54     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.818    |\n","|    disc/disc_acc_expert             | 0.836    |\n","|    disc/disc_acc_gen                | 0.801    |\n","|    disc/disc_entropy                | 0.658    |\n","|    disc/disc_loss                   | 0.556    |\n","|    disc/disc_proportion_expert_pred | 0.518    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.818    |\n","|    disc/disc_acc_expert             | 0.837    |\n","|    disc/disc_acc_gen                | 0.799    |\n","|    disc/disc_entropy                | 0.656    |\n","|    disc/disc_loss                   | 0.554    |\n","|    disc/disc_proportion_expert_pred | 0.519    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.812    |\n","|    disc/disc_acc_expert             | 0.865    |\n","|    disc/disc_acc_gen                | 0.758    |\n","|    disc/disc_entropy                | 0.661    |\n","|    disc/disc_loss                   | 0.568    |\n","|    disc/disc_proportion_expert_pred | 0.553    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 7        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 194      |\n","|    gen/rollout/ep_rew_mean          | 194      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 78.9     |\n","|    gen/time/fps                     | 3.9e+03  |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 1.15e+05 |\n","|    gen/train/explained_variance     | 0.909    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00782  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 7        |\n","|    gen/train/policy_objective       | 0.0151   |\n","|    gen/train/value_loss             | 5.96     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  58%|█████▊    | 7/12 [01:01<00:44,  8.92s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 252      |\n","|    gen/rollout/ep_rew_mean          | 252      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 100      |\n","|    gen/time/fps                     | 3688     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 131072   |\n","|    gen/train/explained_variance     | 0.909    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00782  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 7        |\n","|    gen/train/policy_objective       | 0.0151   |\n","|    gen/train/value_loss             | 5.96     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.79     |\n","|    disc/disc_acc_expert             | 0.845    |\n","|    disc/disc_acc_gen                | 0.734    |\n","|    disc/disc_entropy                | 0.678    |\n","|    disc/disc_loss                   | 0.601    |\n","|    disc/disc_proportion_expert_pred | 0.555    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.801    |\n","|    disc/disc_acc_expert             | 0.842    |\n","|    disc/disc_acc_gen                | 0.761    |\n","|    disc/disc_entropy                | 0.678    |\n","|    disc/disc_loss                   | 0.592    |\n","|    disc/disc_proportion_expert_pred | 0.541    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.804    |\n","|    disc/disc_acc_expert             | 0.832    |\n","|    disc/disc_acc_gen                | 0.775    |\n","|    disc/disc_entropy                | 0.677    |\n","|    disc/disc_loss                   | 0.594    |\n","|    disc/disc_proportion_expert_pred | 0.528    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.811    |\n","|    disc/disc_acc_expert             | 0.838    |\n","|    disc/disc_acc_gen                | 0.784    |\n","|    disc/disc_entropy                | 0.677    |\n","|    disc/disc_loss                   | 0.591    |\n","|    disc/disc_proportion_expert_pred | 0.527    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.829    |\n","|    disc/disc_acc_expert             | 0.853    |\n","|    disc/disc_acc_gen                | 0.805    |\n","|    disc/disc_entropy                | 0.676    |\n","|    disc/disc_loss                   | 0.583    |\n","|    disc/disc_proportion_expert_pred | 0.524    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.812    |\n","|    disc/disc_acc_expert             | 0.831    |\n","|    disc/disc_acc_gen                | 0.792    |\n","|    disc/disc_entropy                | 0.675    |\n","|    disc/disc_loss                   | 0.587    |\n","|    disc/disc_proportion_expert_pred | 0.52     |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.814    |\n","|    disc/disc_acc_expert             | 0.837    |\n","|    disc/disc_acc_gen                | 0.792    |\n","|    disc/disc_entropy                | 0.674    |\n","|    disc/disc_loss                   | 0.579    |\n","|    disc/disc_proportion_expert_pred | 0.522    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.82     |\n","|    disc/disc_acc_expert             | 0.812    |\n","|    disc/disc_acc_gen                | 0.828    |\n","|    disc/disc_entropy                | 0.673    |\n","|    disc/disc_loss                   | 0.574    |\n","|    disc/disc_proportion_expert_pred | 0.492    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.81     |\n","|    disc/disc_acc_expert             | 0.836    |\n","|    disc/disc_acc_gen                | 0.784    |\n","|    disc/disc_entropy                | 0.676    |\n","|    disc/disc_loss                   | 0.587    |\n","|    disc/disc_proportion_expert_pred | 0.526    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 8        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 252      |\n","|    gen/rollout/ep_rew_mean          | 252      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 100      |\n","|    gen/time/fps                     | 3.69e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 1.31e+05 |\n","|    gen/train/explained_variance     | 0.932    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00645  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 8        |\n","|    gen/train/policy_objective       | 0.0153   |\n","|    gen/train/value_loss             | 7.45     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  67%|██████▋   | 8/12 [01:10<00:35,  8.81s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 279      |\n","|    gen/rollout/ep_rew_mean          | 279      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 129      |\n","|    gen/time/fps                     | 2999     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 147456   |\n","|    gen/train/explained_variance     | 0.932    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00645  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 8        |\n","|    gen/train/policy_objective       | 0.0153   |\n","|    gen/train/value_loss             | 7.45     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.786    |\n","|    disc/disc_acc_expert             | 0.814    |\n","|    disc/disc_acc_gen                | 0.757    |\n","|    disc/disc_entropy                | 0.661    |\n","|    disc/disc_loss                   | 0.582    |\n","|    disc/disc_proportion_expert_pred | 0.529    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.8      |\n","|    disc/disc_acc_expert             | 0.832    |\n","|    disc/disc_acc_gen                | 0.768    |\n","|    disc/disc_entropy                | 0.66     |\n","|    disc/disc_loss                   | 0.574    |\n","|    disc/disc_proportion_expert_pred | 0.532    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.778    |\n","|    disc/disc_acc_expert             | 0.806    |\n","|    disc/disc_acc_gen                | 0.751    |\n","|    disc/disc_entropy                | 0.659    |\n","|    disc/disc_loss                   | 0.575    |\n","|    disc/disc_proportion_expert_pred | 0.527    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.776    |\n","|    disc/disc_acc_expert             | 0.798    |\n","|    disc/disc_acc_gen                | 0.754    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.572    |\n","|    disc/disc_proportion_expert_pred | 0.522    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.786    |\n","|    disc/disc_acc_expert             | 0.783    |\n","|    disc/disc_acc_gen                | 0.789    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.561    |\n","|    disc/disc_proportion_expert_pred | 0.497    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.802    |\n","|    disc/disc_acc_expert             | 0.817    |\n","|    disc/disc_acc_gen                | 0.786    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.555    |\n","|    disc/disc_proportion_expert_pred | 0.516    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.8      |\n","|    disc/disc_acc_expert             | 0.811    |\n","|    disc/disc_acc_gen                | 0.789    |\n","|    disc/disc_entropy                | 0.654    |\n","|    disc/disc_loss                   | 0.563    |\n","|    disc/disc_proportion_expert_pred | 0.511    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.801    |\n","|    disc/disc_acc_expert             | 0.788    |\n","|    disc/disc_acc_gen                | 0.814    |\n","|    disc/disc_entropy                | 0.652    |\n","|    disc/disc_loss                   | 0.557    |\n","|    disc/disc_proportion_expert_pred | 0.487    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.791    |\n","|    disc/disc_acc_expert             | 0.806    |\n","|    disc/disc_acc_gen                | 0.776    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.567    |\n","|    disc/disc_proportion_expert_pred | 0.515    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 9        |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 279      |\n","|    gen/rollout/ep_rew_mean          | 279      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 129      |\n","|    gen/time/fps                     | 3e+03    |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 1.47e+05 |\n","|    gen/train/explained_variance     | 0.938    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00804  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 9        |\n","|    gen/train/policy_objective       | 0.0185   |\n","|    gen/train/value_loss             | 4.19     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  75%|███████▌  | 9/12 [01:19<00:26,  8.88s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 323      |\n","|    gen/rollout/ep_rew_mean          | 323      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 166      |\n","|    gen/time/fps                     | 2764     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 163840   |\n","|    gen/train/explained_variance     | 0.938    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00804  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 9        |\n","|    gen/train/policy_objective       | 0.0185   |\n","|    gen/train/value_loss             | 4.19     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.8      |\n","|    disc/disc_acc_expert             | 0.795    |\n","|    disc/disc_acc_gen                | 0.806    |\n","|    disc/disc_entropy                | 0.662    |\n","|    disc/disc_loss                   | 0.559    |\n","|    disc/disc_proportion_expert_pred | 0.495    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.811    |\n","|    disc/disc_acc_expert             | 0.821    |\n","|    disc/disc_acc_gen                | 0.8      |\n","|    disc/disc_entropy                | 0.662    |\n","|    disc/disc_loss                   | 0.555    |\n","|    disc/disc_proportion_expert_pred | 0.511    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.809    |\n","|    disc/disc_acc_expert             | 0.822    |\n","|    disc/disc_acc_gen                | 0.796    |\n","|    disc/disc_entropy                | 0.66     |\n","|    disc/disc_loss                   | 0.554    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.807    |\n","|    disc/disc_acc_expert             | 0.819    |\n","|    disc/disc_acc_gen                | 0.794    |\n","|    disc/disc_entropy                | 0.659    |\n","|    disc/disc_loss                   | 0.549    |\n","|    disc/disc_proportion_expert_pred | 0.513    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.806    |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.785    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.541    |\n","|    disc/disc_proportion_expert_pred | 0.521    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.805    |\n","|    disc/disc_acc_expert             | 0.832    |\n","|    disc/disc_acc_gen                | 0.778    |\n","|    disc/disc_entropy                | 0.656    |\n","|    disc/disc_loss                   | 0.541    |\n","|    disc/disc_proportion_expert_pred | 0.527    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.81     |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.794    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.541    |\n","|    disc/disc_proportion_expert_pred | 0.516    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.812    |\n","|    disc/disc_acc_expert             | 0.849    |\n","|    disc/disc_acc_gen                | 0.776    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.54     |\n","|    disc/disc_proportion_expert_pred | 0.536    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.807    |\n","|    disc/disc_acc_expert             | 0.824    |\n","|    disc/disc_acc_gen                | 0.791    |\n","|    disc/disc_entropy                | 0.658    |\n","|    disc/disc_loss                   | 0.547    |\n","|    disc/disc_proportion_expert_pred | 0.516    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 10       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 323      |\n","|    gen/rollout/ep_rew_mean          | 323      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 166      |\n","|    gen/time/fps                     | 2.76e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 1.64e+05 |\n","|    gen/train/explained_variance     | 0.949    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00791  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 10       |\n","|    gen/train/policy_objective       | 0.0187   |\n","|    gen/train/value_loss             | 4.72     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  83%|████████▎ | 10/12 [01:29<00:18,  9.33s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 384      |\n","|    gen/rollout/ep_rew_mean          | 384      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 197      |\n","|    gen/time/fps                     | 4033     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 180224   |\n","|    gen/train/explained_variance     | 0.949    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00791  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 10       |\n","|    gen/train/policy_objective       | 0.0187   |\n","|    gen/train/value_loss             | 4.72     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.75     |\n","|    disc/disc_acc_expert             | 0.816    |\n","|    disc/disc_acc_gen                | 0.684    |\n","|    disc/disc_entropy                | 0.645    |\n","|    disc/disc_loss                   | 0.573    |\n","|    disc/disc_proportion_expert_pred | 0.566    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.75     |\n","|    disc/disc_acc_expert             | 0.839    |\n","|    disc/disc_acc_gen                | 0.661    |\n","|    disc/disc_entropy                | 0.644    |\n","|    disc/disc_loss                   | 0.567    |\n","|    disc/disc_proportion_expert_pred | 0.589    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.75     |\n","|    disc/disc_acc_expert             | 0.818    |\n","|    disc/disc_acc_gen                | 0.682    |\n","|    disc/disc_entropy                | 0.643    |\n","|    disc/disc_loss                   | 0.571    |\n","|    disc/disc_proportion_expert_pred | 0.568    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.755    |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.685    |\n","|    disc/disc_entropy                | 0.641    |\n","|    disc/disc_loss                   | 0.563    |\n","|    disc/disc_proportion_expert_pred | 0.571    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.769    |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.712    |\n","|    disc/disc_entropy                | 0.64     |\n","|    disc/disc_loss                   | 0.553    |\n","|    disc/disc_proportion_expert_pred | 0.557    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.786    |\n","|    disc/disc_acc_expert             | 0.827    |\n","|    disc/disc_acc_gen                | 0.744    |\n","|    disc/disc_entropy                | 0.635    |\n","|    disc/disc_loss                   | 0.543    |\n","|    disc/disc_proportion_expert_pred | 0.542    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.766    |\n","|    disc/disc_acc_expert             | 0.821    |\n","|    disc/disc_acc_gen                | 0.71     |\n","|    disc/disc_entropy                | 0.635    |\n","|    disc/disc_loss                   | 0.546    |\n","|    disc/disc_proportion_expert_pred | 0.556    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.775    |\n","|    disc/disc_acc_expert             | 0.826    |\n","|    disc/disc_acc_gen                | 0.725    |\n","|    disc/disc_entropy                | 0.633    |\n","|    disc/disc_loss                   | 0.541    |\n","|    disc/disc_proportion_expert_pred | 0.551    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.763    |\n","|    disc/disc_acc_expert             | 0.825    |\n","|    disc/disc_acc_gen                | 0.7      |\n","|    disc/disc_entropy                | 0.639    |\n","|    disc/disc_loss                   | 0.557    |\n","|    disc/disc_proportion_expert_pred | 0.562    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 11       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 384      |\n","|    gen/rollout/ep_rew_mean          | 384      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 197      |\n","|    gen/time/fps                     | 4.03e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 4        |\n","|    gen/time/total_timesteps         | 1.8e+05  |\n","|    gen/train/explained_variance     | 0.911    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00848  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 11       |\n","|    gen/train/policy_objective       | 0.0219   |\n","|    gen/train/value_loss             | 6.01     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\rround:  92%|█████████▏| 11/12 [01:38<00:09,  9.09s/it]"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","| raw/                                |          |\n","|    gen/rollout/ep_len_mean          | 445      |\n","|    gen/rollout/ep_rew_mean          | 445      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 256      |\n","|    gen/time/fps                     | 3048     |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 196608   |\n","|    gen/train/explained_variance     | 0.911    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00848  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 11       |\n","|    gen/train/policy_objective       | 0.0219   |\n","|    gen/train/value_loss             | 6.01     |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.67     |\n","|    disc/disc_acc_expert             | 0.812    |\n","|    disc/disc_acc_gen                | 0.528    |\n","|    disc/disc_entropy                | 0.653    |\n","|    disc/disc_loss                   | 0.645    |\n","|    disc/disc_proportion_expert_pred | 0.642    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.667    |\n","|    disc/disc_acc_expert             | 0.813    |\n","|    disc/disc_acc_gen                | 0.52     |\n","|    disc/disc_entropy                | 0.653    |\n","|    disc/disc_loss                   | 0.639    |\n","|    disc/disc_proportion_expert_pred | 0.647    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.668    |\n","|    disc/disc_acc_expert             | 0.829    |\n","|    disc/disc_acc_gen                | 0.508    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.645    |\n","|    disc/disc_proportion_expert_pred | 0.661    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.679    |\n","|    disc/disc_acc_expert             | 0.812    |\n","|    disc/disc_acc_gen                | 0.545    |\n","|    disc/disc_entropy                | 0.653    |\n","|    disc/disc_loss                   | 0.628    |\n","|    disc/disc_proportion_expert_pred | 0.634    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.667    |\n","|    disc/disc_acc_expert             | 0.806    |\n","|    disc/disc_acc_gen                | 0.528    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.635    |\n","|    disc/disc_proportion_expert_pred | 0.639    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.674    |\n","|    disc/disc_acc_expert             | 0.811    |\n","|    disc/disc_acc_gen                | 0.538    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.635    |\n","|    disc/disc_proportion_expert_pred | 0.636    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.684    |\n","|    disc/disc_acc_expert             | 0.795    |\n","|    disc/disc_acc_gen                | 0.572    |\n","|    disc/disc_entropy                | 0.655    |\n","|    disc/disc_loss                   | 0.619    |\n","|    disc/disc_proportion_expert_pred | 0.611    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| raw/                                |          |\n","|    disc/disc_acc                    | 0.676    |\n","|    disc/disc_acc_expert             | 0.802    |\n","|    disc/disc_acc_gen                | 0.551    |\n","|    disc/disc_entropy                | 0.657    |\n","|    disc/disc_loss                   | 0.626    |\n","|    disc/disc_proportion_expert_pred | 0.625    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","--------------------------------------------------\n","--------------------------------------------------\n","| mean/                               |          |\n","|    disc/disc_acc                    | 0.673    |\n","|    disc/disc_acc_expert             | 0.81     |\n","|    disc/disc_acc_gen                | 0.536    |\n","|    disc/disc_entropy                | 0.654    |\n","|    disc/disc_loss                   | 0.634    |\n","|    disc/disc_proportion_expert_pred | 0.637    |\n","|    disc/disc_proportion_expert_true | 0.5      |\n","|    disc/global_step                 | 12       |\n","|    disc/n_expert                    | 1.02e+03 |\n","|    disc/n_generated                 | 1.02e+03 |\n","|    gen/rollout/ep_len_mean          | 445      |\n","|    gen/rollout/ep_rew_mean          | 445      |\n","|    gen/rollout/ep_rew_wrapped_mean  | 256      |\n","|    gen/time/fps                     | 3.05e+03 |\n","|    gen/time/iterations              | 1        |\n","|    gen/time/time_elapsed            | 5        |\n","|    gen/time/total_timesteps         | 1.97e+05 |\n","|    gen/train/explained_variance     | 0.891    |\n","|    gen/train/is_line_search_success | 1        |\n","|    gen/train/kl_divergence_loss     | 0.00347  |\n","|    gen/train/learning_rate          | 0.001    |\n","|    gen/train/n_updates              | 12       |\n","|    gen/train/policy_objective       | 0.0167   |\n","|    gen/train/value_loss             | 5.87     |\n","--------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["round: 100%|██████████| 12/12 [01:47<00:00,  8.96s/it]\n"]}]},{"cell_type":"code","source":["env_TRPO.seed(SEED)\n","learner_rewards_after_training, _ = evaluate_policy(\n","    learner_TRPO, env_TRPO, 100, return_episode_rewards=True,\n",")"],"metadata":{"id":"i2gSVckaRsLt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\n","    \"Rewards before training:\",\n","    np.mean(learner_rewards_before_training),\n","    \"+/-\",\n","    np.std(learner_rewards_before_training),\n",")\n","print(\n","    \"Rewards after training:\",\n","    np.mean(learner_rewards_after_training),\n","    \"+/-\",\n","    np.std(learner_rewards_after_training),\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Q15RyXePzgX","executionInfo":{"status":"ok","timestamp":1708948777972,"user_tz":0,"elapsed":12,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"0a0b10d7-271e-4256-c716-e5db8c8e38f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rewards before training: 9.57 +/- 0.6819824044651006\n","Rewards after training: 495.8 +/- 13.60147050873544\n"]}]},{"cell_type":"markdown","source":["Rewards before training: 9.57 +/- 0.6819824044651006\n","\n","Rewards after training: 495.8 +/- 13.60147050873544"],"metadata":{"id":"DVoGhSRvcdfM"}},{"cell_type":"markdown","source":["# BC-PPO\n","\n","(imitation library, BC does not work as expected - RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)). Try run BC with cpu to avoid this issue (changing runtime type from GPU to CPU, and it can run on colab but having some issue on my Win10 laptop)."],"metadata":{"id":"yfbMSp7iW73z"}},{"cell_type":"code","source":["import numpy as np\n","import gymnasium as gym\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","from imitation.algorithms import bc\n","from imitation.data import rollout\n","from imitation.data.wrappers import RolloutInfoWrapper\n","from imitation.policies.serialize import load_policy\n","from imitation.util.util import make_vec_env"],"metadata":{"id":"BD0a33EGW6Jt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","rng = np.random.default_rng(0)\n","env = make_vec_env(\n","    \"CartPole-v1\", # seals:seals/CartPole-v0\n","    rng=np.random.default_rng(SEED),\n","    n_envs=8,\n","    post_wrappers=[\n","        lambda env, _: RolloutInfoWrapper(env)\n","    ],  # needed for computing rollouts later\n",")\n","expert = load_policy(\n","    \"ppo-huggingface\",\n","    organization=\"HumanCompatibleAI\",\n","    env_name=\"CartPole-v1\",\n","    venv=env,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["3820a9432ff4455cb352e1473e388c61","e3caffd7dcbd4d4daaa75ff936435d83","06fb7e28ba2746d3940a94368fc903cc","c06505ff0ec74e8390a941724e49a8bd","52a6721e56b44183b6fe6bdc504e53b3","d28436dc5d4e41ca92253e52255f1b86","4bc4c0ab5a9b4072ae7a1d663bccabaa","4da27747dc4e4b2ba79e0cf20309c8d7","a28c09f4dbda4c1aadf4dcbeefd70f12","ed14d805a0464773bee83dc2c5af0d92","24dc4bdbf5a64b55b36936e92b3795ac","b069967c4dd248ef97924a9b03a31126","aae235807f0e4cb6be811c64d288525b","d083c4339a354ab689722b4f2c683191","6a2211150d7c430885d46d7c2efc6ef5","8cd960b4c56540fa943c62877fd8ad8c","958d3f10b3f140ee92a41b3f98971282","5798c021dad74e52b994cc71876beda3","ae7d9a67659f4750acafa5a537dc8193","d0659c4a881c4ad2bb078f1ee9297cee","c77ddbb886e14d1c99bd4236964415a8","518c61cc94a34d1c9325de643a29e1ba"]},"id":"LTSorppZaKUT","executionInfo":{"status":"ok","timestamp":1709044889652,"user_tz":0,"elapsed":2747,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"98d1df89-f8f3-41d5-b6a8-9ce064df6cc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["ppo-CartPole-v1.zip:   0%|          | 0.00/139k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3820a9432ff4455cb352e1473e388c61"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b069967c4dd248ef97924a9b03a31126"}},"metadata":{}}]},{"cell_type":"code","source":["rollouts = rollout.rollout(\n","    expert,\n","    env,\n","    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n","    rng=rng,\n",")"],"metadata":{"id":"jvHdAReOdT-1","executionInfo":{"status":"ok","timestamp":1709044894922,"user_tz":0,"elapsed":5273,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c504eb9-7164-424a-fa08-8a209373ed55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["transitions = rollout.flatten_trajectories(rollouts)\n","\n","bc_trainer = bc.BC(\n","    observation_space=env.observation_space,\n","    action_space=env.action_space,\n","    demonstrations=transitions,\n","    rng=rng,\n",")\n","bc_trainer.train(n_epochs=2)\n","reward, _ = evaluate_policy(bc_trainer.policy, env, 10)\n","print(\"Reward:\", reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVCHRcIddZuj","executionInfo":{"status":"ok","timestamp":1709044900733,"user_tz":0,"elapsed":5825,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"5c3d4dfc-8591-4e49-dc2c-ce360990dae5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0batch [00:00, ?batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 0         |\n","|    ent_loss       | -0.000693 |\n","|    entropy        | 0.693     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 72.5      |\n","|    loss           | 0.693     |\n","|    neglogp        | 0.694     |\n","|    prob_true_act  | 0.5       |\n","|    samples_so_far | 32        |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["484batch [00:02, 199.52batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 500       |\n","|    ent_loss       | -0.000281 |\n","|    entropy        | 0.281     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 89.3      |\n","|    loss           | 0.399     |\n","|    neglogp        | 0.399     |\n","|    prob_true_act  | 0.774     |\n","|    samples_so_far | 16032     |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["865batch [00:04, 199.62batch/s]\n","875batch [00:04, 177.86batch/s]\n"]},{"output_type":"stream","name":"stdout","text":["Reward: 470.4\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.ppo import MlpPolicy\n","\n","from imitation.algorithms import bc\n","from imitation.data import rollout\n","from imitation.data.wrappers import RolloutInfoWrapper\n","from imitation.policies.serialize import load_policy\n","from imitation.util.util import make_vec_env\n","\n","rng = np.random.default_rng(0)\n","env = make_vec_env(\n","    \"seals:seals/CartPole-v0\",\n","    rng=rng,\n","    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",")\n","\n","\n","def train_expert():\n","    # note: use `download_expert` instead to download a pretrained, competent expert\n","    print(\"Training a expert.\")\n","    expert = PPO(\n","        policy=MlpPolicy,\n","        env=env,\n","        seed=0,\n","        batch_size=64,\n","        ent_coef=0.0,\n","        learning_rate=0.0003,\n","        n_epochs=10,\n","        n_steps=64,\n","    )\n","    expert.learn(1_000)  # Note: change this to 100_000 to train a decent expert.\n","    return expert\n","\n","\n","def download_expert():\n","    print(\"Downloading a pretrained expert.\")\n","    expert = load_policy(\n","        \"ppo-huggingface\",\n","        organization=\"HumanCompatibleAI\",\n","        env_name=\"seals-CartPole-v0\",\n","        venv=env,\n","    )\n","    return expert\n","\n","\n","def sample_expert_transitions():\n","    # expert = train_expert()  # uncomment to train your own expert\n","    expert = download_expert()\n","\n","    print(\"Sampling expert transitions.\")\n","    rollouts = rollout.rollout(\n","        expert,\n","        env,\n","        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n","        rng=rng,\n","    )\n","    return rollout.flatten_trajectories(rollouts)\n","\n","\n","transitions = sample_expert_transitions()\n","bc_trainer = bc.BC(\n","    observation_space=env.observation_space,\n","    action_space=env.action_space,\n","    demonstrations=transitions,\n","    rng=rng,\n",")\n","\n","evaluation_env = make_vec_env(\n","    \"seals:seals/CartPole-v0\",\n","    rng=rng,\n","    env_make_kwargs={\"render_mode\": \"human\"},  # for rendering\n",")\n","\n","print(\"Evaluating the untrained policy.\")\n","reward, _ = evaluate_policy(\n","    bc_trainer.policy,  # type: ignore[arg-type]\n","    evaluation_env,\n","    n_eval_episodes=3,\n","    render=True,  # comment out to speed up\n",")\n","print(f\"Reward before training: {reward}\")\n","\n","print(\"Training a policy using Behavior Cloning\")\n","bc_trainer.train(n_epochs=1)\n","\n","print(\"Evaluating the trained policy.\")\n","reward, _ = evaluate_policy(\n","    bc_trainer.policy,  # type: ignore[arg-type]\n","    evaluation_env,\n","    n_eval_episodes=3,\n","    render=True,  # comment out to speed up\n",")\n","print(f\"Reward after training: {reward}\")"],"metadata":{"id":"nw123-FrfKqb","executionInfo":{"status":"ok","timestamp":1709045322185,"user_tz":0,"elapsed":79422,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"colab":{"base_uri":"https://localhost:8080/","height":691,"referenced_widgets":["4d6cfca918504a29888b312bd9f17ca4","f343274531264e559776a4f6f412466a","516b7de559774808a8850a1e5ede1c06","84a42027c1724615940cdc4504a50183","e044f0fe858f4622b617dd21e750b735","2851baacd2a0444399bda8cc07480ba7","1fd7ad8da8c74a7aa52a97b61243e9f8","0d5af323c57249cd9f33c3df03c1b1f7","14d9c2d9df104c14ac3ad2835ab9cf58","1a2ae27113bc4fff88a07982101c56a9","f8f65a1bcc944e57baf6f6ee77b164ee"]},"outputId":"b91a8e8c-ae03-4c58-8882-0e43faf90955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading a pretrained expert.\n"]},{"output_type":"display_data","data":{"text/plain":["ppo-seals-CartPole-v0.zip:   0%|          | 0.00/139k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6cfca918504a29888b312bd9f17ca4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Sampling expert transitions.\n","Evaluating the untrained policy.\n","Reward before training: 8.0\n","Training a policy using Behavior Cloning\n"]},{"output_type":"stream","name":"stderr","text":["\r0batch [00:00, ?batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 0         |\n","|    ent_loss       | -0.000693 |\n","|    entropy        | 0.693     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 72.5      |\n","|    loss           | 0.693     |\n","|    neglogp        | 0.694     |\n","|    prob_true_act  | 0.5       |\n","|    samples_so_far | 32        |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["496batch [00:02, 223.20batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 500       |\n","|    ent_loss       | -0.000388 |\n","|    entropy        | 0.388     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 93.7      |\n","|    loss           | 0.267     |\n","|    neglogp        | 0.268     |\n","|    prob_true_act  | 0.799     |\n","|    samples_so_far | 16032     |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["865batch [00:04, 207.19batch/s]\n","875batch [00:04, 210.10batch/s]\n"]},{"output_type":"stream","name":"stdout","text":["Evaluating the trained policy.\n","Reward after training: 500.0\n"]}]},{"cell_type":"markdown","source":["# BC-TRPO"],"metadata":{"id":"8qgcKIbY8TBh"}},{"cell_type":"code","source":["import numpy as np\n","import gymnasium as gym\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","from imitation.algorithms import bc\n","from imitation.data import rollout\n","from imitation.data.wrappers import RolloutInfoWrapper\n","from imitation.policies.serialize import load_policy\n","from imitation.util.util import make_vec_env"],"metadata":{"id":"VUVUs0q6DAXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","rng = np.random.default_rng(0)\n","env_TRPO = make_vec_env(\n","    \"CartPole-v1\",\n","    rng=np.random.default_rng(SEED),\n","    n_envs=8,\n","    post_wrappers=[\n","        lambda env, _: RolloutInfoWrapper(env)\n","    ],  # needed for computing rollouts later\n",")\n","expert_TRPO = load_policy(\n","    \"ppo-huggingface\",\n","    organization=\"HumanCompatibleAI\",\n","    env_name=\"CartPole-v1\",\n","    venv=env_TRPO,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["3820a9432ff4455cb352e1473e388c61","e3caffd7dcbd4d4daaa75ff936435d83","06fb7e28ba2746d3940a94368fc903cc","c06505ff0ec74e8390a941724e49a8bd","52a6721e56b44183b6fe6bdc504e53b3","d28436dc5d4e41ca92253e52255f1b86","4bc4c0ab5a9b4072ae7a1d663bccabaa","4da27747dc4e4b2ba79e0cf20309c8d7","a28c09f4dbda4c1aadf4dcbeefd70f12","ed14d805a0464773bee83dc2c5af0d92","24dc4bdbf5a64b55b36936e92b3795ac","b069967c4dd248ef97924a9b03a31126","aae235807f0e4cb6be811c64d288525b","d083c4339a354ab689722b4f2c683191","6a2211150d7c430885d46d7c2efc6ef5","8cd960b4c56540fa943c62877fd8ad8c","958d3f10b3f140ee92a41b3f98971282","5798c021dad74e52b994cc71876beda3","ae7d9a67659f4750acafa5a537dc8193","d0659c4a881c4ad2bb078f1ee9297cee","c77ddbb886e14d1c99bd4236964415a8","518c61cc94a34d1c9325de643a29e1ba"]},"executionInfo":{"status":"ok","timestamp":1709044889652,"user_tz":0,"elapsed":2747,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"98d1df89-f8f3-41d5-b6a8-9ce064df6cc6","id":"babNcpyFDAXV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["ppo-CartPole-v1.zip:   0%|          | 0.00/139k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3820a9432ff4455cb352e1473e388c61"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b069967c4dd248ef97924a9b03a31126"}},"metadata":{}}]},{"cell_type":"code","source":["rollouts_TRPO = rollout.rollout(\n","    expert_TRPO,\n","    env_TRPO,\n","    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n","    rng=np.random.default_rng(SEED), # 随机数？ The random state to use for sampling trajectories.\n",")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1709044894922,"user_tz":0,"elapsed":5273,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c504eb9-7164-424a-fa08-8a209373ed55","id":"ethZ6jw4DAXV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["transitions = rollout.flatten_trajectories(rollouts)\n","\n","bc_trainer = bc.BC(\n","    observation_space=env.observation_space,\n","    action_space=env.action_space,\n","    demonstrations=transitions,\n","    rng=rng,\n",")\n","bc_trainer.train(n_epochs=2)\n","reward, _ = evaluate_policy(bc_trainer.policy, env, 10)\n","print(\"Reward:\", reward)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709044900733,"user_tz":0,"elapsed":5825,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"5c3d4dfc-8591-4e49-dc2c-ce360990dae5","id":"KYTeJ_1pDAXW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0batch [00:00, ?batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 0         |\n","|    ent_loss       | -0.000693 |\n","|    entropy        | 0.693     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 72.5      |\n","|    loss           | 0.693     |\n","|    neglogp        | 0.694     |\n","|    prob_true_act  | 0.5       |\n","|    samples_so_far | 32        |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["484batch [00:02, 199.52batch/s]"]},{"output_type":"stream","name":"stdout","text":["---------------------------------\n","| batch_size        | 32        |\n","| bc/               |           |\n","|    batch          | 500       |\n","|    ent_loss       | -0.000281 |\n","|    entropy        | 0.281     |\n","|    epoch          | 0         |\n","|    l2_loss        | 0         |\n","|    l2_norm        | 89.3      |\n","|    loss           | 0.399     |\n","|    neglogp        | 0.399     |\n","|    prob_true_act  | 0.774     |\n","|    samples_so_far | 16032     |\n","---------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["865batch [00:04, 199.62batch/s]\n","875batch [00:04, 177.86batch/s]\n"]},{"output_type":"stream","name":"stdout","text":["Reward: 470.4\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kYwvl9EItkEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RecurrentPPO\n","not quite exchangable for PPO/TRPO"],"metadata":{"id":"GoWnkl0LzJLU"}},{"cell_type":"code","source":["import numpy as np\n","\n","from sb3_contrib import RecurrentPPO\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","model = RecurrentPPO(\"MlpLstmPolicy\", \"CartPole-v1\", verbose=1)\n","model.learn(5000)\n","\n","vec_env = model.get_env()\n","mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=20, warn=False)\n","print(mean_reward)\n","\n","model.save(\"ppo_recurrent\")\n","del model # remove to demonstrate saving and loading\n","\n","model = RecurrentPPO.load(\"ppo_recurrent\")\n","\n","obs = vec_env.reset()\n","# cell and hidden state of the LSTM\n","lstm_states = None\n","num_envs = 1\n","# Episode start signals are used to reset the lstm states\n","episode_starts = np.ones((num_envs,), dtype=bool)\n","while True:\n","    action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)\n","    obs, rewards, dones, info = vec_env.step(action)\n","    episode_starts = dones\n","    vec_env.render(\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KUqHzcJ78RgU","executionInfo":{"status":"error","timestamp":1709123841565,"user_tz":0,"elapsed":120878,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"a8a88088-7545-4fbe-cbde-66ce7952723a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Creating environment from the given name 'CartPole-v1'\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 20       |\n","|    ep_rew_mean     | 20       |\n","| time/              |          |\n","|    fps             | 377      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 0        |\n","|    total_timesteps | 128      |\n","---------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 25.5         |\n","|    ep_rew_mean          | 25.5         |\n","| time/                   |              |\n","|    fps                  | 122          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 2            |\n","|    total_timesteps      | 256          |\n","| train/                  |              |\n","|    approx_kl            | 8.374453e-06 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.693       |\n","|    explained_variance   | -0.000416    |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 30.1         |\n","|    n_updates            | 10           |\n","|    policy_gradient_loss | -0.000192    |\n","|    value_loss           | 63.5         |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 26.4          |\n","|    ep_rew_mean          | 26.4          |\n","| time/                   |               |\n","|    fps                  | 74            |\n","|    iterations           | 3             |\n","|    time_elapsed         | 5             |\n","|    total_timesteps      | 384           |\n","| train/                  |               |\n","|    approx_kl            | 2.0799693e-05 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.693        |\n","|    explained_variance   | -0.00887      |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 45.9          |\n","|    n_updates            | 20            |\n","|    policy_gradient_loss | -0.000371     |\n","|    value_loss           | 101           |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 25            |\n","|    ep_rew_mean          | 25            |\n","| time/                   |               |\n","|    fps                  | 63            |\n","|    iterations           | 4             |\n","|    time_elapsed         | 8             |\n","|    total_timesteps      | 512           |\n","| train/                  |               |\n","|    approx_kl            | 2.0183623e-05 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.693        |\n","|    explained_variance   | -0.012        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 36.5          |\n","|    n_updates            | 30            |\n","|    policy_gradient_loss | -7.14e-05     |\n","|    value_loss           | 90.7          |\n","-------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 23.3        |\n","|    ep_rew_mean          | 23.3        |\n","| time/                   |             |\n","|    fps                  | 64          |\n","|    iterations           | 5           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 640         |\n","| train/                  |             |\n","|    approx_kl            | 8.29529e-06 |\n","|    clip_fraction        | 0           |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.693      |\n","|    explained_variance   | -0.443      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 21.2        |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.000321   |\n","|    value_loss           | 47.2        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 23.2         |\n","|    ep_rew_mean          | 23.2         |\n","| time/                   |              |\n","|    fps                  | 66           |\n","|    iterations           | 6            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 768          |\n","| train/                  |              |\n","|    approx_kl            | 1.473818e-05 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.693       |\n","|    explained_variance   | -0.582       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 18.2         |\n","|    n_updates            | 50           |\n","|    policy_gradient_loss | -0.000246    |\n","|    value_loss           | 39.2         |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 24.6          |\n","|    ep_rew_mean          | 24.6          |\n","| time/                   |               |\n","|    fps                  | 65            |\n","|    iterations           | 7             |\n","|    time_elapsed         | 13            |\n","|    total_timesteps      | 896           |\n","| train/                  |               |\n","|    approx_kl            | 4.3713022e-05 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.693        |\n","|    explained_variance   | -0.267        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 24.1          |\n","|    n_updates            | 60            |\n","|    policy_gradient_loss | -0.000255     |\n","|    value_loss           | 50.6          |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 23.5         |\n","|    ep_rew_mean          | 23.5         |\n","| time/                   |              |\n","|    fps                  | 62           |\n","|    iterations           | 8            |\n","|    time_elapsed         | 16           |\n","|    total_timesteps      | 1024         |\n","| train/                  |              |\n","|    approx_kl            | 7.597543e-05 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.692       |\n","|    explained_variance   | -0.119       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 39.2         |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.001       |\n","|    value_loss           | 81.1         |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 22.4          |\n","|    ep_rew_mean          | 22.4          |\n","| time/                   |               |\n","|    fps                  | 61            |\n","|    iterations           | 9             |\n","|    time_elapsed         | 18            |\n","|    total_timesteps      | 1152          |\n","| train/                  |               |\n","|    approx_kl            | 0.00012813043 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.691        |\n","|    explained_variance   | -0.296        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 19.7          |\n","|    n_updates            | 80            |\n","|    policy_gradient_loss | -0.000401     |\n","|    value_loss           | 40.7          |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 22.5         |\n","|    ep_rew_mean          | 22.5         |\n","| time/                   |              |\n","|    fps                  | 62           |\n","|    iterations           | 10           |\n","|    time_elapsed         | 20           |\n","|    total_timesteps      | 1280         |\n","| train/                  |              |\n","|    approx_kl            | 6.227614e-05 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.691       |\n","|    explained_variance   | -0.292       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 16.8         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.000207    |\n","|    value_loss           | 34.8         |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 23            |\n","|    ep_rew_mean          | 23            |\n","| time/                   |               |\n","|    fps                  | 60            |\n","|    iterations           | 11            |\n","|    time_elapsed         | 23            |\n","|    total_timesteps      | 1408          |\n","| train/                  |               |\n","|    approx_kl            | 0.00027306657 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.691        |\n","|    explained_variance   | -0.091        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 31.5          |\n","|    n_updates            | 100           |\n","|    policy_gradient_loss | -0.0017       |\n","|    value_loss           | 64.5          |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 23.1          |\n","|    ep_rew_mean          | 23.1          |\n","| time/                   |               |\n","|    fps                  | 61            |\n","|    iterations           | 12            |\n","|    time_elapsed         | 25            |\n","|    total_timesteps      | 1536          |\n","| train/                  |               |\n","|    approx_kl            | 0.00010700803 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.692        |\n","|    explained_variance   | -0.124        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 27.9          |\n","|    n_updates            | 110           |\n","|    policy_gradient_loss | 0.000403      |\n","|    value_loss           | 57.3          |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 23.7          |\n","|    ep_rew_mean          | 23.7          |\n","| time/                   |               |\n","|    fps                  | 61            |\n","|    iterations           | 13            |\n","|    time_elapsed         | 27            |\n","|    total_timesteps      | 1664          |\n","| train/                  |               |\n","|    approx_kl            | 0.00022281078 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.691        |\n","|    explained_variance   | -0.123        |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 20.5          |\n","|    n_updates            | 120           |\n","|    policy_gradient_loss | -0.00192      |\n","|    value_loss           | 42            |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 23.3          |\n","|    ep_rew_mean          | 23.3          |\n","| time/                   |               |\n","|    fps                  | 58            |\n","|    iterations           | 14            |\n","|    time_elapsed         | 30            |\n","|    total_timesteps      | 1792          |\n","| train/                  |               |\n","|    approx_kl            | 0.00023237802 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.69         |\n","|    explained_variance   | -0.0564       |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 31.9          |\n","|    n_updates            | 130           |\n","|    policy_gradient_loss | -0.00135      |\n","|    value_loss           | 65.1          |\n","-------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 23.3         |\n","|    ep_rew_mean          | 23.3         |\n","| time/                   |              |\n","|    fps                  | 58           |\n","|    iterations           | 15           |\n","|    time_elapsed         | 33           |\n","|    total_timesteps      | 1920         |\n","| train/                  |              |\n","|    approx_kl            | 0.0028347385 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.684       |\n","|    explained_variance   | -0.115       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 16.2         |\n","|    n_updates            | 140          |\n","|    policy_gradient_loss | -0.00423     |\n","|    value_loss           | 32.9         |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 23.1        |\n","|    ep_rew_mean          | 23.1        |\n","| time/                   |             |\n","|    fps                  | 57          |\n","|    iterations           | 16          |\n","|    time_elapsed         | 35          |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.009106841 |\n","|    clip_fraction        | 0.0305      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.664      |\n","|    explained_variance   | -0.0679     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 26.4        |\n","|    n_updates            | 150         |\n","|    policy_gradient_loss | -0.0145     |\n","|    value_loss           | 53.9        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 24.2         |\n","|    ep_rew_mean          | 24.2         |\n","| time/                   |              |\n","|    fps                  | 57           |\n","|    iterations           | 17           |\n","|    time_elapsed         | 38           |\n","|    total_timesteps      | 2176         |\n","| train/                  |              |\n","|    approx_kl            | 0.0017221444 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.669       |\n","|    explained_variance   | -0.0484      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 27.8         |\n","|    n_updates            | 160          |\n","|    policy_gradient_loss | -0.00167     |\n","|    value_loss           | 56.6         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 24.5         |\n","|    ep_rew_mean          | 24.5         |\n","| time/                   |              |\n","|    fps                  | 55           |\n","|    iterations           | 18           |\n","|    time_elapsed         | 41           |\n","|    total_timesteps      | 2304         |\n","| train/                  |              |\n","|    approx_kl            | 0.0016408856 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.661       |\n","|    explained_variance   | -0.0226      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 48.8         |\n","|    n_updates            | 170          |\n","|    policy_gradient_loss | -0.00164     |\n","|    value_loss           | 99.3         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 24.6         |\n","|    ep_rew_mean          | 24.6         |\n","| time/                   |              |\n","|    fps                  | 54           |\n","|    iterations           | 19           |\n","|    time_elapsed         | 44           |\n","|    total_timesteps      | 2432         |\n","| train/                  |              |\n","|    approx_kl            | 0.0030804696 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.631       |\n","|    explained_variance   | -0.0407      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 28.7         |\n","|    n_updates            | 180          |\n","|    policy_gradient_loss | -0.00358     |\n","|    value_loss           | 58.3         |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 25.5         |\n","|    ep_rew_mean          | 25.5         |\n","| time/                   |              |\n","|    fps                  | 53           |\n","|    iterations           | 20           |\n","|    time_elapsed         | 47           |\n","|    total_timesteps      | 2560         |\n","| train/                  |              |\n","|    approx_kl            | 0.0020386793 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.644       |\n","|    explained_variance   | -0.0212      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 41.4         |\n","|    n_updates            | 190          |\n","|    policy_gradient_loss | -0.000995    |\n","|    value_loss           | 84.1         |\n","------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 25.9          |\n","|    ep_rew_mean          | 25.9          |\n","| time/                   |               |\n","|    fps                  | 54            |\n","|    iterations           | 21            |\n","|    time_elapsed         | 49            |\n","|    total_timesteps      | 2688          |\n","| train/                  |               |\n","|    approx_kl            | 0.00075607374 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.654        |\n","|    explained_variance   | -0.0245       |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 36.1          |\n","|    n_updates            | 200           |\n","|    policy_gradient_loss | -0.00221      |\n","|    value_loss           | 73.3          |\n","-------------------------------------------\n","-------------------------------------------\n","| rollout/                |               |\n","|    ep_len_mean          | 26.8          |\n","|    ep_rew_mean          | 26.8          |\n","| time/                   |               |\n","|    fps                  | 53            |\n","|    iterations           | 22            |\n","|    time_elapsed         | 53            |\n","|    total_timesteps      | 2816          |\n","| train/                  |               |\n","|    approx_kl            | 0.00012467429 |\n","|    clip_fraction        | 0             |\n","|    clip_range           | 0.2           |\n","|    entropy_loss         | -0.635        |\n","|    explained_variance   | -0.0101       |\n","|    learning_rate        | 0.0003        |\n","|    loss                 | 66.4          |\n","|    n_updates            | 210           |\n","|    policy_gradient_loss | 0.000634      |\n","|    value_loss           | 135           |\n","-------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 27          |\n","|    ep_rew_mean          | 27          |\n","| time/                   |             |\n","|    fps                  | 52          |\n","|    iterations           | 23          |\n","|    time_elapsed         | 55          |\n","|    total_timesteps      | 2944        |\n","| train/                  |             |\n","|    approx_kl            | 0.008268557 |\n","|    clip_fraction        | 0.032       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.59       |\n","|    explained_variance   | -0.0558     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 23.3        |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.01       |\n","|    value_loss           | 47.2        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 28          |\n","|    ep_rew_mean          | 28          |\n","| time/                   |             |\n","|    fps                  | 52          |\n","|    iterations           | 24          |\n","|    time_elapsed         | 58          |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.009475583 |\n","|    clip_fraction        | 0.0359      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.592      |\n","|    explained_variance   | -0.0204     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 34.9        |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.0122     |\n","|    value_loss           | 70.9        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 28.2         |\n","|    ep_rew_mean          | 28.2         |\n","| time/                   |              |\n","|    fps                  | 51           |\n","|    iterations           | 25           |\n","|    time_elapsed         | 61           |\n","|    total_timesteps      | 3200         |\n","| train/                  |              |\n","|    approx_kl            | 0.0047565075 |\n","|    clip_fraction        | 0.00703      |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.596       |\n","|    explained_variance   | -0.00362     |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 58           |\n","|    n_updates            | 240          |\n","|    policy_gradient_loss | -0.00651     |\n","|    value_loss           | 118          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 28.2        |\n","|    ep_rew_mean          | 28.2        |\n","| time/                   |             |\n","|    fps                  | 51          |\n","|    iterations           | 26          |\n","|    time_elapsed         | 64          |\n","|    total_timesteps      | 3328        |\n","| train/                  |             |\n","|    approx_kl            | 0.014939087 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.53       |\n","|    explained_variance   | -0.021      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 39.9        |\n","|    n_updates            | 250         |\n","|    policy_gradient_loss | -0.0102     |\n","|    value_loss           | 80.9        |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 29.8        |\n","|    ep_rew_mean          | 29.8        |\n","| time/                   |             |\n","|    fps                  | 49          |\n","|    iterations           | 27          |\n","|    time_elapsed         | 69          |\n","|    total_timesteps      | 3456        |\n","| train/                  |             |\n","|    approx_kl            | 0.007661652 |\n","|    clip_fraction        | 0.0258      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.534      |\n","|    explained_variance   | -0.00793    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 64.7        |\n","|    n_updates            | 260         |\n","|    policy_gradient_loss | -0.00151    |\n","|    value_loss           | 131         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 31.1        |\n","|    ep_rew_mean          | 31.1        |\n","| time/                   |             |\n","|    fps                  | 49          |\n","|    iterations           | 28          |\n","|    time_elapsed         | 72          |\n","|    total_timesteps      | 3584        |\n","| train/                  |             |\n","|    approx_kl            | 0.012410376 |\n","|    clip_fraction        | 0.0508      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.52       |\n","|    explained_variance   | -0.00723    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 61.2        |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.00753    |\n","|    value_loss           | 124         |\n","-----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 31.8        |\n","|    ep_rew_mean          | 31.8        |\n","| time/                   |             |\n","|    fps                  | 49          |\n","|    iterations           | 29          |\n","|    time_elapsed         | 75          |\n","|    total_timesteps      | 3712        |\n","| train/                  |             |\n","|    approx_kl            | 0.003396464 |\n","|    clip_fraction        | 0.0148      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.554      |\n","|    explained_variance   | -0.00328    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 64.5        |\n","|    n_updates            | 280         |\n","|    policy_gradient_loss | -0.00138    |\n","|    value_loss           | 131         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 32.9       |\n","|    ep_rew_mean          | 32.9       |\n","| time/                   |            |\n","|    fps                  | 48         |\n","|    iterations           | 30         |\n","|    time_elapsed         | 78         |\n","|    total_timesteps      | 3840       |\n","| train/                  |            |\n","|    approx_kl            | 0.01030259 |\n","|    clip_fraction        | 0.0437     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.556     |\n","|    explained_variance   | -0.0081    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 60.2       |\n","|    n_updates            | 290        |\n","|    policy_gradient_loss | -0.00657   |\n","|    value_loss           | 122        |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 33.6        |\n","|    ep_rew_mean          | 33.6        |\n","| time/                   |             |\n","|    fps                  | 47          |\n","|    iterations           | 31          |\n","|    time_elapsed         | 83          |\n","|    total_timesteps      | 3968        |\n","| train/                  |             |\n","|    approx_kl            | 0.001369107 |\n","|    clip_fraction        | 0           |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.56       |\n","|    explained_variance   | -0.00807    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 59.8        |\n","|    n_updates            | 300         |\n","|    policy_gradient_loss | -0.000643   |\n","|    value_loss           | 121         |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 35           |\n","|    ep_rew_mean          | 35           |\n","| time/                   |              |\n","|    fps                  | 46           |\n","|    iterations           | 32           |\n","|    time_elapsed         | 87           |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0011659896 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.557       |\n","|    explained_variance   | -0.00616     |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 71.4         |\n","|    n_updates            | 310          |\n","|    policy_gradient_loss | -0.000883    |\n","|    value_loss           | 145          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 35          |\n","|    ep_rew_mean          | 35          |\n","| time/                   |             |\n","|    fps                  | 46          |\n","|    iterations           | 33          |\n","|    time_elapsed         | 89          |\n","|    total_timesteps      | 4224        |\n","| train/                  |             |\n","|    approx_kl            | 0.002948327 |\n","|    clip_fraction        | 0.00703     |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.568      |\n","|    explained_variance   | -0.0143     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 48.5        |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.00582    |\n","|    value_loss           | 98.4        |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 37           |\n","|    ep_rew_mean          | 37           |\n","| time/                   |              |\n","|    fps                  | 47           |\n","|    iterations           | 34           |\n","|    time_elapsed         | 90           |\n","|    total_timesteps      | 4352         |\n","| train/                  |              |\n","|    approx_kl            | 0.0012440756 |\n","|    clip_fraction        | 0            |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.514       |\n","|    explained_variance   | 0            |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 83.1         |\n","|    n_updates            | 330          |\n","|    policy_gradient_loss | 0.00115      |\n","|    value_loss           | 169          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 38.6        |\n","|    ep_rew_mean          | 38.6        |\n","| time/                   |             |\n","|    fps                  | 47          |\n","|    iterations           | 35          |\n","|    time_elapsed         | 95          |\n","|    total_timesteps      | 4480        |\n","| train/                  |             |\n","|    approx_kl            | 0.002006008 |\n","|    clip_fraction        | 0.00625     |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.474      |\n","|    explained_variance   | -0.00713    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 61.9        |\n","|    n_updates            | 340         |\n","|    policy_gradient_loss | -0.00438    |\n","|    value_loss           | 126         |\n","-----------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 39.4         |\n","|    ep_rew_mean          | 39.4         |\n","| time/                   |              |\n","|    fps                  | 46           |\n","|    iterations           | 36           |\n","|    time_elapsed         | 99           |\n","|    total_timesteps      | 4608         |\n","| train/                  |              |\n","|    approx_kl            | 0.0065912697 |\n","|    clip_fraction        | 0.0656       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.443       |\n","|    explained_variance   | -0.00479     |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 60.5         |\n","|    n_updates            | 350          |\n","|    policy_gradient_loss | -0.00831     |\n","|    value_loss           | 123          |\n","------------------------------------------\n","------------------------------------------\n","| rollout/                |              |\n","|    ep_len_mean          | 40.3         |\n","|    ep_rew_mean          | 40.3         |\n","| time/                   |              |\n","|    fps                  | 46           |\n","|    iterations           | 37           |\n","|    time_elapsed         | 102          |\n","|    total_timesteps      | 4736         |\n","| train/                  |              |\n","|    approx_kl            | 0.0048368545 |\n","|    clip_fraction        | 0.0508       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -0.499       |\n","|    explained_variance   | -0.00737     |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 57.9         |\n","|    n_updates            | 360          |\n","|    policy_gradient_loss | -0.00947     |\n","|    value_loss           | 118          |\n","------------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 42          |\n","|    ep_rew_mean          | 42          |\n","| time/                   |             |\n","|    fps                  | 46          |\n","|    iterations           | 38          |\n","|    time_elapsed         | 105         |\n","|    total_timesteps      | 4864        |\n","| train/                  |             |\n","|    approx_kl            | 0.012406364 |\n","|    clip_fraction        | 0.0477      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.544      |\n","|    explained_variance   | -0.011      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 57.3        |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.00732    |\n","|    value_loss           | 116         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 42         |\n","|    ep_rew_mean          | 42         |\n","| time/                   |            |\n","|    fps                  | 45         |\n","|    iterations           | 39         |\n","|    time_elapsed         | 110        |\n","|    total_timesteps      | 4992       |\n","| train/                  |            |\n","|    approx_kl            | 0.01798037 |\n","|    clip_fraction        | 0.307      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -0.557     |\n","|    explained_variance   | -0.000203  |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 64.9       |\n","|    n_updates            | 380        |\n","|    policy_gradient_loss | -0.00832   |\n","|    value_loss           | 132        |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 43.4        |\n","|    ep_rew_mean          | 43.4        |\n","| time/                   |             |\n","|    fps                  | 45          |\n","|    iterations           | 40          |\n","|    time_elapsed         | 111         |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.009599838 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -0.507      |\n","|    explained_variance   | -2.38e-07   |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 80.7        |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.00333    |\n","|    value_loss           | 164         |\n","-----------------------------------------\n","181.3\n"]},{"output_type":"error","ename":"DisabledFunctionError","evalue":"cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDisabledFunctionError\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f11836a8ecfb>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVecEnvObs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vecenv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_cv2.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mDisabledFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDisabledFunctionError\u001b[0m: cv2.imshow() is disabled in Colab, because it causes Jupyter sessions\nto crash; see https://github.com/jupyter/notebook/issues/3935.\nAs a substitution, consider using\n  from google.colab.patches import cv2_imshow\n"],"errorDetails":{"actions":[{"action":"open_snippet","actionText":"Search Snippets for cv2.imshow","snippetFilter":"cv2.imshow"}]}}]},{"cell_type":"code","source":[],"metadata":{"id":"9ZSi1jqO8UEx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DI-Engine try"],"metadata":{"id":"O6qQ2sRIYioW"}},{"cell_type":"code","source":["!pip install DI-engine"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IhaES5KIVnUb","executionInfo":{"status":"ok","timestamp":1709135085123,"user_tz":0,"elapsed":47150,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"09fdb448-8dd8-44c5-d556-d31251d53824"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting DI-engine\n","  Downloading DI_engine-0.5.1-py3-none-any.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools<=66.1.1 (from DI-engine)\n","  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yapf==0.29.0 (from DI-engine)\n","  Downloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gym==0.25.1 (from DI-engine)\n","  Downloading gym-0.25.1.tar.gz (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting gymnasium (from DI-engine)\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.1.0+cu121)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.25.2)\n","Collecting DI-treetensor>=0.4.0 (from DI-engine)\n","  Downloading DI_treetensor-0.4.1-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting DI-toolkit>=0.1.0 (from DI-engine)\n","  Downloading DI_toolkit-0.2.1-py3-none-any.whl (29 kB)\n","Collecting trueskill (from DI-engine)\n","  Downloading trueskill-0.4.5.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorboardX>=2.2 (from DI-engine)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb (from DI-engine)\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from DI-engine) (3.7.1)\n","Collecting easydict==1.9 (from DI-engine)\n","  Downloading easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from DI-engine) (6.0.1)\n","Collecting enum-tools (from DI-engine)\n","  Downloading enum_tools-0.11.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.2.1)\n","Collecting hickle (from DI-engine)\n","  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from DI-engine) (0.9.0)\n","Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (8.1.7)\n","Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.31.0)\n","Collecting flask~=1.1.2 (from DI-engine)\n","  Downloading Flask-1.1.4-py2.py3-none-any.whl (94 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses~=0.12.1 (from DI-engine)\n","  Downloading responses-0.12.1-py2.py3-none-any.whl (16 kB)\n","Collecting URLObject>=2.4.0 (from DI-engine)\n","  Downloading URLObject-2.4.3.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting MarkupSafe==2.0.1 (from DI-engine)\n","  Downloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n","Collecting pynng (from DI-engine)\n","  Downloading pynng-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.4/936.4 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.3.0)\n","Collecting redis (from DI-engine)\n","  Downloading redis-5.0.2-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mpire>=2.3.5 (from DI-engine)\n","  Downloading mpire-2.10.0-py3-none-any.whl (272 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.1/272.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1->DI-engine) (0.0.8)\n","Collecting hbutils>=0.9.1 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading hbutils-0.9.3-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=12.2.0 in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (13.7.0)\n","Collecting yattag>=1.14.0 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading yattag-1.15.2.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.5.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (2.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (4.66.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (0.13.1)\n","Collecting treevalue>=1.4.11 (from DI-treetensor>=0.4.0->DI-engine)\n","  Downloading treevalue-1.4.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Werkzeug<2.0,>=0.15 (from flask~=1.1.2->DI-engine)\n","  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Jinja2<3.0,>=2.10.1 (from flask~=1.1.2->DI-engine)\n","  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itsdangerous<2.0,>=0.24 (from flask~=1.1.2->DI-engine)\n","  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting click>=7.0.0 (from DI-engine)\n","  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire>=2.3.5->DI-engine) (2.16.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2024.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from responses~=0.12.1->DI-engine) (1.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (23.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.2.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2.1.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium->DI-engine)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle->DI-engine) (3.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (2.8.2)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from pynng->DI-engine) (1.16.0)\n","Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis->DI-engine) (4.0.3)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->DI-engine)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb->DI-engine)\n","  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->DI-engine)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb->DI-engine)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (1.4.4)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytimeparse>=1.1.8 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n","Collecting bitmath>=1.3.3.1 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading bitmath-1.3.3.1.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting chardet<5,>=3.0.4 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deprecation>=2.0.0 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (3.0.0)\n","Requirement already satisfied: graphviz>=0.17 in /usr/local/lib/python3.10/dist-packages (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine) (0.20.1)\n","Collecting dill>=0.3.4 (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->pynng->DI-engine) (2.21)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->DI-toolkit>=0.1.0->DI-engine) (2023.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (3.3.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->DI-engine) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.60.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.7.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (0.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.2.2)\n","Building wheels for collected packages: easydict, gym, URLObject, trueskill, yattag, bitmath\n","  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6344 sha256=1d53885f4514d33a744fcb14d06318311ce1fa62ac4926c697705442ee25fcac\n","  Stored in directory: /root/.cache/pip/wheels/fd/d2/35/4c11d19a72280492846f4c4df975311a2bac475e8021f86c1d\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.25.1-py3-none-any.whl size=849018 sha256=581ab4810af46b85814ffba08098bf722c6d5b22130c19db637f199225203418\n","  Stored in directory: /root/.cache/pip/wheels/2e/3b/df/78994c45c86a980cd5d8404c6d38cd28b871d5120e45c32ce4\n","  Building wheel for URLObject (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for URLObject: filename=URLObject-2.4.3-py3-none-any.whl size=14511 sha256=3919d5b90234e5c941faee6c9cfec499c04e85a3768d34f6d64910e103ecf17c\n","  Stored in directory: /root/.cache/pip/wheels/0d/a2/8a/05c4a3cbe66487af088bc8967fad6de3cc30f4680a5e2e27b8\n","  Building wheel for trueskill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for trueskill: filename=trueskill-0.4.5-py3-none-any.whl size=18049 sha256=423f35aaf68ce3aba3ef50418f63b0a4c6343bd9de50e13c36c0e457abe949a5\n","  Stored in directory: /root/.cache/pip/wheels/b9/4f/29/c79f0a2956775524c7a23638ac2b6fbb516c680f8e5eed9b53\n","  Building wheel for yattag (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yattag: filename=yattag-1.15.2-py3-none-any.whl size=15668 sha256=f8d0cae56863b153c0247a8cece0e3c774292d49304e367c6f2fbd4043180f06\n","  Stored in directory: /root/.cache/pip/wheels/3f/6e/e5/d526243c27041915f63eacc0804babeb86b6973b0bc1991f06\n","  Building wheel for bitmath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bitmath: filename=bitmath-1.3.3.1-py3-none-any.whl size=23976 sha256=1bfc12951cbb6d93d2c14108920b139c19da931cb260b64b25ca7fac02be0164\n","  Stored in directory: /root/.cache/pip/wheels/2d/32/d2/936069b5a9583c55bc7c7dce6c746a543ce61d7dfbb4013e13\n","Successfully built easydict gym URLObject trueskill yattag bitmath\n","Installing collected packages: yattag, yapf, URLObject, pytimeparse, farama-notifications, easydict, bitmath, Werkzeug, trueskill, tensorboardX, smmap, setuptools, setproctitle, sentry-sdk, redis, mpire, MarkupSafe, itsdangerous, gymnasium, gym, enum-tools, docker-pycreds, dill, deprecation, click, chardet, responses, pynng, Jinja2, hickle, hbutils, gitdb, treevalue, GitPython, flask, wandb, DI-treetensor, DI-toolkit, DI-engine\n","  Attempting uninstall: easydict\n","    Found existing installation: easydict 1.12\n","    Uninstalling easydict-1.12:\n","      Successfully uninstalled easydict-1.12\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.0.1\n","    Uninstalling Werkzeug-3.0.1:\n","      Successfully uninstalled Werkzeug-3.0.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: itsdangerous\n","    Found existing installation: itsdangerous 2.1.2\n","    Uninstalling itsdangerous-2.1.2:\n","      Successfully uninstalled itsdangerous-2.1.2\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 3.1.3\n","    Uninstalling Jinja2-3.1.3:\n","      Successfully uninstalled Jinja2-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 2.2.5\n","    Uninstalling Flask-2.2.5:\n","      Successfully uninstalled Flask-2.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","branca 0.7.1 requires jinja2>=3, but you have jinja2 2.11.3 which is incompatible.\n","dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n","nbconvert 6.5.4 requires jinja2>=3.0, but you have jinja2 2.11.3 which is incompatible.\n","pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed DI-engine-0.5.1 DI-toolkit-0.2.1 DI-treetensor-0.4.1 GitPython-3.1.42 Jinja2-2.11.3 MarkupSafe-2.0.1 URLObject-2.4.3 Werkzeug-1.0.1 bitmath-1.3.3.1 chardet-4.0.0 click-7.1.2 deprecation-2.1.0 dill-0.3.8 docker-pycreds-0.4.0 easydict-1.9 enum-tools-0.11.0 farama-notifications-0.0.4 flask-1.1.4 gitdb-4.0.11 gym-0.25.1 gymnasium-0.29.1 hbutils-0.9.3 hickle-5.0.2 itsdangerous-1.1.0 mpire-2.10.0 pynng-0.8.0 pytimeparse-1.1.8 redis-5.0.2 responses-0.12.1 sentry-sdk-1.40.6 setproctitle-1.3.3 setuptools-66.1.1 smmap-5.0.1 tensorboardX-2.6.2.2 treevalue-1.4.12 trueskill-0.4.5 wandb-0.16.3 yapf-0.29.0 yattag-1.15.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","pkg_resources","setuptools"]},"id":"3f3eb40b20c64b19a3f09797fa8930ad"}},"metadata":{}}]},{"cell_type":"code","source":["import gym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbV6LJZ7cjBx","executionInfo":{"status":"ok","timestamp":1709136039757,"user_tz":0,"elapsed":3,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"266773a7-ccc0-4f76-fb2c-0e4fa5bde65b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["from dizoo.classic_control.cartpole.config.cartpole_dqn_config import main_config, create_config\n","from ding.config import compile_config\n","\n","cfg = compile_config(main_config, create_cfg=create_config, auto=True)"],"metadata":{"id":"bn-wfcF5Yh0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ding.envs import DingEnvWrapper, BaseEnvManagerV2\n","\n","collector_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"CartPole-v0\")) for _ in range(cfg.env.collector_env_num)],\n","    cfg=cfg.env.manager\n",")\n","evaluator_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"CartPole-v0\")) for _ in range(cfg.env.evaluator_env_num)],\n","    cfg=cfg.env.manager\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOGThqbucQeP","executionInfo":{"status":"ok","timestamp":1709136042024,"user_tz":0,"elapsed":283,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"0d3f78b4-fd2b-42fd-fc70-6f6ecbb6ceea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["from ding.model import DQN\n","from ding.policy import DQNPolicy\n","from ding.data import DequeBuffer\n","\n","model = DQN(**cfg.policy.model)\n","buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\n","policy = DQNPolicy(cfg.policy, model=model)"],"metadata":{"id":"GI0dw-K0coRK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"undbHuwKhgb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ding.framework import task\n","from ding.framework.context import OnlineRLContext\n","from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, eps_greedy_handler, CkptSaver\n","\n","import logging\n","logging.getLogger().setLevel(logging.INFO)\n","\n","with task.start(async_mode=False, ctx=OnlineRLContext()):\n","    # Evaluating, we place it on the first place to get the score of the random model as a benchmark value\n","    task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\n","    task.use(eps_greedy_handler(cfg))  # Decay probability of explore-exploit\n","    task.use(StepCollector(cfg, policy.collect_mode, collector_env))  # Collect environmental data\n","    task.use(data_pusher(cfg, buffer_))  # Push data to buffer\n","    task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))  # Train the model\n","    task.use(CkptSaver(policy, cfg.exp_name, train_freq=100))  # Save the model\n","    # In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\n","    task.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsz57BjocmIB","executionInfo":{"status":"ok","timestamp":1709136722567,"user_tz":0,"elapsed":8838,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"b294e166-01b8-4216-c9ba-7518977a5b57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Evaluation: Train Iter(0) Env Step(0) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(8) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(16) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(24) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(32) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(40) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(48) Episode Return(12.200) \n","INFO:root:Evaluation: Train Iter(0) Env Step(56) Episode Return(12.200) \n","INFO:root:Training: Train Iter(0)\tEnv Step(64)\tLoss(0.984)\n","INFO:root:Evaluation: Train Iter(40) Env Step(120) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(80) Env Step(184) Episode Return(21.800) \n","INFO:root:collect end:\n","episode_count: 10\n","envstep_count: 125\n","train_sample_count: 500\n","avg_envstep_per_episode: 12.5\n","avg_sample_per_episode: 50.0\n","avg_envstep_per_sec: 9982.61142752421\n","avg_train_sample_per_sec: 39930.44571009684\n","avg_episode_per_sec: 798.6089142019368\n","reward_mean: 23.4\n","reward_std: 9.28654941299512\n","reward_max: 45.0\n","reward_min: 11.0\n","total_envstep_count: 224\n","total_train_sample_count: 896\n","total_episode_count: 10\n","INFO:root:Training: Train Iter(100)\tEnv Step(224)\tLoss(0.884)\n","INFO:root:Evaluation: Train Iter(120) Env Step(248) Episode Return(9.600) \n","INFO:root:Evaluation: Train Iter(160) Env Step(312) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(200) Env Step(376) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 4\n","envstep_count: 144\n","train_sample_count: 576\n","avg_envstep_per_episode: 36.0\n","avg_sample_per_episode: 144.0\n","avg_envstep_per_sec: 10559.410051880957\n","avg_train_sample_per_sec: 42237.64020752383\n","avg_episode_per_sec: 293.3169458855822\n","reward_mean: 15.25\n","reward_std: 2.48746859276655\n","reward_max: 19.0\n","reward_min: 12.0\n","total_envstep_count: 384\n","total_train_sample_count: 1536\n","total_episode_count: 14\n","INFO:root:Training: Train Iter(200)\tEnv Step(384)\tLoss(0.879)\n","INFO:root:Evaluation: Train Iter(240) Env Step(440) Episode Return(12.400) \n","INFO:root:Evaluation: Train Iter(280) Env Step(504) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 7\n","envstep_count: 420\n","train_sample_count: 1680\n","avg_envstep_per_episode: 60.0\n","avg_sample_per_episode: 240.0\n","avg_envstep_per_sec: 8231.38367222732\n","avg_train_sample_per_sec: 32925.53468890928\n","avg_episode_per_sec: 137.18972787045533\n","reward_mean: 33.857142857142854\n","reward_std: 15.532717280709418\n","reward_max: 57.0\n","reward_min: 12.0\n","total_envstep_count: 544\n","total_train_sample_count: 2176\n","total_episode_count: 21\n","INFO:root:Training: Train Iter(300)\tEnv Step(544)\tLoss(0.895)\n","INFO:root:Evaluation: Train Iter(320) Env Step(568) Episode Return(48.200) \n","INFO:root:Evaluation: Train Iter(360) Env Step(632) Episode Return(10.800) \n","INFO:root:Evaluation: Train Iter(400) Env Step(696) Episode Return(56.000) \n","INFO:root:collect end:\n","episode_count: 6\n","envstep_count: 487\n","train_sample_count: 1948\n","avg_envstep_per_episode: 81.16666666666667\n","avg_sample_per_episode: 324.6666666666667\n","avg_envstep_per_sec: 9595.503652995385\n","avg_train_sample_per_sec: 38382.01461198154\n","avg_episode_per_sec: 118.21975753177067\n","reward_mean: 29.666666666666668\n","reward_std: 16.62995957768856\n","reward_max: 63.0\n","reward_min: 15.0\n","total_envstep_count: 704\n","total_train_sample_count: 2816\n","total_episode_count: 27\n","INFO:root:Training: Train Iter(400)\tEnv Step(704)\tLoss(0.789)\n","INFO:root:Evaluation: Train Iter(440) Env Step(760) Episode Return(14.600) \n","INFO:root:Evaluation: Train Iter(480) Env Step(824) Episode Return(16.400) \n","INFO:root:collect end:\n","episode_count: 7\n","envstep_count: 711\n","train_sample_count: 2844\n","avg_envstep_per_episode: 101.57142857142857\n","avg_sample_per_episode: 406.2857142857143\n","avg_envstep_per_sec: 8642.501226067769\n","avg_train_sample_per_sec: 34570.004904271074\n","avg_episode_per_sec: 85.08791643104695\n","reward_mean: 22.285714285714285\n","reward_std: 4.060762972443398\n","reward_max: 29.0\n","reward_min: 16.0\n","total_envstep_count: 864\n","total_train_sample_count: 3456\n","total_episode_count: 34\n","INFO:root:Training: Train Iter(500)\tEnv Step(864)\tLoss(0.722)\n","INFO:root:Evaluation: Train Iter(520) Env Step(888) Episode Return(22.000) \n","INFO:root:Evaluation: Train Iter(560) Env Step(952) Episode Return(33.600) \n","INFO:root:Evaluation: Train Iter(600) Env Step(1016) Episode Return(79.200) \n","INFO:root:collect end:\n","episode_count: 2\n","envstep_count: 242\n","train_sample_count: 968\n","avg_envstep_per_episode: 121.0\n","avg_sample_per_episode: 484.0\n","avg_envstep_per_sec: 9540.07987179909\n","avg_train_sample_per_sec: 38160.31948719636\n","avg_episode_per_sec: 78.8436353041247\n","reward_mean: 20.5\n","reward_std: 4.5\n","reward_max: 25.0\n","reward_min: 16.0\n","total_envstep_count: 1024\n","total_train_sample_count: 4096\n","total_episode_count: 36\n","INFO:root:Training: Train Iter(600)\tEnv Step(1024)\tLoss(1.031)\n","INFO:root:Evaluation: Train Iter(640) Env Step(1080) Episode Return(52.600) \n","INFO:root:Evaluation: Train Iter(680) Env Step(1144) Episode Return(68.800) \n","INFO:root:collect end:\n","episode_count: 6\n","envstep_count: 817\n","train_sample_count: 3268\n","avg_envstep_per_episode: 136.16666666666666\n","avg_sample_per_episode: 544.6666666666666\n","avg_envstep_per_sec: 8839.042124622436\n","avg_train_sample_per_sec: 35356.168498489744\n","avg_episode_per_sec: 64.91340605597873\n","reward_mean: 41.333333333333336\n","reward_std: 28.905977851571734\n","reward_max: 103.0\n","reward_min: 17.0\n","total_envstep_count: 1184\n","total_train_sample_count: 4736\n","total_episode_count: 42\n","INFO:root:Training: Train Iter(700)\tEnv Step(1184)\tLoss(1.010)\n","INFO:root:Evaluation: Train Iter(720) Env Step(1208) Episode Return(138.800) \n","INFO:root:Evaluation: Train Iter(760) Env Step(1272) Episode Return(187.200) \n","INFO:root:Evaluation: Train Iter(800) Env Step(1336) Episode Return(165.000) \n","INFO:root:collect end:\n","episode_count: 3\n","envstep_count: 495\n","train_sample_count: 1980\n","avg_envstep_per_episode: 165.0\n","avg_sample_per_episode: 660.0\n","avg_envstep_per_sec: 10875.100972899074\n","avg_train_sample_per_sec: 43500.403891596296\n","avg_episode_per_sec: 65.909702866055\n","reward_mean: 43.666666666666664\n","reward_std: 19.154343864744856\n","reward_max: 70.0\n","reward_min: 25.0\n","total_envstep_count: 1344\n","total_train_sample_count: 5376\n","total_episode_count: 45\n","INFO:root:Training: Train Iter(800)\tEnv Step(1344)\tLoss(0.768)\n","INFO:root:Evaluation: Train Iter(840) Env Step(1400) Episode Return(200.000) \n"]}]},{"cell_type":"markdown","source":["# DI-Engine GAIL\n","since it does not has expert data, we train a dqn as the expert, then perform gail"],"metadata":{"id":"Qr4y5YZ_3oyT"}},{"cell_type":"markdown","source":["## preparations for the gail pipeline"],"metadata":{"id":"loIPcTr-5H70"}},{"cell_type":"code","source":["from typing import Optional, Callable, List, Any\n","\n","from ding.policy import PolicyFactory\n","from ding.worker import IMetric, MetricSerialEvaluator\n","\n","\n","class AccMetric(IMetric):\n","\n","    def eval(self, inputs: Any, label: Any) -> dict:\n","        return {'Acc': (inputs['logit'].sum(dim=1) == label).sum().item() / label.shape[0]}\n","\n","    def reduce_mean(self, inputs: List[Any]) -> Any:\n","        s = 0\n","        for item in inputs:\n","            s += item['Acc']\n","        return {'Acc': s / len(inputs)}\n","\n","    def gt(self, metric1: Any, metric2: Any) -> bool:\n","        if metric2 is None:\n","            return True\n","        if isinstance(metric2, dict):\n","            m2 = metric2['Acc']\n","        else:\n","            m2 = metric2\n","        return metric1['Acc'] > m2\n","\n","\n","def mark_not_expert(ori_data: List[dict]) -> List[dict]:\n","    for i in range(len(ori_data)):\n","        # Set is_expert flag (expert 1, agent 0)\n","        ori_data[i]['is_expert'] = 0\n","    return ori_data\n","\n","\n","def mark_warm_up(ori_data: List[dict]) -> List[dict]:\n","    # for td3_vae\n","    for i in range(len(ori_data)):\n","        ori_data[i]['warm_up'] = True\n","    return ori_data\n","\n","\n","def random_collect( #粘过来是为了用这个\n","        policy_cfg: 'EasyDict',  # noqa\n","        policy: 'Policy',  # noqa\n","        collector: 'ISerialCollector',  # noqa\n","        collector_env: 'BaseEnvManager',  # noqa\n","        commander: 'BaseSerialCommander',  # noqa\n","        replay_buffer: 'IBuffer',  # noqa\n","        postprocess_data_fn: Optional[Callable] = None\n",") -> None:  # noqa\n","    assert policy_cfg.random_collect_size > 0\n","    if policy_cfg.get('transition_with_policy_data', False):\n","        collector.reset_policy(policy.collect_mode)\n","    else:\n","        action_space = collector_env.action_space\n","        random_policy = PolicyFactory.get_random_policy(policy.collect_mode, action_space=action_space)\n","        collector.reset_policy(random_policy)\n","    collect_kwargs = commander.step()\n","    if policy_cfg.collect.collector.type == 'episode':\n","        new_data = collector.collect(n_episode=policy_cfg.random_collect_size, policy_kwargs=collect_kwargs)\n","    else:\n","        new_data = collector.collect(\n","            n_sample=policy_cfg.random_collect_size,\n","            random_collect=True,\n","            record_random_collect=False,\n","            policy_kwargs=collect_kwargs\n","        )  # 'record_random_collect=False' means random collect without output log\n","    if postprocess_data_fn is not None:\n","        new_data = postprocess_data_fn(new_data)\n","    replay_buffer.push(new_data, cur_collector_envstep=0)\n","    collector.reset_policy(policy.collect_mode)"],"metadata":{"id":"BLpa8r6Lr91o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Tuple\n","import os\n","import torch\n","from ditk import logging\n","from functools import partial\n","from tensorboardX import SummaryWriter\n","from copy import deepcopy\n","import numpy as np\n","\n","from ding.envs import get_vec_env_setting, create_env_manager\n","from ding.worker import BaseLearner, InteractionSerialEvaluator, BaseSerialCommander, create_buffer, \\\n","    create_serial_collector\n","from ding.config import read_config, compile_config\n","from ding.policy import create_policy\n","from ding.reward_model import create_reward_model\n","from ding.utils import set_pkg_seed\n","from ding.entry import collect_demo_data\n","from ding.utils import save_file\n","#from .utils import random_collect\n","\n","\n","def save_reward_model(path, reward_model, weights_name='best'):\n","    path = os.path.join(path, 'reward_model', 'ckpt')\n","    if not os.path.exists(path):\n","        try:\n","            os.makedirs(path)\n","        except FileExistsError:\n","            pass\n","    path = os.path.join(path, 'ckpt_{}.pth.tar'.format(weights_name))\n","    state_dict = reward_model.state_dict()\n","    save_file(path, state_dict)\n","    print('Saved reward model ckpt in {}'.format(path))\n","\n","\n","def serial_pipeline_gail(\n","        input_cfg: Tuple[dict, dict],\n","        expert_cfg: Tuple[dict, dict],\n","        seed: int = 0,\n","        model: Optional[torch.nn.Module] = None,\n","        max_train_iter: Optional[int] = int(1e10),\n","        max_env_step: Optional[int] = int(1e10),\n","        collect_data: bool = True,\n",") -> 'Policy':  # noqa\n","    \"\"\"\n","    Overview:\n","        Serial pipeline entry for GAIL reward model.\n","    Arguments:\n","        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type. \\\n","            ``str`` type means config file path. \\\n","            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n","        - expert_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Expert config in dict type. \\\n","            ``str`` type means config file path. \\\n","            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n","        - seed (:obj:`int`): Random seed.\n","        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\n","        - max_train_iter (:obj:`Optional[int]`): Maximum policy update iterations in training.\n","        - max_env_step (:obj:`Optional[int]`): Maximum collected environment interaction steps.\n","        - collect_data (:obj:`bool`): Collect expert data.\n","    Returns:\n","        - policy (:obj:`Policy`): Converged policy.\n","    \"\"\"\n","    if isinstance(input_cfg, str):\n","        cfg, create_cfg = read_config(input_cfg)\n","    else:\n","        cfg, create_cfg = deepcopy(input_cfg)\n","    if isinstance(expert_cfg, str):\n","        expert_cfg, expert_create_cfg = read_config(expert_cfg)\n","    else:\n","        expert_cfg, expert_create_cfg = expert_cfg\n","    create_cfg.policy.type = create_cfg.policy.type + '_command'\n","    cfg = compile_config(cfg, seed=seed, auto=True, create_cfg=create_cfg, save_cfg=True)\n","    if 'data_path' not in cfg.reward_model:\n","        cfg.reward_model.data_path = cfg.exp_name\n","    # Load expert data\n","    if collect_data:\n","        if expert_cfg.policy.get('other', None) is not None and expert_cfg.policy.other.get('eps', None) is not None:\n","            expert_cfg.policy.other.eps.collect = -1\n","        if expert_cfg.policy.get('load_path', None) is None:\n","            expert_cfg.policy.load_path = cfg.reward_model.expert_model_path\n","        collect_demo_data(\n","            (expert_cfg, expert_create_cfg),\n","            seed,\n","            state_dict_path=expert_cfg.policy.load_path,\n","            expert_data_path=cfg.reward_model.data_path + '/expert_data.pkl',\n","            collect_count=cfg.reward_model.collect_count\n","        )\n","    # Create main components: env, policy\n","    env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg.env)\n","    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n","    evaluator_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in evaluator_env_cfg])\n","    collector_env.seed(cfg.seed)\n","    evaluator_env.seed(cfg.seed, dynamic_seed=False)\n","    set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)\n","    policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval', 'command'])\n","\n","    # Create worker components: learner, collector, evaluator, replay buffer, commander.\n","    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n","    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)\n","    collector = create_serial_collector(\n","        cfg.policy.collect.collector,\n","        env=collector_env,\n","        policy=policy.collect_mode,\n","        tb_logger=tb_logger,\n","        exp_name=cfg.exp_name\n","    )\n","    evaluator = InteractionSerialEvaluator(\n","        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name\n","    )\n","    replay_buffer = create_buffer(cfg.policy.other.replay_buffer, tb_logger=tb_logger, exp_name=cfg.exp_name)\n","    commander = BaseSerialCommander(\n","        cfg.policy.other.commander, learner, collector, evaluator, replay_buffer, policy.command_mode\n","    )\n","    reward_model = create_reward_model(cfg.reward_model, policy.collect_mode.get_attribute('device'), tb_logger)\n","\n","    # ==========\n","    # Main loop\n","    # ==========\n","    # Learner's before_run hook.\n","    learner.call_hook('before_run')\n","\n","    # Accumulate plenty of data at the beginning of training.\n","    if cfg.policy.get('random_collect_size', 0) > 0:\n","        random_collect(cfg.policy, policy, collector, collector_env, commander, replay_buffer)\n","    best_reward = -np.inf\n","    while True:\n","        collect_kwargs = commander.step()\n","        # Evaluate policy performance\n","        if evaluator.should_eval(learner.train_iter):\n","            stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)\n","            print(\"look here\")\n","            print(reward)\n","            # reward_mean = np.array([r['eval_episode_return'] for r in reward]).mean()\n","            reward_mean = np.array(reward['eval_episode_return']).mean()\n","            if reward_mean >= best_reward:\n","                save_reward_model(cfg.exp_name, reward_model, 'best')\n","                best_reward = reward_mean\n","            if stop:\n","                break\n","        new_data_count, target_new_data_count = 0, cfg.reward_model.get('target_new_data_count', 1)\n","        while new_data_count < target_new_data_count:\n","            new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs=collect_kwargs)\n","            new_data_count += len(new_data)\n","            # collect data for reward_model training\n","            reward_model.collect_data(new_data)\n","            replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)\n","        # update reward_model\n","        reward_model.train()\n","        reward_model.clear_data()\n","        # Learn policy from collected data\n","        for i in range(cfg.policy.learn.update_per_collect):\n","            # Learner will train ``update_per_collect`` times in one iteration.\n","            train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)\n","            if train_data is None:\n","                # It is possible that replay buffer's data count is too few to train ``update_per_collect`` times\n","                logging.warning(\n","                    \"Replay buffer's data can only train for {} steps. \".format(i) +\n","                    \"You can modify data collect config, e.g. increasing n_sample, n_episode.\"\n","                )\n","                break\n","            # update train_data reward using the augmented reward\n","            train_data_augmented = reward_model.estimate(train_data)\n","            learner.train(train_data_augmented, collector.envstep)\n","            if learner.policy.get_attribute('priority'):\n","                replay_buffer.update(learner.priority_info)\n","        if collector.envstep >= max_env_step or learner.train_iter >= max_train_iter:\n","            break\n","\n","    # Learner's after_run hook.\n","    learner.call_hook('after_run')\n","    save_reward_model(cfg.exp_name, reward_model, 'last')\n","    # evaluate\n","    # evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)\n","    return policy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gwrIwt1rql2","executionInfo":{"status":"ok","timestamp":1709140395483,"user_tz":0,"elapsed":385,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"11bffb12-faad-45bd-edc3-3a64ce17f64b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["# classic_control cartpole_dqn_gail_config.py\n","from easydict import EasyDict\n","\n","cartpole_dqn_gail_config = dict(\n","    exp_name='cartpole_dqn_gail_seed0',\n","    env=dict(\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        n_evaluator_episode=5,\n","        stop_value=195,\n","    ),\n","    reward_model=dict(\n","        type='gail',\n","        input_size=5,\n","        hidden_size=64,\n","        batch_size=64,\n","        learning_rate=1e-3,\n","        update_per_collect=100,\n","        # Users should add their own model path here. Model path should lead to a model.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is ``exp_name/ckpt/ckpt_best.pth.tar``.\n","        # If collect_data is True, we will use this expert_model_path to collect expert data first, rather than we\n","        # will load data directly from user-defined data_path\n","        expert_model_path='cartpole_dqn_seed0/ckpt/ckpt_best.pth.tar',\n","\n","        collect_count=1000,\n","    ),\n","    policy=dict(\n","        cuda=False,\n","        model=dict(\n","            obs_shape=4,\n","            action_shape=2,\n","            encoder_hidden_size_list=[128, 128, 64],\n","            dueling=True,\n","        ),\n","        nstep=1,\n","        discount_factor=0.97,\n","        learn=dict(\n","            batch_size=64,\n","            learning_rate=0.001,\n","            update_per_collect=3,\n","        ),\n","        collect=dict(n_sample=64),\n","        eval=dict(evaluator=dict(eval_freq=10, )),\n","        other=dict(\n","            eps=dict(\n","                type='exp',\n","                start=0.95,\n","                end=0.1,\n","                decay=10000,\n","            ),\n","            replay_buffer=dict(replay_buffer_size=20000, ),\n","        ),\n","    ),\n",")\n","cartpole_dqn_gail_config = EasyDict(cartpole_dqn_gail_config)\n","main_config = cartpole_dqn_gail_config\n","cartpole_dqn_gail_create_config = dict(\n","    env=dict(\n","        type='cartpole',\n","        import_names=['dizoo.classic_control.cartpole.envs.cartpole_env'],\n","    ),\n","    env_manager=dict(type='base'),\n","    policy=dict(type='dqn'),\n",")\n","cartpole_dqn_gail_create_config = EasyDict(cartpole_dqn_gail_create_config)\n","create_config = cartpole_dqn_gail_create_config\n"],"metadata":{"id":"NPzZcyP7kvkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from ding.entry import serial_pipeline_gail\n","from dizoo.classic_control.cartpole.config import cartpole_dqn_config, cartpole_dqn_create_config\n","expert_main_config = cartpole_dqn_config\n","expert_create_config = cartpole_dqn_create_config\n","serial_pipeline_gail(\n","    (main_config, create_config), (expert_main_config, expert_create_config),\n","    max_env_step=1000000,\n","    seed=0,\n","    collect_data=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6vBlzorgkwmv","executionInfo":{"status":"ok","timestamp":1709140522984,"user_tz":0,"elapsed":122072,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"22550bd4-8ffa-428b-8ecb-e1d5a5a5747e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:collector_logger:File './default_experiment/log/collector/collector_logger.txt' has already been added to logger <Logger collector_logger (INFO)>, so this configuration will be ignored.\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","INFO:learner_logger:[RANK0]: DI-engine DRL Policy\n","DQN(\n","  (encoder): FCEncoder(\n","    (act): ReLU()\n","    (init): Linear(in_features=4, out_features=128, bias=True)\n","    (main): Sequential(\n","      (0): Linear(in_features=128, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=128, out_features=64, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (head): DuelingHead(\n","    (A): Sequential(\n","      (0): Sequential(\n","        (0): Linear(in_features=64, out_features=64, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): Sequential(\n","        (0): Linear(in_features=64, out_features=2, bias=True)\n","      )\n","    )\n","    (V): Sequential(\n","      (0): Sequential(\n","        (0): Linear(in_features=64, out_features=64, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): Sequential(\n","        (0): Linear(in_features=64, out_features=1, bias=True)\n","      )\n","    )\n","  )\n",")\n"]},{"output_type":"stream","name":"stdout","text":["Collect demo data successfully\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+---------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name           | episode_count | envstep_count |\n","+-------+------------+---------------------+---------------+---------------+\n","| Value | 0.000000   | iteration_0.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+---------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.031790      | 1730.128773         | 157.284434           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 0 Times ===\n","INFO:buffer_logger:\n","+-------+----------+----------+--------------+--------------+\n","| Name  | use_avg  | use_max  | priority_avg | priority_max |\n","+-------+----------+----------+--------------+--------------+\n","| Value | 1.000000 | 1.000000 | 1.000000     | 1.000000     |\n","+-------+----------+----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 0.000000      | 0.000000      | 0.400006 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 0 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 0.266589       | 0.074841    | 0.075293           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/iteration_0.pth.tar\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 12.000000  | iteration_12.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.031092      | 1768.961413         | 160.814674           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 24.000000  | iteration_24.pth.tar | 5.000000      | 50.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 10.000000               | 0.028830      | 1734.287108         | 173.428711           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 0.800000   | 10.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [8.0, 9.0, 10.0, 10.0, 10.0] | 9.400000                 |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [8.0, 9.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 9.4}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 36.000000  | iteration_36.pth.tar | 5.000000      | 70.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 14.000000               | 0.033663      | 2079.461722         | 148.532980           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 11.800000   | 1.469694   | 14.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 13.0, 11.0, 11.0, 10.0] | 11.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 13.0, 11.0, 11.0, 10.0], 'eval_episode_return_mean': 11.8}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 11.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 12.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 12.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 48.000000  | iteration_48.pth.tar | 5.000000      | 80.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 16.000000               | 0.040742      | 1963.579504         | 122.723719           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 13.000000   | 1.788854   | 16.000000  | 11.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [16.0, 14.0, 12.0, 12.0, 11.0] | 13.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [16.0, 14.0, 12.0, 12.0, 11.0], 'eval_episode_return_mean': 13.0}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 60.000000  | iteration_60.pth.tar | 5.000000      | 60.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 12.000000               | 0.031790      | 1887.356587         | 157.279716           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 10.000000   | 1.264911   | 12.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [12.0, 11.0, 9.0, 9.0, 9.0] | 10.000000                |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [12.0, 11.0, 9.0, 9.0, 9.0], 'eval_episode_return_mean': 10.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 72.000000  | iteration_72.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.029968      | 1835.274949         | 166.843177           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 84.000000  | iteration_84.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033319      | 1650.733606         | 150.066691           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 20.0000, current episode: 5\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name            | episode_count | envstep_count |\n","+-------+------------+----------------------+---------------+---------------+\n","| Value | 96.000000  | iteration_96.pth.tar | 5.000000      | 100.000000    |\n","+-------+------------+----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 20.000000               | 0.073817      | 1354.709973         | 67.735499            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 11.800000   | 4.118252   | 20.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-------------------------------+--------------------------+\n","| Name  | eval_episode_return           | eval_episode_return_mean |\n","+-------+-------------------------------+--------------------------+\n","| Value | [20.0, 9.0, 10.0, 10.0, 10.0] | 11.800000                |\n","+-------+-------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [20.0, 9.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 11.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 100 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 0.679317       | 0.575857    | 0.108209           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 128\n","envstep_count: 2404\n","train_sample_count: 2404\n","avg_envstep_per_episode: 18.78125\n","avg_sample_per_episode: 18.78125\n","avg_envstep_per_sec: 3795.659850669113\n","avg_train_sample_per_sec: 3795.659850669113\n","avg_episode_per_sec: 202.09836143329719\n","reward_mean: 18.78125\n","reward_std: 8.023342410585503\n","reward_max: 44.0\n","reward_min: 9.0\n","total_envstep_count: 2464\n","total_train_sample_count: 2444\n","total_episode_count: 128\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 108.000000 | iteration_108.pth.tar | 5.000000      | 50.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 10.000000               | 0.028667      | 1744.167401         | 174.416740           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 0.800000   | 10.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [8.0, 9.0, 10.0, 10.0, 10.0] | 9.400000                 |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [8.0, 9.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 120.000000 | iteration_120.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.031049      | 1771.420059         | 161.038187           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 12.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 17.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 132.000000 | iteration_132.pth.tar | 5.000000      | 85.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 17.000000               | 0.039978      | 2126.155259         | 125.067956           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 12.200000   | 2.481935   | 17.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [11.0, 17.0, 10.0, 11.0, 12.0] | 12.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 17.0, 10.0, 11.0, 12.0], 'eval_episode_return_mean': 12.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 144.000000 | iteration_144.pth.tar | 5.000000      | 50.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 10.000000               | 0.028350      | 1763.646455         | 176.364646           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 0.800000   | 10.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [8.0, 9.0, 10.0, 10.0, 10.0] | 9.400000                 |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [8.0, 9.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 12.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 12.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 15.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 17.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 156.000000 | iteration_156.pth.tar | 5.000000      | 85.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 17.000000               | 0.039094      | 2174.248286         | 127.896958           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 13.800000   | 1.939072   | 17.000000  | 12.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [15.0, 17.0, 13.0, 12.0, 12.0] | 13.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [15.0, 17.0, 13.0, 12.0, 12.0], 'eval_episode_return_mean': 13.8}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 168.000000 | iteration_168.pth.tar | 5.000000      | 50.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 10.000000               | 0.028409      | 1760.020142         | 176.002014           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 0.800000   | 10.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [8.0, 9.0, 10.0, 10.0, 10.0] | 9.400000                 |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [8.0, 9.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 12.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 12.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 180.000000 | iteration_180.pth.tar | 5.000000      | 60.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 12.000000               | 0.035786      | 1676.626204         | 139.718850           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 11.000000   | 1.264911   | 12.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-------------------------------+--------------------------+\n","| Name  | eval_episode_return           | eval_episode_return_mean |\n","+-------+-------------------------------+--------------------------+\n","| Value | [12.0, 9.0, 12.0, 12.0, 10.0] | 11.000000                |\n","+-------+-------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [12.0, 9.0, 12.0, 12.0, 10.0], 'eval_episode_return_mean': 11.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 33.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 35.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 37.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 58.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 62.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 192.000000 | iteration_192.pth.tar | 5.000000      | 310.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 62.000000               | 0.096256      | 3220.577866         | 51.944804            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 45.000000   | 12.377399  | 62.000000  | 33.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [37.0, 62.0, 33.0, 35.0, 58.0] | 45.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [37.0, 62.0, 33.0, 35.0, 58.0], 'eval_episode_return_mean': 45.0}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 200 Times ===\n","INFO:buffer_logger:\n","+-------+----------+-----------+--------------+--------------+\n","| Name  | use_avg  | use_max   | priority_avg | priority_max |\n","+-------+----------+-----------+--------------+--------------+\n","| Value | 4.083333 | 22.000000 | 1.000000     | 1.000000     |\n","+-------+----------+-----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 100.171875    | 200.000000    | 0.401206 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 200 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 1.714545       | 1.255342    | 0.496940           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 204.000000 | iteration_204.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.032902      | 1671.642899         | 151.967536           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 114\n","envstep_count: 2379\n","train_sample_count: 2379\n","avg_envstep_per_episode: 20.86842105263158\n","avg_sample_per_episode: 20.86842105263158\n","avg_envstep_per_sec: 3806.2495397153652\n","avg_train_sample_per_sec: 3806.2495397153652\n","avg_episode_per_sec: 182.3927900494122\n","reward_mean: 20.86842105263158\n","reward_std: 12.59874204100144\n","reward_max: 81.0\n","reward_min: 8.0\n","total_envstep_count: 4848\n","total_train_sample_count: 4823\n","total_episode_count: 242\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 216.000000 | iteration_216.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.034965      | 1572.999850         | 142.999986           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 228.000000 | iteration_228.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.030581      | 1798.502487         | 163.500226           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 240.000000 | iteration_240.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033554      | 1639.168360         | 149.015305           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 27.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 33.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 252.000000 | iteration_252.pth.tar | 5.000000      | 165.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 33.000000               | 0.087021      | 1896.086401         | 57.457164            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 17.800000   | 10.166612  | 33.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [11.0, 33.0, 9.0, 9.0, 27.0] | 17.800000                |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 33.0, 9.0, 9.0, 27.0], 'eval_episode_return_mean': 17.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 26.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 30.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 33.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 33.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 37.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 264.000000 | iteration_264.pth.tar | 5.000000      | 185.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 37.000000               | 0.069904      | 2646.483242         | 71.526574            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 31.800000   | 3.655133   | 37.000000  | 26.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [33.0, 26.0, 37.0, 33.0, 30.0] | 31.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [33.0, 26.0, 37.0, 33.0, 30.0], 'eval_episode_return_mean': 31.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 20.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 20.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 20.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 71.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 77.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 77.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 276.000000 | iteration_276.pth.tar | 5.000000      | 385.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 77.000000               | 0.143960      | 2674.345146         | 34.731755            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 52.200000   | 28.038545  | 77.000000  | 16.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [71.0, 16.0, 77.0, 77.0, 20.0] | 52.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [71.0, 16.0, 77.0, 77.0, 20.0], 'eval_episode_return_mean': 52.2}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 18.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 18.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 45.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 48.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 49.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 288.000000 | iteration_288.pth.tar | 5.000000      | 245.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 49.000000               | 0.113013      | 2167.896205         | 44.242780            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 34.800000   | 15.458331  | 49.000000  | 14.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [45.0, 14.0, 49.0, 48.0, 18.0] | 34.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [45.0, 14.0, 49.0, 48.0, 18.0], 'eval_episode_return_mean': 34.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 12.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 300.000000 | iteration_300.pth.tar | 5.000000      | 80.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 16.000000               | 0.037984      | 2106.129376         | 131.633086           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 12.400000   | 1.854724   | 16.000000  | 11.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [12.0, 16.0, 11.0, 11.0, 12.0] | 12.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [12.0, 16.0, 11.0, 11.0, 12.0], 'eval_episode_return_mean': 12.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 300 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 3.076146       | 2.561294    | 1.202248           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 118\n","envstep_count: 2324\n","train_sample_count: 2324\n","avg_envstep_per_episode: 19.694915254237287\n","avg_sample_per_episode: 19.694915254237287\n","avg_envstep_per_sec: 3914.690415033841\n","avg_train_sample_per_sec: 3914.690415033841\n","avg_episode_per_sec: 198.76655291479915\n","reward_mean: 19.694915254237287\n","reward_std: 13.199581705349907\n","reward_max: 85.0\n","reward_min: 9.0\n","total_envstep_count: 7192\n","total_train_sample_count: 7187\n","total_episode_count: 360\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 312.000000 | iteration_312.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033381      | 1647.668133         | 149.788012           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 324.000000 | iteration_324.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.042350      | 1298.706961         | 118.064269           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 336.000000 | iteration_336.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.043182      | 1273.667845         | 115.787986           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 348.000000 | iteration_348.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.034456      | 1596.249049         | 145.113550           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 360.000000 | iteration_360.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.032700      | 1681.954008         | 152.904910           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 372.000000 | iteration_372.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.030369      | 1811.040525         | 164.640048           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 384.000000 | iteration_384.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.037480      | 1467.454104         | 133.404919           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 396.000000 | iteration_396.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.035361      | 1555.404584         | 141.400417           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 400 Times ===\n","INFO:buffer_logger:\n","+-------+----------+-----------+--------------+--------------+\n","| Name  | use_avg  | use_max   | priority_avg | priority_max |\n","+-------+----------+-----------+--------------+--------------+\n","| Value | 4.078125 | 21.000000 | 1.000000     | 1.000000     |\n","+-------+----------+-----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 200.210938    | 398.000000    | 0.402406 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 400 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 0.908084       | 1.964778    | 1.968600           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 408.000000 | iteration_408.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.035206      | 1562.219604         | 142.019964           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 173\n","envstep_count: 2480\n","train_sample_count: 2480\n","avg_envstep_per_episode: 14.335260115606937\n","avg_sample_per_episode: 14.335260115606937\n","avg_envstep_per_sec: 3714.0383998091897\n","avg_train_sample_per_sec: 3714.0383998091897\n","avg_episode_per_sec: 259.0841303092701\n","reward_mean: 14.335260115606937\n","reward_std: 5.731242293529175\n","reward_max: 50.0\n","reward_min: 8.0\n","total_envstep_count: 9632\n","total_train_sample_count: 9619\n","total_episode_count: 533\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 420.000000 | iteration_420.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.029514      | 1863.532757         | 169.412069           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 432.000000 | iteration_432.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.034402      | 1598.749203         | 145.340837           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 444.000000 | iteration_444.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.031349      | 1754.458421         | 159.496220           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 30.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 36.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 456.000000 | iteration_456.pth.tar | 5.000000      | 180.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 36.000000               | 0.131170      | 1372.264640         | 38.118462            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 19.000000   | 11.610340  | 36.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [11.0, 30.0, 9.0, 9.0, 36.0] | 19.000000                |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 30.0, 9.0, 9.0, 36.0], 'eval_episode_return_mean': 19.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 468.000000 | iteration_468.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.038748      | 1419.444619         | 129.040420           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 480.000000 | iteration_480.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.030268      | 1817.103337         | 165.191212           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 492.000000 | iteration_492.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033566      | 1638.574564         | 148.961324           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 500 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 1.734077       | 1.894996    | 1.885130           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 50.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 52.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 504.000000 | iteration_504.pth.tar | 5.000000      | 260.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 52.000000               | 0.137448      | 1891.626753         | 36.377438            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 26.200000   | 20.272148  | 52.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [11.0, 52.0, 9.0, 9.0, 50.0] | 26.200000                |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 52.0, 9.0, 9.0, 50.0], 'eval_episode_return_mean': 26.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:collector_logger:collect end:\n","episode_count: 182\n","envstep_count: 2393\n","train_sample_count: 2393\n","avg_envstep_per_episode: 13.148351648351648\n","avg_sample_per_episode: 13.148351648351648\n","avg_envstep_per_sec: 3661.457171728226\n","avg_train_sample_per_sec: 3661.457171728226\n","avg_episode_per_sec: 278.47271427268583\n","reward_mean: 13.148351648351648\n","reward_std: 4.489912078175923\n","reward_max: 38.0\n","reward_min: 8.0\n","total_envstep_count: 12080\n","total_train_sample_count: 12060\n","total_episode_count: 715\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 51.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 516.000000 | iteration_516.pth.tar | 5.000000      | 255.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 51.000000               | 0.134910      | 1890.154175         | 37.061847            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 18.800000   | 16.203703  | 51.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [11.0, 14.0, 9.0, 9.0, 51.0] | 18.800000                |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 14.0, 9.0, 9.0, 51.0], 'eval_episode_return_mean': 18.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 528.000000 | iteration_528.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033642      | 1634.858580         | 148.623507           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 12.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 540.000000 | iteration_540.pth.tar | 5.000000      | 65.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 13.000000               | 0.033684      | 1929.698686         | 148.438360           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 10.800000   | 1.469694   | 13.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-------------------------------+--------------------------+\n","| Name  | eval_episode_return           | eval_episode_return_mean |\n","+-------+-------------------------------+--------------------------+\n","| Value | [13.0, 12.0, 10.0, 10.0, 9.0] | 10.800000                |\n","+-------+-------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [13.0, 12.0, 10.0, 10.0, 9.0], 'eval_episode_return_mean': 10.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 31.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 33.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 35.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 36.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 51.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 552.000000 | iteration_552.pth.tar | 5.000000      | 255.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 51.000000               | 0.084173      | 3029.476302         | 59.401496            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 37.200000   | 7.110556   | 51.000000  | 31.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [35.0, 51.0, 31.0, 33.0, 36.0] | 37.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [35.0, 51.0, 31.0, 33.0, 36.0], 'eval_episode_return_mean': 37.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 564.000000 | iteration_564.pth.tar | 5.000000      | 70.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 14.000000               | 0.033388      | 2096.552985         | 149.753785           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 11.600000   | 1.959592   | 14.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 14.0, 10.0, 10.0, 10.0] | 11.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 14.0, 10.0, 10.0, 10.0], 'eval_episode_return_mean': 11.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 18.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 20.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 27.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 18.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 40.0000, current episode: 5\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 20.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 576.000000 | iteration_576.pth.tar | 5.000000      | 200.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 40.000000               | 0.086944      | 2300.334824         | 57.508371            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 24.800000   | 8.231646   | 40.000000  | 18.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [19.0, 40.0, 18.0, 20.0, 27.0] | 24.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [19.0, 40.0, 18.0, 20.0, 27.0], 'eval_episode_return_mean': 24.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 12.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 588.000000 | iteration_588.pth.tar | 5.000000      | 65.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 13.000000               | 0.041612      | 1562.034893         | 120.156530           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 10.800000   | 1.469694   | 13.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-------------------------------+--------------------------+\n","| Name  | eval_episode_return           | eval_episode_return_mean |\n","+-------+-------------------------------+--------------------------+\n","| Value | [13.0, 12.0, 10.0, 10.0, 9.0] | 10.800000                |\n","+-------+-------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [13.0, 12.0, 10.0, 10.0, 9.0], 'eval_episode_return_mean': 10.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+--------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have |\n","+-------+--------------+--------------+----------+--------------+\n","| Value | 12672.000000 | 38016.000000 | 0.000000 | 12672.000000 |\n","+-------+--------------+--------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 600.000000 | iteration_600.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.035439      | 1551.982777         | 141.089343           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 600 Times ===\n","INFO:buffer_logger:\n","+-------+----------+-----------+--------------+--------------+\n","| Name  | use_avg  | use_max   | priority_avg | priority_max |\n","+-------+----------+-----------+--------------+--------------+\n","| Value | 3.885417 | 20.000000 | 1.000000     | 1.000000     |\n","+-------+----------+-----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 300.242188    | 597.000000    | 0.403606 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 600 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 1.904683       | 3.344920    | 2.776056           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 27.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 32.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 34.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 35.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 35.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 612.000000 | iteration_612.pth.tar | 5.000000      | 175.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 35.000000               | 0.062849      | 2784.439075         | 79.555402            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 32.600000   | 3.006659   | 35.000000  | 27.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [27.0, 32.0, 35.0, 34.0, 35.0] | 32.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 121\n","envstep_count: 2241\n","train_sample_count: 2241\n","avg_envstep_per_episode: 18.520661157024794\n","avg_sample_per_episode: 18.520661157024794\n","avg_envstep_per_sec: 3612.9225337103044\n","avg_train_sample_per_sec: 3612.9225337103044\n","avg_episode_per_sec: 195.07524613072147\n","reward_mean: 18.520661157024794\n","reward_std: 12.316151576101849\n","reward_max: 63.0\n","reward_min: 9.0\n","total_envstep_count: 14464\n","total_train_sample_count: 14429\n","total_episode_count: 836\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [27.0, 32.0, 35.0, 34.0, 35.0], 'eval_episode_return_mean': 32.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 39.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 42.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 46.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 46.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 47.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 624.000000 | iteration_624.pth.tar | 5.000000      | 235.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 47.000000               | 0.074547      | 3152.371455         | 67.071733            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 44.000000   | 3.033150   | 47.000000  | 39.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [39.0, 42.0, 46.0, 46.0, 47.0] | 44.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [39.0, 42.0, 46.0, 46.0, 47.0], 'eval_episode_return_mean': 44.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 54.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 60.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 61.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 73.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 84.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 636.000000 | iteration_636.pth.tar | 5.000000      | 420.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 84.000000               | 0.126866      | 3310.576999         | 39.411631            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 66.400000   | 10.744301  | 84.000000  | 54.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [84.0, 73.0, 60.0, 61.0, 54.0] | 66.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [84.0, 73.0, 60.0, 61.0, 54.0], 'eval_episode_return_mean': 66.4}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 43.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 52.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 55.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 66.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 648.000000 | iteration_648.pth.tar | 5.000000      | 330.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 66.000000               | 0.124032      | 2660.593045         | 40.312016            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 46.400000   | 16.883128  | 66.000000  | 16.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [16.0, 66.0, 52.0, 55.0, 43.0] | 46.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [16.0, 66.0, 52.0, 55.0, 43.0], 'eval_episode_return_mean': 46.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 43.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 51.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 51.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 53.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 60.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 660.000000 | iteration_660.pth.tar | 5.000000      | 300.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 60.000000               | 0.099419      | 3017.535546         | 50.292259            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 51.600000   | 5.425864   | 60.000000  | 43.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [51.0, 60.0, 53.0, 51.0, 43.0] | 51.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [51.0, 60.0, 53.0, 51.0, 43.0], 'eval_episode_return_mean': 51.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 672.000000 | iteration_672.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.036508      | 1506.535357         | 136.957760           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.600000    | 1.200000   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 11.0, 9.0, 9.0, 8.0] | 9.600000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 11.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 684.000000 | iteration_684.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.031547      | 1743.426594         | 158.493327           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.600000    | 1.200000   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 11.0, 9.0, 9.0, 8.0] | 9.600000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 11.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 52.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 66.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 696.000000 | iteration_696.pth.tar | 5.000000      | 330.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 66.000000               | 0.186491      | 1769.520008         | 26.810909            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 29.800000   | 24.293209  | 66.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------+--------------------------+\n","| Name  | eval_episode_return          | eval_episode_return_mean |\n","+-------+------------------------------+--------------------------+\n","| Value | [13.0, 66.0, 9.0, 9.0, 52.0] | 29.800000                |\n","+-------+------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [13.0, 66.0, 9.0, 9.0, 52.0], 'eval_episode_return_mean': 29.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 700 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 1.557642       | 3.185915    | 2.881745           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 16.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 78.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 708.000000 | iteration_708.pth.tar | 5.000000      | 390.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 78.000000               | 0.206839      | 1885.528065         | 24.173437            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 25.400000   | 26.393939  | 78.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [13.0, 16.0, 10.0, 10.0, 78.0] | 25.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [13.0, 16.0, 10.0, 10.0, 78.0], 'eval_episode_return_mean': 25.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:collector_logger:collect end:\n","episode_count: 84\n","envstep_count: 2489\n","train_sample_count: 2489\n","avg_envstep_per_episode: 29.63095238095238\n","avg_sample_per_episode: 29.63095238095238\n","avg_envstep_per_sec: 4047.4005559852253\n","avg_train_sample_per_sec: 4047.4005559852253\n","avg_episode_per_sec: 136.59367083276774\n","reward_mean: 29.63095238095238\n","reward_std: 22.426482942900535\n","reward_max: 123.0\n","reward_min: 9.0\n","total_envstep_count: 16808\n","total_train_sample_count: 16790\n","total_episode_count: 920\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 17.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 30.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 720.000000 | iteration_720.pth.tar | 5.000000      | 150.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 30.000000               | 0.085885      | 1746.525385         | 58.217513            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 16.600000   | 7.059745   | 30.000000  | 11.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 17.0, 11.0, 11.0, 30.0] | 16.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 17.0, 11.0, 11.0, 30.0], 'eval_episode_return_mean': 16.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 24.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 24.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 25.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 37.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 42.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 732.000000 | iteration_732.pth.tar | 5.000000      | 210.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 42.000000               | 0.073990      | 2838.235584         | 67.577038            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 30.400000   | 7.605261   | 42.000000  | 24.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [24.0, 42.0, 24.0, 25.0, 37.0] | 30.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [24.0, 42.0, 24.0, 25.0, 37.0], 'eval_episode_return_mean': 30.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 22.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 22.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 26.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 40.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 41.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 744.000000 | iteration_744.pth.tar | 5.000000      | 205.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 41.000000               | 0.070253      | 2918.029200         | 71.171444            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 30.200000   | 8.541663   | 41.000000  | 22.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [22.0, 40.0, 26.0, 22.0, 41.0] | 30.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [22.0, 40.0, 26.0, 22.0, 41.0], 'eval_episode_return_mean': 30.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 20.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 23.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 26.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 20.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 23.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 26.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 20.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 23.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 26.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 20.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 23.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 92.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 756.000000 | iteration_756.pth.tar | 5.000000      | 460.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 92.000000               | 0.185756      | 2476.364094         | 26.917001            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 39.400000   | 26.844739  | 92.000000  | 20.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [26.0, 36.0, 20.0, 23.0, 92.0] | 39.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [26.0, 36.0, 20.0, 23.0, 92.0], 'eval_episode_return_mean': 39.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 50.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 53.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 57.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 77.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 90.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 768.000000 | iteration_768.pth.tar | 5.000000      | 450.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 90.000000               | 0.139378      | 3228.622893         | 35.873588            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 65.400000   | 15.499677  | 90.000000  | 50.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [77.0, 90.0, 53.0, 57.0, 50.0] | 65.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [77.0, 90.0, 53.0, 57.0, 50.0], 'eval_episode_return_mean': 65.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 15.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 20.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 15.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 16.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 20.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 41.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 44.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 780.000000 | iteration_780.pth.tar | 5.000000      | 220.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 44.000000               | 0.099344      | 2214.532277         | 50.330279            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 27.200000   | 12.639620  | 44.000000  | 15.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [20.0, 44.0, 15.0, 16.0, 41.0] | 27.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [20.0, 44.0, 15.0, 16.0, 41.0], 'eval_episode_return_mean': 27.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 24.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 43.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 24.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 57.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 64.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 69.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 792.000000 | iteration_792.pth.tar | 5.000000      | 345.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 69.000000               | 0.117643      | 2932.610122         | 42.501596            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 51.400000   | 16.255461  | 69.000000  | 24.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [24.0, 43.0, 64.0, 69.0, 57.0] | 51.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [24.0, 43.0, 64.0, 69.0, 57.0], 'eval_episode_return_mean': 51.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 800 Times ===\n","INFO:buffer_logger:\n","+-------+----------+-----------+--------------+--------------+\n","| Name  | use_avg  | use_max   | priority_avg | priority_max |\n","+-------+----------+-----------+--------------+--------------+\n","| Value | 4.023438 | 29.000000 | 1.000000     | 1.000000     |\n","+-------+----------+-----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 400.679688    | 799.000000    | 0.404806 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 800 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 4.082778       | 3.671216    | 3.264268           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 30.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 43.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 52.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 30.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 60.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 804.000000 | iteration_804.pth.tar | 5.000000      | 300.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 60.000000               | 0.111042      | 2701.685694         | 45.028095            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 40.800000   | 14.770240  | 60.000000  | 19.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [19.0, 30.0, 60.0, 52.0, 43.0] | 40.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [19.0, 30.0, 60.0, 52.0, 43.0], 'eval_episode_return_mean': 40.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 25.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 35.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 35.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 37.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 41.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 816.000000 | iteration_816.pth.tar | 5.000000      | 205.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 41.000000               | 0.077952      | 2629.826581         | 64.142112            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 34.600000   | 5.276362   | 41.000000  | 25.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [25.0, 35.0, 37.0, 35.0, 41.0] | 34.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 72\n","envstep_count: 2288\n","train_sample_count: 2288\n","avg_envstep_per_episode: 31.77777777777778\n","avg_sample_per_episode: 31.77777777777778\n","avg_envstep_per_sec: 3722.306794621476\n","avg_train_sample_per_sec: 3722.306794621476\n","avg_episode_per_sec: 117.13552850207442\n","reward_mean: 31.77777777777778\n","reward_std: 17.774218393678325\n","reward_max: 97.0\n","reward_min: 9.0\n","total_envstep_count: 19136\n","total_train_sample_count: 19102\n","total_episode_count: 992\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [25.0, 35.0, 37.0, 35.0, 41.0], 'eval_episode_return_mean': 34.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 30.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 39.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 39.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 42.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 48.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 828.000000 | iteration_828.pth.tar | 5.000000      | 240.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 48.000000               | 0.116393      | 2061.978090         | 42.957877            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 39.600000   | 5.817216   | 48.000000  | 30.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [30.0, 42.0, 39.0, 39.0, 48.0] | 39.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [30.0, 42.0, 39.0, 39.0, 48.0], 'eval_episode_return_mean': 39.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 64.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 68.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 87.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 114.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 118.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 840.000000 | iteration_840.pth.tar | 5.000000      | 590.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 118.000000              | 0.291154      | 2026.416332         | 17.173020            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 90.200000   | 22.489108  | 118.000000 | 64.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+----------------------------------+--------------------------+\n","| Name  | eval_episode_return              | eval_episode_return_mean |\n","+-------+----------------------------------+--------------------------+\n","| Value | [87.0, 64.0, 114.0, 118.0, 68.0] | 90.200000                |\n","+-------+----------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [87.0, 64.0, 114.0, 118.0, 68.0], 'eval_episode_return_mean': 90.2}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 43.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 52.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 58.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 59.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 61.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 852.000000 | iteration_852.pth.tar | 5.000000      | 305.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 61.000000               | 0.100319      | 3040.311431         | 49.841171            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 54.600000   | 6.529931   | 61.000000  | 43.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [61.0, 43.0, 59.0, 58.0, 52.0] | 54.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [61.0, 43.0, 59.0, 58.0, 52.0], 'eval_episode_return_mean': 54.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 12.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 16.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 864.000000 | iteration_864.pth.tar | 5.000000      | 80.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 16.000000               | 0.041618      | 1922.229148         | 120.139322           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 12.600000   | 2.154066   | 16.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [16.0, 14.0, 12.0, 11.0, 10.0] | 12.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [16.0, 14.0, 12.0, 11.0, 10.0], 'eval_episode_return_mean': 12.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 13.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 14.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 19.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 876.000000 | iteration_876.pth.tar | 5.000000      | 95.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 19.000000               | 0.040591      | 2340.433950         | 123.180734           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 15.800000   | 2.638181   | 19.000000  | 13.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [19.0, 19.0, 14.0, 14.0, 13.0] | 15.800000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [19.0, 19.0, 14.0, 14.0, 13.0], 'eval_episode_return_mean': 15.8}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 35.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 53.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 54.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 69.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 35.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 82.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 888.000000 | iteration_888.pth.tar | 5.000000      | 410.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 82.000000               | 0.149925      | 2734.696453         | 33.349957            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 58.600000   | 15.907231  | 82.000000  | 35.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [53.0, 82.0, 35.0, 69.0, 54.0] | 58.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [53.0, 82.0, 35.0, 69.0, 54.0], 'eval_episode_return_mean': 58.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 71.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 74.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 75.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 87.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 93.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 900.000000 | iteration_900.pth.tar | 5.000000      | 465.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 93.000000               | 0.141662      | 3282.469319         | 35.295369            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 80.000000   | 8.485281   | 93.000000  | 71.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [93.0, 71.0, 87.0, 75.0, 74.0] | 80.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [93.0, 71.0, 87.0, 75.0, 74.0], 'eval_episode_return_mean': 80.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 900 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 3.622623       | 5.207012    | 4.677261           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 11.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 12.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 912.000000 | iteration_912.pth.tar | 5.000000      | 60.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 12.000000               | 0.032598      | 1840.602665         | 153.383555           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 10.000000   | 1.264911   | 12.000000  | 9.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [12.0, 11.0, 9.0, 9.0, 9.0] | 10.000000                |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [12.0, 11.0, 9.0, 9.0, 9.0], 'eval_episode_return_mean': 10.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:collector_logger:collect end:\n","episode_count: 78\n","envstep_count: 2398\n","train_sample_count: 2398\n","avg_envstep_per_episode: 30.743589743589745\n","avg_sample_per_episode: 30.743589743589745\n","avg_envstep_per_sec: 3777.8530113394004\n","avg_train_sample_per_sec: 3777.8530113394004\n","avg_episode_per_sec: 122.88262505607725\n","reward_mean: 30.743589743589745\n","reward_std: 21.8411725391054\n","reward_max: 106.0\n","reward_min: 9.0\n","total_envstep_count: 21448\n","total_train_sample_count: 21428\n","total_episode_count: 1070\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 924.000000 | iteration_924.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.028205      | 1950.031023         | 177.275548           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 936.000000 | iteration_936.pth.tar | 5.000000      | 55.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.033284      | 1652.459993         | 150.223636           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 10.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 10.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 948.000000 | iteration_948.pth.tar | 5.000000      | 70.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 14.000000               | 0.044036      | 1589.611695         | 113.543692           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 11.600000   | 1.624808   | 14.000000  | 10.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 13.0, 11.0, 10.0, 10.0] | 11.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 13.0, 11.0, 10.0, 10.0], 'eval_episode_return_mean': 11.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 23.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 34.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 44.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 23.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 48.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 51.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 960.000000 | iteration_960.pth.tar | 5.000000      | 255.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 51.000000               | 0.129657      | 1966.723524         | 38.563206            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 40.000000   | 10.256705  | 51.000000  | 23.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [23.0, 34.0, 51.0, 48.0, 44.0] | 40.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [23.0, 34.0, 51.0, 48.0, 44.0], 'eval_episode_return_mean': 40.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 53.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 15.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 19.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 76.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 972.000000 | iteration_972.pth.tar | 5.000000      | 380.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 76.000000               | 0.174743      | 2174.619975         | 28.613421            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 35.400000   | 24.904618  | 76.000000  | 14.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [19.0, 76.0, 14.0, 15.0, 53.0] | 35.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [19.0, 76.0, 14.0, 15.0, 53.0], 'eval_episode_return_mean': 35.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 11.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 11.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 11.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 14.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 984.000000 | iteration_984.pth.tar | 5.000000      | 70.000000     |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 14.000000               | 0.036969      | 1893.456640         | 135.246903           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 12.200000   | 1.469694   | 14.000000  | 11.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 14.0, 11.0, 11.0, 11.0] | 12.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 14.0, 11.0, 11.0, 11.0], 'eval_episode_return_mean': 12.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 14.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 14.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 15.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 17.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 21.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+-----------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name             | episode_count | envstep_count |\n","+-------+------------+-----------------------+---------------+---------------+\n","| Value | 996.000000 | iteration_996.pth.tar | 5.000000      | 105.000000    |\n","+-------+------------+-----------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 21.000000               | 0.045491      | 2308.149872         | 109.911899           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 16.200000   | 2.638181   | 21.000000  | 14.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [17.0, 21.0, 14.0, 14.0, 15.0] | 16.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [17.0, 21.0, 14.0, 14.0, 15.0], 'eval_episode_return_mean': 16.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:buffer_logger:=== Sample data 1000 Times ===\n","INFO:buffer_logger:\n","+-------+----------+-----------+--------------+--------------+\n","| Name  | use_avg  | use_max   | priority_avg | priority_max |\n","+-------+----------+-----------+--------------+--------------+\n","| Value | 3.533854 | 14.000000 | 1.000000     | 1.000000     |\n","+-------+----------+-----------+--------------+--------------+\n","+-------+--------------+---------------+---------------+----------+\n","| Name  | priority_min | staleness_avg | staleness_max | beta     |\n","+-------+--------------+---------------+---------------+----------+\n","| Value | 1.000000     | 472.742188    | 938.000000    | 0.406006 |\n","+-------+--------------+---------------+---------------+----------+\n","\n","\n","INFO:learner_logger:[RANK0]: === Training Iteration 1000 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 2.558125       | 5.969637    | 5.574273           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 66.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 68.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 83.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 91.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 104.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1008.000000 | iteration_1008.pth.tar | 5.000000      | 520.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 104.000000              | 0.160435      | 3241.188802         | 31.165277            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 82.400000   | 14.263239  | 104.000000 | 66.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+---------------------------------+--------------------------+\n","| Name  | eval_episode_return             | eval_episode_return_mean |\n","+-------+---------------------------------+--------------------------+\n","| Value | [83.0, 91.0, 66.0, 68.0, 104.0] | 82.400000                |\n","+-------+---------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [83.0, 91.0, 66.0, 68.0, 104.0], 'eval_episode_return_mean': 82.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 87.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 115.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 128.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 140.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 142.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1020.000000 | iteration_1020.pth.tar | 5.000000      | 710.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 142.000000              | 0.206292      | 3441.720840         | 24.237471            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 122.400000  | 20.165317  | 142.000000 | 87.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------------+--------------------------+\n","| Name  | eval_episode_return                | eval_episode_return_mean |\n","+-------+------------------------------------+--------------------------+\n","| Value | [128.0, 115.0, 142.0, 140.0, 87.0] | 122.400000               |\n","+-------+------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n","INFO:collector_logger:collect end:\n","episode_count: 135\n","envstep_count: 2211\n","train_sample_count: 2211\n","avg_envstep_per_episode: 16.377777777777776\n","avg_sample_per_episode: 16.377777777777776\n","avg_envstep_per_sec: 3516.202682583398\n","avg_train_sample_per_sec: 3516.202682583398\n","avg_episode_per_sec: 214.69351521879636\n","reward_mean: 16.377777777777776\n","reward_std: 11.015051430869098\n","reward_max: 67.0\n","reward_min: 8.0\n","total_envstep_count: 23808\n","total_train_sample_count: 23799\n","total_episode_count: 1205\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [128.0, 115.0, 142.0, 140.0, 87.0], 'eval_episode_return_mean': 122.4}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 47.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 61.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 75.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 87.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 91.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1032.000000 | iteration_1032.pth.tar | 5.000000      | 455.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 91.000000               | 0.144387      | 3151.253082         | 34.629155            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 72.200000   | 16.375592  | 91.000000  | 47.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [61.0, 75.0, 91.0, 87.0, 47.0] | 72.200000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [61.0, 75.0, 91.0, 87.0, 47.0], 'eval_episode_return_mean': 72.2}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 8.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 9.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 9.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 10.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 11.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1044.000000 | iteration_1044.pth.tar | 5.000000      | 55.000000     |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 11.000000               | 0.037061      | 1484.031239         | 134.911931           |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 9.400000    | 1.019804   | 11.000000  | 8.000000   |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------+--------------------------+\n","| Name  | eval_episode_return         | eval_episode_return_mean |\n","+-------+-----------------------------+--------------------------+\n","| Value | [11.0, 10.0, 9.0, 9.0, 8.0] | 9.400000                 |\n","+-------+-----------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [11.0, 10.0, 9.0, 9.0, 8.0], 'eval_episode_return_mean': 9.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 24.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 24.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 25.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 14.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 31.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1056.000000 | iteration_1056.pth.tar | 5.000000      | 155.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 31.000000               | 0.063595      | 2437.296363         | 78.622463            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 23.600000   | 5.462600   | 31.000000  | 14.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [14.0, 24.0, 24.0, 25.0, 31.0] | 23.600000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [14.0, 24.0, 24.0, 25.0, 31.0], 'eval_episode_return_mean': 23.6}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 16.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 13.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 31.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 16.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 36.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 36.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1068.000000 | iteration_1068.pth.tar | 5.000000      | 180.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 36.000000               | 0.102153      | 1762.065817         | 48.946273            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 26.400000   | 9.931767   | 36.000000  | 13.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [31.0, 13.0, 36.0, 36.0, 16.0] | 26.400000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [31.0, 13.0, 36.0, 36.0, 16.0], 'eval_episode_return_mean': 26.4}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 34.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 38.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 39.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 39.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 34.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 38.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 39.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 39.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 85.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1080.000000 | iteration_1080.pth.tar | 5.000000      | 425.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 85.000000               | 0.212417      | 2000.780298         | 23.538592            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 47.000000   | 19.089264  | 85.000000  | 34.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------+--------------------------+\n","| Name  | eval_episode_return            | eval_episode_return_mean |\n","+-------+--------------------------------+--------------------------+\n","| Value | [85.0, 34.0, 39.0, 39.0, 38.0] | 47.000000                |\n","+-------+--------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [85.0, 34.0, 39.0, 39.0, 38.0], 'eval_episode_return_mean': 47.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 96.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 112.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 166.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 36.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 96.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 200.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1092.000000 | iteration_1092.pth.tar | 5.000000      | 1000.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 200.000000              | 0.323873      | 3087.627344         | 15.438137            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 122.000000  | 56.906942  | 200.000000 | 36.000000  |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------+--------------------------+\n","| Name  | eval_episode_return               | eval_episode_return_mean |\n","+-------+-----------------------------------+--------------------------+\n","| Value | [112.0, 36.0, 200.0, 166.0, 96.0] | 122.000000               |\n","+-------+-----------------------------------+--------------------------+\n","\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [112.0, 36.0, 200.0, 166.0, 96.0], 'eval_episode_return_mean': 122.0}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 1100 Result ===\n","INFO:learner_logger:\n","+-------+------------+----------------+-------------+--------------------+\n","| Name  | cur_lr_avg | total_loss_avg | q_value_avg | target_q_value_avg |\n","+-------+------------+----------------+-------------+--------------------+\n","| Value | 0.001000   | 6.945438       | 7.002361    | 5.840093           |\n","+-------+------------+----------------+-------------+--------------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 200.0000, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 200.0000, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 200.0000, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 200.0000, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 200.0000, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1104.000000 | iteration_1104.pth.tar | 5.000000      | 1000.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 200.000000              | 0.258134      | 3873.951458         | 19.369757            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 200.000000  | 0.000000   | 200.000000 | 200.000000 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-------------------------------------+--------------------------+\n","| Name  | eval_episode_return                 | eval_episode_return_mean |\n","+-------+-------------------------------------+--------------------------+\n","| Value | [200.0, 200.0, 200.0, 200.0, 200.0] | 200.000000               |\n","+-------+-------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/ckpt_best.pth.tar\n","INFO:evaluator_logger:[DI-engine serial pipeline] Current episode_return: 200.0000 is greater than stop_value: 195, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./cartpole_dqn_gail_seed0_240228_171321/ckpt/iteration_1104.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [200.0, 200.0, 200.0, 200.0, 200.0], 'eval_episode_return_mean': 200.0}\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_best.pth.tar\n","Saved reward model ckpt in cartpole_dqn_gail_seed0_240228_171321/reward_model/ckpt/ckpt_last.pth.tar\n"]},{"output_type":"execute_result","data":{"text/plain":["DI-engine DRL Policy\n","DQN(\n","  (encoder): FCEncoder(\n","    (act): ReLU()\n","    (init): Linear(in_features=4, out_features=128, bias=True)\n","    (main): Sequential(\n","      (0): Linear(in_features=128, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): Linear(in_features=128, out_features=64, bias=True)\n","      (3): ReLU()\n","    )\n","  )\n","  (head): DuelingHead(\n","    (A): Sequential(\n","      (0): Sequential(\n","        (0): Linear(in_features=64, out_features=64, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): Sequential(\n","        (0): Linear(in_features=64, out_features=2, bias=True)\n","      )\n","    )\n","    (V): Sequential(\n","      (0): Sequential(\n","        (0): Linear(in_features=64, out_features=64, bias=True)\n","        (1): ReLU()\n","      )\n","      (1): Sequential(\n","        (0): Linear(in_features=64, out_features=1, bias=True)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["## box2d bipedalwalker"],"metadata":{"id":"ux1OaCwN9zt8"}},{"cell_type":"code","source":["# dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py\n","from easydict import EasyDict\n","\n","obs_shape = 24\n","act_shape = 4\n","bipedalwalker_sac_gail_default_config = dict(\n","    exp_name='bipedalwalker_sac_gail_seed0',\n","    env=dict(\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        # (bool) Scale output action into legal range.\n","        act_scale=True,\n","        n_evaluator_episode=5,\n","        stop_value=300,\n","        rew_clip=True,\n","        # The path to save the game replay\n","        replay_path=None,\n","    ),\n","    reward_model=dict(\n","        type='gail',\n","        input_size=obs_shape + act_shape,\n","        hidden_size=64,\n","        batch_size=64,\n","        learning_rate=1e-3,\n","        update_per_collect=100,\n","        # Users should add their own model path here. Model path should lead to a model.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is ``exp_name/ckpt/ckpt_best.pth.tar``.\n","        expert_model_path='model_path_placeholder',\n","        # Path where to store the reward model\n","        reward_model_path='data_path_placeholder+/reward_model/ckpt/ckpt_best.pth.tar',\n","        # Users should add their own data path here. Data path should lead to a file to store data or load the stored data.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is usually located in ``exp_name`` directory\n","        data_path='data_path_placeholder',\n","        collect_count=100000,\n","    ),\n","    policy=dict(\n","        cuda=False,\n","        priority=False,\n","        random_collect_size=1000,\n","        model=dict(\n","            obs_shape=obs_shape,\n","            action_shape=act_shape,\n","            twin_critic=True,\n","            action_space='reparameterization',\n","            actor_head_hidden_size=128,\n","            critic_head_hidden_size=128,\n","        ),\n","        learn=dict(\n","            update_per_collect=1,\n","            batch_size=128,\n","            learning_rate_q=0.001,\n","            learning_rate_policy=0.001,\n","            learning_rate_alpha=0.0003,\n","            ignore_done=True,\n","            target_theta=0.005,\n","            discount_factor=0.99,\n","            auto_alpha=True,\n","            value_network=False,\n","        ),\n","        collect=dict(\n","            n_sample=128,\n","            unroll_len=1,\n","        ),\n","        other=dict(replay_buffer=dict(replay_buffer_size=100000, ), ),\n","    ),\n",")\n","bipedalwalker_sac_gail_default_config = EasyDict(bipedalwalker_sac_gail_default_config)\n","main_config = bipedalwalker_sac_gail_default_config\n","\n","bipedalwalker_sac_gail_create_config = dict(\n","    env=dict(\n","        type='bipedalwalker',\n","        import_names=['dizoo.box2d.bipedalwalker.envs.bipedalwalker_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(\n","        type='sac',\n","        import_names=['ding.policy.sac'],\n","    ),\n","    replay_buffer=dict(type='naive', ),\n",")\n","bipedalwalker_sac_gail_create_config = EasyDict(bipedalwalker_sac_gail_create_config)\n","create_config = bipedalwalker_sac_gail_create_config\n"],"metadata":{"id":"w0cc_zom5tkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ding.entry import serial_pipeline_gail\n","from dizoo.box2d.bipedalwalker.config import bipedalwalker_sac_config, bipedalwalker_sac_create_config\n","expert_main_config = bipedalwalker_sac_config\n","expert_create_config = bipedalwalker_sac_create_config\n","serial_pipeline_gail(\n","    [main_config, create_config], [expert_main_config, expert_create_config], seed=0, collect_data=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"GdJeXNR9926V","executionInfo":{"status":"error","timestamp":1709144901080,"user_tz":0,"elapsed":5489,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"4abac6e3-7dd7-48c2-a590-f6e9c76e918b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"DependencyNotInstalled","evalue":"box2D is not installed, run `pip install gym[box2d]`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fn, cfg)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py\u001b[0m in \u001b[0;36mobservation_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'BipedalWalkerEnv' object has no attribute '_observation_space'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mBox2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     from Box2D.b2 import (\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-2566c8a6d96d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpert_main_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbipedalwalker_sac_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexpert_create_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbipedalwalker_sac_create_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m serial_pipeline_gail(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mmain_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_config\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpert_main_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_create_config\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/entry/serial_entry_gail.py\u001b[0m in \u001b[0;36mserial_pipeline_gail\u001b[0;34m(input_cfg, expert_cfg, seed, model, max_train_iter, max_env_step, collect_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpert_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mexpert_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         collect_demo_data(\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mexpert_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_create_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/entry/application_entry.py\u001b[0m in \u001b[0;36mcollect_demo_data\u001b[0;34m(input_cfg, seed, collect_count, expert_data_path, env_setting, model, state_dict, state_dict_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollector_env_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mcollector_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollector_env_cfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mcollector_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mset_pkg_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36mcreate_env_manager\u001b[0;34m(manager_cfg, env_fn)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'import_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0mmanager_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mENV_MANAGER_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/utils/registry.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, obj_type, *obj_args, **obj_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0m_innest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/utils/registry.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, obj_type, *obj_args, **obj_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mbuild_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mobj_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mobj_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# get build_fn fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/subprocess_env_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fn, cfg)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0mstep_wait_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtime\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mminimum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shared_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_on_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_on_get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fn, cfg)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# when using in a subprocess mode, which would cause opengl rendering bugs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# leading to no response subprocesses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BipedalWalker-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Assume it's a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0menv_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"render_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[1;32m     61\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbipedal_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBipedalWalker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBipedalWalkerHardcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcar_racing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarRacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlunar_lander\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLunarLander\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLunarLanderContinuous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"box2D is not installed, run `pip install gym[box2d]`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"]}]},{"cell_type":"markdown","source":["## Another task other than cartpole (mujoco 版本有问题，后面看看能不能修一下)"],"metadata":{"id":"efe4TOVv5vwj"}},{"cell_type":"code","source":["# dizoo/mujoco/config/ant_gail_sac_config.py\n","from easydict import EasyDict\n","\n","obs_shape = 111\n","act_shape = 8\n","ant_sac_gail_config = dict(\n","    exp_name='ant_sac_gail_seed0',\n","    env=dict(\n","        env_id='Ant-v3',\n","        norm_obs=dict(use_norm=False, ),\n","        norm_reward=dict(use_norm=False, ),\n","        collector_env_num=1,\n","        evaluator_env_num=8,\n","        n_evaluator_episode=8,\n","        stop_value=6000,\n","    ),\n","    reward_model=dict(\n","        input_size=obs_shape + act_shape,\n","        hidden_size=256,\n","        batch_size=64,\n","        learning_rate=1e-3,\n","        update_per_collect=100,\n","        # Users should add their own model path here. Model path should lead to a model.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is ``exp_name/ckpt/ckpt_best.pth.tar``.\n","        expert_model_path='model_path_placeholder',\n","        # Path where to store the reward model\n","        reward_model_path='data_path_placeholder+/reward_model/ckpt/ckpt_best.pth.tar',\n","        # Users should add their own data path here. Data path should lead to a file to store data or load the stored data.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is usually located in ``exp_name`` directory\n","        data_path='data_path_placeholder',\n","        collect_count=300000,\n","    ),\n","    policy=dict(\n","        cuda=True,\n","        random_collect_size=25000,\n","        model=dict(\n","            obs_shape=obs_shape,\n","            action_shape=act_shape,\n","            twin_critic=True,\n","            action_space='reparameterization',\n","            actor_head_hidden_size=256,\n","            critic_head_hidden_size=256,\n","        ),\n","        learn=dict(\n","            update_per_collect=1,\n","            batch_size=256,\n","            learning_rate_q=1e-3,\n","            learning_rate_policy=1e-3,\n","            learning_rate_alpha=3e-4,\n","            ignore_done=False,\n","            target_theta=0.005,\n","            discount_factor=0.99,\n","            alpha=0.2,\n","            reparameterization=True,\n","            auto_alpha=False,\n","        ),\n","        collect=dict(\n","            n_sample=64,\n","            unroll_len=1,\n","        ),\n","        command=dict(),\n","        eval=dict(),\n","        other=dict(replay_buffer=dict(replay_buffer_size=1000000, ), ),\n","    ),\n",")\n","\n","ant_sac_gail_config = EasyDict(ant_sac_gail_config)\n","main_config = ant_sac_gail_config\n","\n","ant_sac_gail_create_config = dict(\n","    env=dict(\n","        type='mujoco',\n","        import_names=['dizoo.mujoco.envs.mujoco_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(\n","        type='sac',\n","        import_names=['ding.policy.sac'],\n","    ),\n","    replay_buffer=dict(type='naive', ),\n","    reward_model=dict(type='gail'),\n",")\n","ant_sac_gail_create_config = EasyDict(ant_sac_gail_create_config)\n","create_config = ant_sac_gail_create_config\n"],"metadata":{"id":"q-JKtSAB7Sbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install glfw\n","#!pip install mujoco\n","!pip3 install -U 'mujoco-py<2.2,>=2.1'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDDYJzDB8W-Q","executionInfo":{"status":"ok","timestamp":1709144531978,"user_tz":0,"elapsed":8554,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"44059e42-2913-4a61-9dd3-eb371aa8d8f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mujoco-py<2.2,>=2.1\n","  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (2.7.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (1.25.2)\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (3.0.8)\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (2.31.6)\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1) (1.16.0)\n","Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1)\n","  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (9.4.0)\n","Installing collected packages: fasteners, mujoco-py\n","Successfully installed fasteners-0.19 mujoco-py-2.1.2.14\n"]}]},{"cell_type":"code","source":["from ding.entry import serial_pipeline_gail\n","from dizoo.mujoco.config.ant_sac_config import ant_sac_config, ant_sac_create_config\n","\n","expert_main_config = ant_sac_config\n","expert_create_config = ant_sac_create_config\n","serial_pipeline_gail(\n","    [main_config, create_config], [expert_main_config, expert_create_config],\n","    max_env_step=10000000,\n","    seed=0,\n","    collect_data=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"fZk3n9k97_8n","executionInfo":{"status":"error","timestamp":1709144541464,"user_tz":0,"elapsed":6468,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"036ad68c-08eb-4285-855d-afac6cc5e57e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"DependencyNotInstalled","evalue":"No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fn, cfg)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/mujoco/envs/mujoco_env.py\u001b[0m in \u001b[0;36mobservation_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'MujocoEnv' object has no attribute '_observation_space'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-3fd8336c27e9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexpert_main_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mant_sac_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexpert_create_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mant_sac_create_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m serial_pipeline_gail(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mmain_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_config\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpert_main_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_create_config\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmax_env_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/entry/serial_entry_gail.py\u001b[0m in \u001b[0;36mserial_pipeline_gail\u001b[0;34m(input_cfg, expert_cfg, seed, model, max_train_iter, max_env_step, collect_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpert_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mexpert_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         collect_demo_data(\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mexpert_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_create_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/entry/application_entry.py\u001b[0m in \u001b[0;36mcollect_demo_data\u001b[0;34m(input_cfg, seed, collect_count, expert_data_path, env_setting, model, state_dict, state_dict_path)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollector_env_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mcollector_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollector_env_cfg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mcollector_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mset_pkg_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36mcreate_env_manager\u001b[0;34m(manager_cfg, env_fn)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'import_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0mmanager_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mENV_MANAGER_REGISTRY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/utils/registry.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, obj_type, *obj_args, **obj_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0m_innest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/utils/registry.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, obj_type, *obj_args, **obj_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mbuild_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mobj_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mobj_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# get build_fn fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/base_env_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fn, cfg)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# when using in a subprocess mode, which would cause opengl rendering bugs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# leading to no response subprocesses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/mujoco/envs/mujoco_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replay_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 self._env = gym.wrappers.RecordVideo(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/mujoco/envs/mujoco_env.py\u001b[0m in \u001b[0;36m_make_env\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         return wrap_mujoco(\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mnorm_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'norm_obs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dizoo/mujoco/envs/mujoco_wrappers.py\u001b[0m in \u001b[0;36mwrap_mujoco\u001b[0;34m(env_id, norm_obs, norm_reward, delay_reward_step)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# import customized gym environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmujoco_gym_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEpisodeReturnWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnorm_obs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnorm_obs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/ant_v3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, xml_file, ctrl_cost_weight, contact_cost_weight, healthy_reward, terminate_when_unhealthy, healthy_z_range, contact_force_range, reset_noise_scale, exclude_current_positions_from_observation, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         MuJocoPyEnv.__init__(\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    231\u001b[0m     ):\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mMUJOCO_PY_NOT_INSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             raise error.DependencyNotInstalled(\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;34mf\"{MUJOCO_PY_IMPORT_ERROR}. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             )\n","\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"]}]},{"cell_type":"markdown","source":["# file ops"],"metadata":{"id":"FV_LzOb77OQU"}},{"cell_type":"code","source":["!zip -r \"cartpole_dqn_seed0.zip\" '/content/cartpole_dqn_seed0'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EtG8v8KO2sBJ","executionInfo":{"status":"ok","timestamp":1709142986382,"user_tz":0,"elapsed":983,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"76c95600-f5f7-4901-a7c5-7872713f26f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["  adding: content/cartpole_dqn_seed0/ (stored 0%)\n","  adding: content/cartpole_dqn_seed0/formatted_total_config.py (deflated 70%)\n","  adding: content/cartpole_dqn_seed0/total_config.py (deflated 71%)\n","  adding: content/cartpole_dqn_seed0/ckpt/ (stored 0%)\n","  adding: content/cartpole_dqn_seed0/ckpt/eval.pth.tar (deflated 18%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_100.pth.tar (deflated 21%)\n","  adding: content/cartpole_dqn_seed0/ckpt/.ipynb_checkpoints/ (stored 0%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_400.pth.tar (deflated 19%)\n","  adding: content/cartpole_dqn_seed0/ckpt/ckpt_best.pth.tar (deflated 18%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_700.pth.tar (deflated 18%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_200.pth.tar (deflated 20%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_800.pth.tar (deflated 18%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_0.pth.tar (deflated 11%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_500.pth.tar (deflated 19%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_300.pth.tar (deflated 19%)\n","  adding: content/cartpole_dqn_seed0/ckpt/iteration_600.pth.tar (deflated 18%)\n"]}]},{"cell_type":"code","source":["!rm -rf /content/cartpole_dqn_gail_seed0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSo3Ks_61sle","executionInfo":{"status":"ok","timestamp":1709142868947,"user_tz":0,"elapsed":302,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"f9f80455-0b6c-448f-d3ec-7bb80384c7fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]}]}