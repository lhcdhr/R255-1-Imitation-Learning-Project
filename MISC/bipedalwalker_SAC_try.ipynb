{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMUUcGakkJDXPcrDUvQsC5M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["trained a SAC expert for GAIL"],"metadata":{"id":"WXR39hS36ngV"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BN-fwojOr-BQ","executionInfo":{"status":"ok","timestamp":1709156948858,"user_tz":0,"elapsed":42327,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"cb098464-da1f-477a-e311-4a4b291a6a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting DI-engine\n","  Downloading DI_engine-0.5.1-py3-none-any.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools<=66.1.1 (from DI-engine)\n","  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yapf==0.29.0 (from DI-engine)\n","  Downloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gym==0.25.1 (from DI-engine)\n","  Downloading gym-0.25.1.tar.gz (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting gymnasium (from DI-engine)\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.1.0+cu121)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.25.2)\n","Collecting DI-treetensor>=0.4.0 (from DI-engine)\n","  Downloading DI_treetensor-0.4.1-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting DI-toolkit>=0.1.0 (from DI-engine)\n","  Downloading DI_toolkit-0.2.1-py3-none-any.whl (29 kB)\n","Collecting trueskill (from DI-engine)\n","  Downloading trueskill-0.4.5.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorboardX>=2.2 (from DI-engine)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb (from DI-engine)\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from DI-engine) (3.7.1)\n","Collecting easydict==1.9 (from DI-engine)\n","  Downloading easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from DI-engine) (6.0.1)\n","Collecting enum-tools (from DI-engine)\n","  Downloading enum_tools-0.11.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.2.1)\n","Collecting hickle (from DI-engine)\n","  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from DI-engine) (0.9.0)\n","Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (8.1.7)\n","Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.31.0)\n","Collecting flask~=1.1.2 (from DI-engine)\n","  Downloading Flask-1.1.4-py2.py3-none-any.whl (94 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses~=0.12.1 (from DI-engine)\n","  Downloading responses-0.12.1-py2.py3-none-any.whl (16 kB)\n","Collecting URLObject>=2.4.0 (from DI-engine)\n","  Downloading URLObject-2.4.3.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting MarkupSafe==2.0.1 (from DI-engine)\n","  Downloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n","Collecting pynng (from DI-engine)\n","  Downloading pynng-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.4/936.4 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.3.0)\n","Collecting redis (from DI-engine)\n","  Downloading redis-5.0.2-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mpire>=2.3.5 (from DI-engine)\n","  Downloading mpire-2.10.0-py3-none-any.whl (272 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.1/272.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1->DI-engine) (0.0.8)\n","Collecting hbutils>=0.9.1 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading hbutils-0.9.3-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=12.2.0 in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (13.7.0)\n","Collecting yattag>=1.14.0 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading yattag-1.15.2.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.5.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (2.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (4.66.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (0.13.1)\n","Collecting treevalue>=1.4.11 (from DI-treetensor>=0.4.0->DI-engine)\n","  Downloading treevalue-1.4.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Werkzeug<2.0,>=0.15 (from flask~=1.1.2->DI-engine)\n","  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Jinja2<3.0,>=2.10.1 (from flask~=1.1.2->DI-engine)\n","  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itsdangerous<2.0,>=0.24 (from flask~=1.1.2->DI-engine)\n","  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting click>=7.0.0 (from DI-engine)\n","  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire>=2.3.5->DI-engine) (2.16.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2024.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from responses~=0.12.1->DI-engine) (1.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (23.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.2.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2.1.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium->DI-engine)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle->DI-engine) (3.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (2.8.2)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from pynng->DI-engine) (1.16.0)\n","Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis->DI-engine) (4.0.3)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->DI-engine)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb->DI-engine)\n","  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->DI-engine)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb->DI-engine)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (1.4.4)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytimeparse>=1.1.8 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n","Collecting bitmath>=1.3.3.1 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading bitmath-1.3.3.1.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting chardet<5,>=3.0.4 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deprecation>=2.0.0 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (3.0.0)\n","Requirement already satisfied: graphviz>=0.17 in /usr/local/lib/python3.10/dist-packages (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine) (0.20.1)\n","Collecting dill>=0.3.4 (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->pynng->DI-engine) (2.21)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->DI-toolkit>=0.1.0->DI-engine) (2023.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (3.3.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->DI-engine) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.60.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.7.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (0.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.2.2)\n","Building wheels for collected packages: easydict, gym, URLObject, trueskill, yattag, bitmath\n","  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6344 sha256=38d5feae3fc676dbba1dcd61c8fc44cc97d8fea8c84c730588c7c87b2bf80284\n","  Stored in directory: /root/.cache/pip/wheels/fd/d2/35/4c11d19a72280492846f4c4df975311a2bac475e8021f86c1d\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.25.1-py3-none-any.whl size=849010 sha256=bcbfb024cca24f359f68abc23362dc85fd179f0b3eb49aea36a72baab8438f25\n","  Stored in directory: /root/.cache/pip/wheels/2e/3b/df/78994c45c86a980cd5d8404c6d38cd28b871d5120e45c32ce4\n","  Building wheel for URLObject (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for URLObject: filename=URLObject-2.4.3-py3-none-any.whl size=14511 sha256=72c6b8d6b5abf15f7f065470f5bcb3dfd070d2abb9d4f4604dc18207577c5b1e\n","  Stored in directory: /root/.cache/pip/wheels/0d/a2/8a/05c4a3cbe66487af088bc8967fad6de3cc30f4680a5e2e27b8\n","  Building wheel for trueskill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for trueskill: filename=trueskill-0.4.5-py3-none-any.whl size=18049 sha256=0f737c739aff35438668707d908a1b35055fb31b39dca4d34f8fb60a787b6bca\n","  Stored in directory: /root/.cache/pip/wheels/b9/4f/29/c79f0a2956775524c7a23638ac2b6fbb516c680f8e5eed9b53\n","  Building wheel for yattag (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yattag: filename=yattag-1.15.2-py3-none-any.whl size=15668 sha256=9bf81732a85a09e0f4d65a3a5d9a133f12058d3bf05a4e0f45d58734116a7422\n","  Stored in directory: /root/.cache/pip/wheels/3f/6e/e5/d526243c27041915f63eacc0804babeb86b6973b0bc1991f06\n","  Building wheel for bitmath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bitmath: filename=bitmath-1.3.3.1-py3-none-any.whl size=23976 sha256=70ecab3dcca68ca746419ddb1b4aea9895160ddeabf88adb37bff0c75621aafa\n","  Stored in directory: /root/.cache/pip/wheels/2d/32/d2/936069b5a9583c55bc7c7dce6c746a543ce61d7dfbb4013e13\n","Successfully built easydict gym URLObject trueskill yattag bitmath\n","Installing collected packages: yattag, yapf, URLObject, pytimeparse, farama-notifications, easydict, bitmath, Werkzeug, trueskill, tensorboardX, smmap, setuptools, setproctitle, sentry-sdk, redis, mpire, MarkupSafe, itsdangerous, gymnasium, gym, enum-tools, docker-pycreds, dill, deprecation, click, chardet, responses, pynng, Jinja2, hickle, hbutils, gitdb, treevalue, GitPython, flask, wandb, DI-treetensor, DI-toolkit, DI-engine\n","  Attempting uninstall: easydict\n","    Found existing installation: easydict 1.12\n","    Uninstalling easydict-1.12:\n","      Successfully uninstalled easydict-1.12\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.0.1\n","    Uninstalling Werkzeug-3.0.1:\n","      Successfully uninstalled Werkzeug-3.0.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: itsdangerous\n","    Found existing installation: itsdangerous 2.1.2\n","    Uninstalling itsdangerous-2.1.2:\n","      Successfully uninstalled itsdangerous-2.1.2\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 3.1.3\n","    Uninstalling Jinja2-3.1.3:\n","      Successfully uninstalled Jinja2-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 2.2.5\n","    Uninstalling Flask-2.2.5:\n","      Successfully uninstalled Flask-2.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","branca 0.7.1 requires jinja2>=3, but you have jinja2 2.11.3 which is incompatible.\n","dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n","nbconvert 6.5.4 requires jinja2>=3.0, but you have jinja2 2.11.3 which is incompatible.\n","pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed DI-engine-0.5.1 DI-toolkit-0.2.1 DI-treetensor-0.4.1 GitPython-3.1.42 Jinja2-2.11.3 MarkupSafe-2.0.1 URLObject-2.4.3 Werkzeug-1.0.1 bitmath-1.3.3.1 chardet-4.0.0 click-7.1.2 deprecation-2.1.0 dill-0.3.8 docker-pycreds-0.4.0 easydict-1.9 enum-tools-0.11.0 farama-notifications-0.0.4 flask-1.1.4 gitdb-4.0.11 gym-0.25.1 gymnasium-0.29.1 hbutils-0.9.3 hickle-5.0.2 itsdangerous-1.1.0 mpire-2.10.0 pynng-0.8.0 pytimeparse-1.1.8 redis-5.0.2 responses-0.12.1 sentry-sdk-1.40.6 setproctitle-1.3.3 setuptools-66.1.1 smmap-5.0.1 tensorboardX-2.6.2.2 treevalue-1.4.12 trueskill-0.4.5 wandb-0.16.3 yapf-0.29.0 yattag-1.15.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","pkg_resources","setuptools"]},"id":"a5d6eadc33644ee8b579259cbd4185d9"}},"metadata":{}}],"source":["!pip install DI-engine"]},{"cell_type":"code","source":["!apt-get install swig3.0\n","!ln -s /usr/bin/swig3.0 /usr/bin/swig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zD-lpgdTsEys","executionInfo":{"status":"ok","timestamp":1709156955174,"user_tz":0,"elapsed":6336,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"cc79045d-d132-47af-e448-7486764d8353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","Suggested packages:\n","  swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig3.0\n","0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n","Need to get 1,109 kB of archives.\n","After this operation, 5,555 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n","Fetched 1,109 kB in 0s (2,761 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 121749 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n","Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["!pip install gym\n","!pip install gym[box2d]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"smpLj1tMsGni","executionInfo":{"status":"ok","timestamp":1709157020279,"user_tz":0,"elapsed":65108,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"b695faf0-e12d-4562-a9e0-5c7388adbf0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n","Collecting box2d-py==2.3.5 (from gym[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pygame==2.1.0 (from gym[box2d])\n","  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2309696 sha256=790662f73ea2be95493bf96fd2d78f808a038d98be9da261876bfdbfe6e49c17\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"]}]},{"cell_type":"code","source":["# dizoo.box2d.bipedalwalker.config bipedalwalker_sac_config\n","from easydict import EasyDict\n","\n","bipedalwalker_sac_config = dict(\n","    exp_name='bipedalwalker_sac_config0',\n","    env=dict(\n","        env_id='BipedalWalker-v3',\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        # (bool) Scale output action into legal range.\n","        act_scale=True,\n","        n_evaluator_episode=5,\n","        rew_clip=True,\n","    ),\n","    policy=dict(\n","        cuda=True,\n","        random_collect_size=10000,\n","        model=dict(\n","            obs_shape=24,\n","            action_shape=4,\n","            twin_critic=True,\n","            action_space='reparameterization',\n","            actor_head_hidden_size=128,\n","            critic_head_hidden_size=128,\n","        ),\n","        learn=dict(\n","            update_per_collect=64,\n","            batch_size=256,\n","            learning_rate_q=0.0003,\n","            learning_rate_policy=0.0003,\n","            learning_rate_alpha=0.0003,\n","            target_theta=0.005,\n","            discount_factor=0.99,\n","            auto_alpha=True,\n","            learner=dict(hook=dict(log_show_after_iter=1000, ))\n","        ),\n","        collect=dict(n_sample=64, ),\n","        other=dict(replay_buffer=dict(replay_buffer_size=300000, ), ),\n","    ),\n",")\n","bipedalwalker_sac_config = EasyDict(bipedalwalker_sac_config)\n","#expert_main_config = bipedalwalker_sac_config\n","bipedalwalker_sac_create_config = dict(\n","    env=dict(\n","        type='bipedalwalker',\n","        import_names=['dizoo.box2d.bipedalwalker.envs.bipedalwalker_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(type='sac', ),\n","    replay_buffer=dict(type='naive', ),\n",")\n","bipedalwalker_sac_create_config = EasyDict(bipedalwalker_sac_create_config)\n","#expert_create_config = bipedalwalker_sac_create_config"],"metadata":{"id":"4fJrOXzSsinf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from dizoo.box2d.bipedalwalker.config import bipedalwalker_ppo_config , bipedalwalker_ppo_create_config\n","from ding.config import compile_config\n","cfg = compile_config(bipedalwalker_sac_config, create_cfg=bipedalwalker_sac_create_config, auto=True)"],"metadata":{"id":"ORvmuNessm_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","from ding.envs import DingEnvWrapper, BaseEnvManagerV2\n","\n","collector_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"BipedalWalker-v3\")) for _ in range(cfg.env.collector_env_num)],\n","    cfg=cfg.env.manager\n",")\n","evaluator_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"BipedalWalker-v3\")) for _ in range(cfg.env.evaluator_env_num)],\n","    cfg=cfg.env.manager\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTMJVV6Vs12r","executionInfo":{"status":"ok","timestamp":1709157320679,"user_tz":0,"elapsed":389,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"fd9fe58f-dba4-45b1-8ad6-40146e845f98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["# from ding.model import DQN\n","from ding.policy import SACPolicy\n","from ding.data import DequeBuffer\n","\n","#model = SAC(**cfg.policy.model)\n","buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\n","policy = SACPolicy(cfg.policy)"],"metadata":{"id":"PZ1QfeDftZJk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ding.framework import task\n","from ding.framework.context import OnlineRLContext\n","from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, eps_greedy_handler, CkptSaver\n","\n","import logging\n","logging.getLogger().setLevel(logging.INFO)\n","\n","with task.start(async_mode=False, ctx=OnlineRLContext()):\n","    # Evaluating, we place it on the first place to get the score of the random model as a benchmark value\n","    task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\n","    #task.use(eps_greedy_handler(cfg))  # Decay probability of explore-exploit\n","    task.use(StepCollector(cfg, policy.collect_mode, collector_env))  # Collect environmental data\n","    task.use(data_pusher(cfg, buffer_))  # Push data to buffer\n","    task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))  # Train the model\n","    task.use(CkptSaver(policy, cfg.exp_name, train_freq=100))  # Save the model\n","    # In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\n","    task.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jNI-_ErFtOsN","executionInfo":{"status":"error","timestamp":1709173422289,"user_tz":0,"elapsed":16094484,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"91525e67-b0d0-47e4-cf70-ac9c4be736ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","avg_envstep_per_episode: 68540.0\n","avg_sample_per_episode: 1644960.0\n","avg_envstep_per_sec: 8750.293974719254\n","avg_train_sample_per_sec: 210007.0553932621\n","avg_episode_per_sec: 0.12766696782490886\n","reward_mean: 297.01214599609375\n","reward_std: 0.0\n","reward_max: 297.01214599609375\n","reward_min: 297.01214599609375\n","total_envstep_count: 548320\n","total_train_sample_count: 13159680\n","total_episode_count: 581\n","INFO:root:Training: Train Iter(548100)\tEnv Step(548352)\tLoss(-22.822)\n","INFO:root:Training: Train Iter(548200)\tEnv Step(548416)\tLoss(-23.125)\n","INFO:root:Training: Train Iter(548300)\tEnv Step(548544)\tLoss(-23.995)\n","INFO:root:Training: Train Iter(548400)\tEnv Step(548608)\tLoss(-24.223)\n","INFO:root:Training: Train Iter(548500)\tEnv Step(548736)\tLoss(-11.951)\n","INFO:root:Training: Train Iter(548600)\tEnv Step(548800)\tLoss(-22.808)\n","INFO:root:Training: Train Iter(548700)\tEnv Step(548928)\tLoss(-22.066)\n","INFO:root:Training: Train Iter(548800)\tEnv Step(549056)\tLoss(-18.769)\n","INFO:root:Evaluation: Train Iter(548864) Env Step(549056) Episode Return(304.472) \n","INFO:root:Training: Train Iter(548900)\tEnv Step(549120)\tLoss(-24.798)\n","INFO:root:Training: Train Iter(549000)\tEnv Step(549248)\tLoss(-23.572)\n","INFO:root:Training: Train Iter(549100)\tEnv Step(549312)\tLoss(-23.772)\n","INFO:root:Training: Train Iter(549200)\tEnv Step(549440)\tLoss(-19.230)\n","INFO:root:Training: Train Iter(549300)\tEnv Step(549504)\tLoss(-21.736)\n","INFO:root:Training: Train Iter(549400)\tEnv Step(549632)\tLoss(-20.935)\n","INFO:root:Training: Train Iter(549500)\tEnv Step(549696)\tLoss(-26.165)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 68717\n","train_sample_count: 1649208\n","avg_envstep_per_episode: 68717.0\n","avg_sample_per_episode: 1649208.0\n","avg_envstep_per_sec: 8812.302221520324\n","avg_train_sample_per_sec: 211495.2533164878\n","avg_episode_per_sec: 0.12824049684241634\n","reward_mean: 299.9095153808594\n","reward_std: 0.0\n","reward_max: 299.9095153808594\n","reward_min: 299.9095153808594\n","total_envstep_count: 549736\n","total_train_sample_count: 13193664\n","total_episode_count: 582\n","INFO:root:Training: Train Iter(549600)\tEnv Step(549824)\tLoss(-22.813)\n","INFO:root:Training: Train Iter(549700)\tEnv Step(549952)\tLoss(-23.249)\n","INFO:root:Training: Train Iter(549800)\tEnv Step(550016)\tLoss(-26.328)\n","INFO:root:Evaluation: Train Iter(549888) Env Step(550080) Episode Return(308.592) \n","INFO:root:Training: Train Iter(549900)\tEnv Step(550144)\tLoss(-25.161)\n","INFO:root:Training: Train Iter(550000)\tEnv Step(550208)\tLoss(-25.674)\n","INFO:root:Training: Train Iter(550100)\tEnv Step(550336)\tLoss(-22.947)\n","INFO:root:Training: Train Iter(550200)\tEnv Step(550400)\tLoss(-22.330)\n","INFO:root:Training: Train Iter(550300)\tEnv Step(550528)\tLoss(-23.978)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 68829\n","train_sample_count: 1651896\n","avg_envstep_per_episode: 68829.0\n","avg_sample_per_episode: 1651896.0\n","avg_envstep_per_sec: 8706.91710879843\n","avg_train_sample_per_sec: 208966.01061116232\n","avg_episode_per_sec: 0.1265007062255507\n","reward_mean: 299.3061218261719\n","reward_std: 0.0\n","reward_max: 299.3061218261719\n","reward_min: 299.3061218261719\n","total_envstep_count: 550632\n","total_train_sample_count: 13215168\n","total_episode_count: 583\n","INFO:root:Training: Train Iter(550400)\tEnv Step(550656)\tLoss(-23.882)\n","INFO:root:Training: Train Iter(550500)\tEnv Step(550720)\tLoss(-24.350)\n","INFO:root:Training: Train Iter(550600)\tEnv Step(550848)\tLoss(-19.095)\n","INFO:root:Training: Train Iter(550700)\tEnv Step(550912)\tLoss(-24.887)\n","INFO:root:Training: Train Iter(550800)\tEnv Step(551040)\tLoss(-21.846)\n","INFO:root:Training: Train Iter(550900)\tEnv Step(551104)\tLoss(-24.528)\n","INFO:root:Evaluation: Train Iter(550912) Env Step(551104) Episode Return(309.781) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 68892\n","train_sample_count: 1653408\n","avg_envstep_per_episode: 68892.0\n","avg_sample_per_episode: 1653408.0\n","avg_envstep_per_sec: 8016.910514521445\n","avg_train_sample_per_sec: 192405.8523485147\n","avg_episode_per_sec: 0.11636925208328174\n","reward_mean: 299.50286865234375\n","reward_std: 0.0\n","reward_max: 299.50286865234375\n","reward_min: 299.50286865234375\n","total_envstep_count: 551136\n","total_train_sample_count: 13227264\n","total_episode_count: 584\n","INFO:root:Training: Train Iter(551000)\tEnv Step(551232)\tLoss(-21.608)\n","INFO:root:Training: Train Iter(551100)\tEnv Step(551296)\tLoss(-26.107)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 68918\n","train_sample_count: 1654032\n","avg_envstep_per_episode: 68918.0\n","avg_sample_per_episode: 1654032.0\n","avg_envstep_per_sec: 8533.206440649588\n","avg_train_sample_per_sec: 204796.95457559012\n","avg_episode_per_sec: 0.12381680316680095\n","reward_mean: 298.23052978515625\n","reward_std: 0.0\n","reward_max: 298.23052978515625\n","reward_min: 298.23052978515625\n","total_envstep_count: 551344\n","total_train_sample_count: 13232256\n","total_episode_count: 585\n","INFO:root:Training: Train Iter(551200)\tEnv Step(551424)\tLoss(-16.690)\n","INFO:root:Training: Train Iter(551300)\tEnv Step(551552)\tLoss(-21.794)\n","INFO:root:Training: Train Iter(551400)\tEnv Step(551616)\tLoss(-21.006)\n","INFO:root:Training: Train Iter(551500)\tEnv Step(551744)\tLoss(-24.427)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 68969\n","train_sample_count: 1655256\n","avg_envstep_per_episode: 68969.0\n","avg_sample_per_episode: 1655256.0\n","avg_envstep_per_sec: 8883.842211657684\n","avg_train_sample_per_sec: 213212.21307978441\n","avg_episode_per_sec: 0.1288092072040726\n","reward_mean: 300.1327209472656\n","reward_std: 0.0\n","reward_max: 300.1327209472656\n","reward_min: 300.1327209472656\n","total_envstep_count: 551752\n","total_train_sample_count: 13242048\n","total_episode_count: 586\n","INFO:root:Training: Train Iter(551600)\tEnv Step(551808)\tLoss(-25.259)\n","INFO:root:Training: Train Iter(551700)\tEnv Step(551936)\tLoss(-20.954)\n","INFO:root:Training: Train Iter(551800)\tEnv Step(552000)\tLoss(-22.790)\n","INFO:root:Training: Train Iter(551900)\tEnv Step(552128)\tLoss(-18.891)\n","INFO:root:Evaluation: Train Iter(551936) Env Step(552128) Episode Return(311.432) \n","INFO:root:Training: Train Iter(552000)\tEnv Step(552256)\tLoss(-22.511)\n","INFO:root:Training: Train Iter(552100)\tEnv Step(552320)\tLoss(-11.600)\n","INFO:root:Training: Train Iter(552200)\tEnv Step(552448)\tLoss(-25.940)\n","INFO:root:Training: Train Iter(552300)\tEnv Step(552512)\tLoss(-22.088)\n","INFO:root:Training: Train Iter(552400)\tEnv Step(552640)\tLoss(-22.038)\n","INFO:root:Training: Train Iter(552500)\tEnv Step(552704)\tLoss(-22.673)\n","INFO:root:Training: Train Iter(552600)\tEnv Step(552832)\tLoss(-25.804)\n","INFO:root:Training: Train Iter(552700)\tEnv Step(552896)\tLoss(-27.196)\n","INFO:root:Training: Train Iter(552800)\tEnv Step(553024)\tLoss(-23.684)\n","INFO:root:Training: Train Iter(552900)\tEnv Step(553152)\tLoss(-20.327)\n","INFO:root:Evaluation: Train Iter(552960) Env Step(553152) Episode Return(307.753) \n","INFO:root:Training: Train Iter(553000)\tEnv Step(553216)\tLoss(-24.504)\n","INFO:root:Training: Train Iter(553100)\tEnv Step(553344)\tLoss(-15.838)\n","INFO:root:Training: Train Iter(553200)\tEnv Step(553408)\tLoss(-19.338)\n","INFO:root:Training: Train Iter(553300)\tEnv Step(553536)\tLoss(-23.218)\n","INFO:root:Training: Train Iter(553400)\tEnv Step(553600)\tLoss(-25.409)\n","INFO:root:Training: Train Iter(553500)\tEnv Step(553728)\tLoss(-23.252)\n","INFO:root:Training: Train Iter(553600)\tEnv Step(553856)\tLoss(-26.512)\n","INFO:root:Training: Train Iter(553700)\tEnv Step(553920)\tLoss(-26.731)\n","INFO:root:Training: Train Iter(553800)\tEnv Step(554048)\tLoss(-17.019)\n","INFO:root:Training: Train Iter(553900)\tEnv Step(554112)\tLoss(-24.934)\n","INFO:root:Evaluation: Train Iter(553984) Env Step(554176) Episode Return(307.527) \n","INFO:root:Training: Train Iter(554000)\tEnv Step(554240)\tLoss(-24.034)\n","INFO:root:Training: Train Iter(554100)\tEnv Step(554304)\tLoss(-16.661)\n","INFO:root:Training: Train Iter(554200)\tEnv Step(554432)\tLoss(-19.574)\n","INFO:root:Training: Train Iter(554300)\tEnv Step(554496)\tLoss(-12.733)\n","INFO:root:Training: Train Iter(554400)\tEnv Step(554624)\tLoss(-25.509)\n","INFO:root:Training: Train Iter(554500)\tEnv Step(554752)\tLoss(-24.267)\n","INFO:root:Training: Train Iter(554600)\tEnv Step(554816)\tLoss(-23.520)\n","INFO:root:Training: Train Iter(554700)\tEnv Step(554944)\tLoss(-22.516)\n","INFO:root:Training: Train Iter(554800)\tEnv Step(555008)\tLoss(-23.428)\n","INFO:root:Training: Train Iter(554900)\tEnv Step(555136)\tLoss(-22.246)\n","INFO:root:Training: Train Iter(555000)\tEnv Step(555200)\tLoss(-21.107)\n","INFO:root:Evaluation: Train Iter(555008) Env Step(555200) Episode Return(307.560) \n","INFO:root:Training: Train Iter(555100)\tEnv Step(555328)\tLoss(-23.428)\n","INFO:root:Training: Train Iter(555200)\tEnv Step(555456)\tLoss(-23.214)\n","INFO:root:Training: Train Iter(555300)\tEnv Step(555520)\tLoss(-23.098)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69448\n","train_sample_count: 1666752\n","avg_envstep_per_episode: 69448.0\n","avg_sample_per_episode: 1666752.0\n","avg_envstep_per_sec: 8739.0844756709\n","avg_train_sample_per_sec: 209738.0274161016\n","avg_episode_per_sec: 0.12583637362733124\n","reward_mean: 302.47540283203125\n","reward_std: 0.0\n","reward_max: 302.47540283203125\n","reward_min: 302.47540283203125\n","total_envstep_count: 555584\n","total_train_sample_count: 13334016\n","total_episode_count: 587\n","INFO:root:Training: Train Iter(555400)\tEnv Step(555648)\tLoss(-23.800)\n","INFO:root:Training: Train Iter(555500)\tEnv Step(555712)\tLoss(-22.401)\n","INFO:root:Training: Train Iter(555600)\tEnv Step(555840)\tLoss(-17.059)\n","INFO:root:Training: Train Iter(555700)\tEnv Step(555904)\tLoss(-23.521)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69500\n","train_sample_count: 1668000\n","avg_envstep_per_episode: 69500.0\n","avg_sample_per_episode: 1668000.0\n","avg_envstep_per_sec: 6122.19649123774\n","avg_train_sample_per_sec: 146932.71578970575\n","avg_episode_per_sec: 0.0880891581473056\n","reward_mean: 305.4514465332031\n","reward_std: 0.0\n","reward_max: 305.4514465332031\n","reward_min: 305.4514465332031\n","total_envstep_count: 556000\n","total_train_sample_count: 13344000\n","total_episode_count: 588\n","INFO:root:Training: Train Iter(555800)\tEnv Step(556032)\tLoss(-19.097)\n","INFO:root:Training: Train Iter(555900)\tEnv Step(556096)\tLoss(-22.411)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69503\n","train_sample_count: 1668072\n","avg_envstep_per_episode: 69503.0\n","avg_sample_per_episode: 1668072.0\n","avg_envstep_per_sec: 8744.796041768548\n","avg_train_sample_per_sec: 209875.10500244514\n","avg_episode_per_sec: 0.1258189724439024\n","reward_mean: 305.05908203125\n","reward_std: 0.0\n","reward_max: 305.05908203125\n","reward_min: 305.05908203125\n","total_envstep_count: 556104\n","total_train_sample_count: 13346496\n","total_episode_count: 589\n","INFO:root:Training: Train Iter(556000)\tEnv Step(556224)\tLoss(-24.882)\n","INFO:root:Evaluation: Train Iter(556032) Env Step(556224) Episode Return(306.933) \n","INFO:root:Training: Train Iter(556100)\tEnv Step(556352)\tLoss(-22.560)\n","INFO:root:Training: Train Iter(556200)\tEnv Step(556416)\tLoss(-24.824)\n","INFO:root:Training: Train Iter(556300)\tEnv Step(556544)\tLoss(-24.199)\n","INFO:root:Training: Train Iter(556400)\tEnv Step(556608)\tLoss(-23.971)\n","INFO:root:Training: Train Iter(556500)\tEnv Step(556736)\tLoss(-19.191)\n","INFO:root:Training: Train Iter(556600)\tEnv Step(556800)\tLoss(-25.775)\n","INFO:root:Training: Train Iter(556700)\tEnv Step(556928)\tLoss(-23.586)\n","INFO:root:Training: Train Iter(556800)\tEnv Step(557056)\tLoss(-23.432)\n","INFO:root:Training: Train Iter(556900)\tEnv Step(557120)\tLoss(-26.394)\n","INFO:root:Training: Train Iter(557000)\tEnv Step(557248)\tLoss(-26.233)\n","INFO:root:Evaluation: Train Iter(557056) Env Step(557248) Episode Return(310.392) \n","INFO:root:Training: Train Iter(557100)\tEnv Step(557312)\tLoss(-25.074)\n","INFO:root:Training: Train Iter(557200)\tEnv Step(557440)\tLoss(-24.289)\n","INFO:root:Training: Train Iter(557300)\tEnv Step(557504)\tLoss(-29.132)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69704\n","train_sample_count: 1672896\n","avg_envstep_per_episode: 69704.0\n","avg_sample_per_episode: 1672896.0\n","avg_envstep_per_sec: 8806.848043214932\n","avg_train_sample_per_sec: 211364.35303715838\n","avg_episode_per_sec: 0.12634637959392478\n","reward_mean: 302.7273864746094\n","reward_std: 0.0\n","reward_max: 302.7273864746094\n","reward_min: 302.7273864746094\n","total_envstep_count: 557632\n","total_train_sample_count: 13383168\n","total_episode_count: 590\n","INFO:root:Training: Train Iter(557400)\tEnv Step(557632)\tLoss(-20.834)\n","INFO:root:Training: Train Iter(557500)\tEnv Step(557696)\tLoss(-24.639)\n","INFO:root:Training: Train Iter(557600)\tEnv Step(557824)\tLoss(-19.604)\n","INFO:root:Training: Train Iter(557700)\tEnv Step(557952)\tLoss(-22.820)\n","INFO:root:Training: Train Iter(557800)\tEnv Step(558016)\tLoss(-18.577)\n","INFO:root:Training: Train Iter(557900)\tEnv Step(558144)\tLoss(-22.257)\n","INFO:root:Training: Train Iter(558000)\tEnv Step(558208)\tLoss(-28.549)\n","INFO:root:Evaluation: Train Iter(558080) Env Step(558272) Episode Return(304.243) \n","INFO:root:Training: Train Iter(558100)\tEnv Step(558336)\tLoss(-22.004)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69797\n","train_sample_count: 1675128\n","avg_envstep_per_episode: 69797.0\n","avg_sample_per_episode: 1675128.0\n","avg_envstep_per_sec: 8705.944894206878\n","avg_train_sample_per_sec: 208942.67746096503\n","avg_episode_per_sec: 0.12473236520490676\n","reward_mean: 302.68463134765625\n","reward_std: 0.0\n","reward_max: 302.68463134765625\n","reward_min: 302.68463134765625\n","total_envstep_count: 558376\n","total_train_sample_count: 13401024\n","total_episode_count: 591\n","INFO:root:Training: Train Iter(558200)\tEnv Step(558400)\tLoss(-24.875)\n","INFO:root:Training: Train Iter(558300)\tEnv Step(558528)\tLoss(-21.888)\n","INFO:root:Training: Train Iter(558400)\tEnv Step(558656)\tLoss(-16.396)\n","INFO:root:Training: Train Iter(558500)\tEnv Step(558720)\tLoss(-15.041)\n","INFO:root:Training: Train Iter(558600)\tEnv Step(558848)\tLoss(-26.591)\n","INFO:root:Training: Train Iter(558700)\tEnv Step(558912)\tLoss(-23.741)\n","INFO:root:Training: Train Iter(558800)\tEnv Step(559040)\tLoss(-22.059)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69882\n","train_sample_count: 1677168\n","avg_envstep_per_episode: 69882.0\n","avg_sample_per_episode: 1677168.0\n","avg_envstep_per_sec: 8017.941511120502\n","avg_train_sample_per_sec: 192430.59626689204\n","avg_episode_per_sec: 0.11473543274549242\n","reward_mean: 301.13623046875\n","reward_std: 0.0\n","reward_max: 301.13623046875\n","reward_min: 301.13623046875\n","total_envstep_count: 559056\n","total_train_sample_count: 13417344\n","total_episode_count: 592\n","INFO:root:Training: Train Iter(558900)\tEnv Step(559104)\tLoss(-20.698)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69902\n","train_sample_count: 1677648\n","avg_envstep_per_episode: 69902.0\n","avg_sample_per_episode: 1677648.0\n","avg_envstep_per_sec: 8534.576361988733\n","avg_train_sample_per_sec: 204829.8326877296\n","avg_episode_per_sec: 0.12209345028738425\n","reward_mean: 302.9242858886719\n","reward_std: 0.0\n","reward_max: 302.9242858886719\n","reward_min: 302.9242858886719\n","total_envstep_count: 559216\n","total_train_sample_count: 13421184\n","total_episode_count: 593\n","INFO:root:Training: Train Iter(559000)\tEnv Step(559232)\tLoss(-22.266)\n","INFO:root:Training: Train Iter(559100)\tEnv Step(559296)\tLoss(-23.540)\n","INFO:root:Evaluation: Train Iter(559104) Env Step(559296) Episode Return(300.581) \n","INFO:root:Training: Train Iter(559200)\tEnv Step(559424)\tLoss(-20.934)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 69943\n","train_sample_count: 1678632\n","avg_envstep_per_episode: 69943.0\n","avg_sample_per_episode: 1678632.0\n","avg_envstep_per_sec: 8878.855034309137\n","avg_train_sample_per_sec: 213092.5208234193\n","avg_episode_per_sec: 0.12694415501635814\n","reward_mean: 303.9356689453125\n","reward_std: 0.0\n","reward_max: 303.9356689453125\n","reward_min: 303.9356689453125\n","total_envstep_count: 559544\n","total_train_sample_count: 13429056\n","total_episode_count: 594\n","INFO:root:Training: Train Iter(559300)\tEnv Step(559552)\tLoss(-19.953)\n","INFO:root:Training: Train Iter(559400)\tEnv Step(559616)\tLoss(-23.829)\n","INFO:root:Training: Train Iter(559500)\tEnv Step(559744)\tLoss(-24.774)\n","INFO:root:Training: Train Iter(559600)\tEnv Step(559808)\tLoss(-22.380)\n","INFO:root:Training: Train Iter(559700)\tEnv Step(559936)\tLoss(-21.557)\n","INFO:root:Training: Train Iter(559800)\tEnv Step(560000)\tLoss(-26.158)\n","INFO:root:Training: Train Iter(559900)\tEnv Step(560128)\tLoss(-21.289)\n","INFO:root:Training: Train Iter(560000)\tEnv Step(560256)\tLoss(-24.810)\n","INFO:root:Training: Train Iter(560100)\tEnv Step(560320)\tLoss(-24.024)\n","INFO:root:Evaluation: Train Iter(560128) Env Step(560320) Episode Return(304.876) \n","INFO:root:Training: Train Iter(560200)\tEnv Step(560448)\tLoss(-21.450)\n","INFO:root:Training: Train Iter(560300)\tEnv Step(560512)\tLoss(-21.458)\n","INFO:root:Training: Train Iter(560400)\tEnv Step(560640)\tLoss(-25.838)\n","INFO:root:Training: Train Iter(560500)\tEnv Step(560704)\tLoss(-23.091)\n","INFO:root:Training: Train Iter(560600)\tEnv Step(560832)\tLoss(-21.238)\n","INFO:root:Training: Train Iter(560700)\tEnv Step(560896)\tLoss(-22.071)\n","INFO:root:Training: Train Iter(560800)\tEnv Step(561024)\tLoss(-22.475)\n","INFO:root:Training: Train Iter(560900)\tEnv Step(561152)\tLoss(-17.219)\n","INFO:root:Training: Train Iter(561000)\tEnv Step(561216)\tLoss(-25.288)\n","INFO:root:Training: Train Iter(561100)\tEnv Step(561344)\tLoss(-20.996)\n","INFO:root:Evaluation: Train Iter(561152) Env Step(561344) Episode Return(304.186) \n","INFO:root:Training: Train Iter(561200)\tEnv Step(561408)\tLoss(-26.403)\n","INFO:root:Training: Train Iter(561300)\tEnv Step(561536)\tLoss(-22.726)\n","INFO:root:Training: Train Iter(561400)\tEnv Step(561600)\tLoss(-25.421)\n","INFO:root:Training: Train Iter(561500)\tEnv Step(561728)\tLoss(-23.536)\n","INFO:root:Training: Train Iter(561600)\tEnv Step(561856)\tLoss(-22.275)\n","INFO:root:Training: Train Iter(561700)\tEnv Step(561920)\tLoss(-22.985)\n","INFO:root:Training: Train Iter(561800)\tEnv Step(562048)\tLoss(-24.171)\n","INFO:root:Training: Train Iter(561900)\tEnv Step(562112)\tLoss(-24.952)\n","INFO:root:Training: Train Iter(562000)\tEnv Step(562240)\tLoss(-25.019)\n","INFO:root:Training: Train Iter(562100)\tEnv Step(562304)\tLoss(-23.162)\n","INFO:root:Evaluation: Train Iter(562176) Env Step(562368) Episode Return(303.775) \n","INFO:root:Training: Train Iter(562200)\tEnv Step(562432)\tLoss(-21.080)\n","INFO:root:Training: Train Iter(562300)\tEnv Step(562496)\tLoss(-25.044)\n","INFO:root:Training: Train Iter(562400)\tEnv Step(562624)\tLoss(-23.067)\n","INFO:root:Training: Train Iter(562500)\tEnv Step(562752)\tLoss(-20.320)\n","INFO:root:Training: Train Iter(562600)\tEnv Step(562816)\tLoss(-25.174)\n","INFO:root:Training: Train Iter(562700)\tEnv Step(562944)\tLoss(-24.756)\n","INFO:root:Training: Train Iter(562800)\tEnv Step(563008)\tLoss(-20.217)\n","INFO:root:Training: Train Iter(562900)\tEnv Step(563136)\tLoss(-16.116)\n","INFO:root:Training: Train Iter(563000)\tEnv Step(563200)\tLoss(-22.929)\n","INFO:root:Training: Train Iter(563100)\tEnv Step(563328)\tLoss(-20.768)\n","INFO:root:Evaluation: Train Iter(563200) Env Step(563392) Episode Return(301.773) \n","INFO:root:Training: Train Iter(563200)\tEnv Step(563456)\tLoss(-23.968)\n","INFO:root:Training: Train Iter(563300)\tEnv Step(563520)\tLoss(-23.059)\n","INFO:root:Training: Train Iter(563400)\tEnv Step(563648)\tLoss(-23.432)\n","INFO:root:Training: Train Iter(563500)\tEnv Step(563712)\tLoss(-20.718)\n","INFO:root:Training: Train Iter(563600)\tEnv Step(563840)\tLoss(-24.256)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70483\n","train_sample_count: 1691592\n","avg_envstep_per_episode: 70483.0\n","avg_sample_per_episode: 1691592.0\n","avg_envstep_per_sec: 8734.304628443282\n","avg_train_sample_per_sec: 209623.3110826388\n","avg_episode_per_sec: 0.12392072738735983\n","reward_mean: 298.2894287109375\n","reward_std: 0.0\n","reward_max: 298.2894287109375\n","reward_min: 298.2894287109375\n","total_envstep_count: 563864\n","total_train_sample_count: 13532736\n","total_episode_count: 595\n","INFO:root:Training: Train Iter(563700)\tEnv Step(563904)\tLoss(-26.152)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70495\n","train_sample_count: 1691880\n","avg_envstep_per_episode: 70495.0\n","avg_sample_per_episode: 1691880.0\n","avg_envstep_per_sec: 8742.991966246123\n","avg_train_sample_per_sec: 209831.80718990698\n","avg_episode_per_sec: 0.12402286639117843\n","reward_mean: 301.52490234375\n","reward_std: 0.0\n","reward_max: 301.52490234375\n","reward_min: 301.52490234375\n","total_envstep_count: 563976\n","total_train_sample_count: 13535424\n","total_episode_count: 596\n","INFO:root:Training: Train Iter(563800)\tEnv Step(564032)\tLoss(-22.274)\n","INFO:root:Training: Train Iter(563900)\tEnv Step(564096)\tLoss(-24.100)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70505\n","train_sample_count: 1692120\n","avg_envstep_per_episode: 70505.0\n","avg_sample_per_episode: 1692120.0\n","avg_envstep_per_sec: 6122.440207544284\n","avg_train_sample_per_sec: 146938.5649810628\n","avg_episode_per_sec: 0.08683696486127628\n","reward_mean: 301.7989501953125\n","reward_std: 0.0\n","reward_max: 301.7989501953125\n","reward_min: 301.7989501953125\n","total_envstep_count: 564104\n","total_train_sample_count: 13538496\n","total_episode_count: 597\n","INFO:root:Training: Train Iter(564000)\tEnv Step(564224)\tLoss(-25.073)\n","INFO:root:Training: Train Iter(564100)\tEnv Step(564352)\tLoss(-15.067)\n","INFO:root:Training: Train Iter(564200)\tEnv Step(564416)\tLoss(-16.601)\n","INFO:root:Evaluation: Train Iter(564224) Env Step(564416) Episode Return(305.615) \n","INFO:root:Training: Train Iter(564300)\tEnv Step(564544)\tLoss(-19.970)\n","INFO:root:Training: Train Iter(564400)\tEnv Step(564608)\tLoss(-23.144)\n","INFO:root:Training: Train Iter(564500)\tEnv Step(564736)\tLoss(-18.594)\n","INFO:root:Training: Train Iter(564600)\tEnv Step(564800)\tLoss(-20.823)\n","INFO:root:Training: Train Iter(564700)\tEnv Step(564928)\tLoss(-27.269)\n","INFO:root:Training: Train Iter(564800)\tEnv Step(565056)\tLoss(-18.905)\n","INFO:root:Training: Train Iter(564900)\tEnv Step(565120)\tLoss(-21.171)\n","INFO:root:Training: Train Iter(565000)\tEnv Step(565248)\tLoss(-20.936)\n","INFO:root:Training: Train Iter(565100)\tEnv Step(565312)\tLoss(-23.677)\n","INFO:root:Training: Train Iter(565200)\tEnv Step(565440)\tLoss(-18.848)\n","INFO:root:Evaluation: Train Iter(565248) Env Step(565440) Episode Return(307.204) \n","INFO:root:Training: Train Iter(565300)\tEnv Step(565504)\tLoss(-17.929)\n","INFO:root:Training: Train Iter(565400)\tEnv Step(565632)\tLoss(-21.804)\n","INFO:root:Training: Train Iter(565500)\tEnv Step(565696)\tLoss(-24.219)\n","INFO:root:Training: Train Iter(565600)\tEnv Step(565824)\tLoss(-19.655)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70739\n","train_sample_count: 1697736\n","avg_envstep_per_episode: 70739.0\n","avg_sample_per_episode: 1697736.0\n","avg_envstep_per_sec: 8799.5922914541\n","avg_train_sample_per_sec: 211190.21499489844\n","avg_episode_per_sec: 0.12439520337372738\n","reward_mean: 297.9874267578125\n","reward_std: 0.0\n","reward_max: 297.9874267578125\n","reward_min: 297.9874267578125\n","total_envstep_count: 565912\n","total_train_sample_count: 13581888\n","total_episode_count: 598\n","INFO:root:Training: Train Iter(565700)\tEnv Step(565952)\tLoss(-22.010)\n","INFO:root:Training: Train Iter(565800)\tEnv Step(566016)\tLoss(-23.785)\n","INFO:root:Training: Train Iter(565900)\tEnv Step(566144)\tLoss(-23.985)\n","INFO:root:Training: Train Iter(566000)\tEnv Step(566208)\tLoss(-17.970)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70789\n","train_sample_count: 1698936\n","avg_envstep_per_episode: 70789.0\n","avg_sample_per_episode: 1698936.0\n","avg_envstep_per_sec: 8702.805872295727\n","avg_train_sample_per_sec: 208867.34093509745\n","avg_episode_per_sec: 0.12294008775792464\n","reward_mean: 300.3466491699219\n","reward_std: 0.0\n","reward_max: 300.3466491699219\n","reward_min: 300.3466491699219\n","total_envstep_count: 566312\n","total_train_sample_count: 13591488\n","total_episode_count: 599\n","INFO:root:Training: Train Iter(566100)\tEnv Step(566336)\tLoss(-23.283)\n","INFO:root:Training: Train Iter(566200)\tEnv Step(566400)\tLoss(-23.069)\n","INFO:root:Evaluation: Train Iter(566272) Env Step(566464) Episode Return(306.949) \n","INFO:root:Training: Train Iter(566300)\tEnv Step(566528)\tLoss(-23.083)\n","INFO:root:Training: Train Iter(566400)\tEnv Step(566656)\tLoss(-22.497)\n","INFO:root:Training: Train Iter(566500)\tEnv Step(566720)\tLoss(-23.789)\n","INFO:root:Training: Train Iter(566600)\tEnv Step(566848)\tLoss(-23.585)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70859\n","train_sample_count: 1700616\n","avg_envstep_per_episode: 70859.0\n","avg_sample_per_episode: 1700616.0\n","avg_envstep_per_sec: 8018.386615211541\n","avg_train_sample_per_sec: 192441.27876507698\n","avg_episode_per_sec: 0.11315974844707857\n","reward_mean: 302.46990966796875\n","reward_std: 0.0\n","reward_max: 302.46990966796875\n","reward_min: 302.46990966796875\n","total_envstep_count: 566872\n","total_train_sample_count: 13604928\n","total_episode_count: 600\n","INFO:root:Training: Train Iter(566700)\tEnv Step(566912)\tLoss(-22.162)\n","INFO:root:Training: Train Iter(566800)\tEnv Step(567040)\tLoss(-27.430)\n","INFO:root:Training: Train Iter(566900)\tEnv Step(567104)\tLoss(-23.833)\n","INFO:root:Training: Train Iter(567000)\tEnv Step(567232)\tLoss(-24.884)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70911\n","train_sample_count: 1701864\n","avg_envstep_per_episode: 70911.0\n","avg_sample_per_episode: 1701864.0\n","avg_envstep_per_sec: 8527.891760533706\n","avg_train_sample_per_sec: 204669.40225280897\n","avg_episode_per_sec: 0.12026190239220581\n","reward_mean: 300.22772216796875\n","reward_std: 0.0\n","reward_max: 300.22772216796875\n","reward_min: 300.22772216796875\n","total_envstep_count: 567288\n","total_train_sample_count: 13614912\n","total_episode_count: 601\n","INFO:root:Training: Train Iter(567100)\tEnv Step(567296)\tLoss(-24.595)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 70927\n","train_sample_count: 1702248\n","avg_envstep_per_episode: 70927.0\n","avg_sample_per_episode: 1702248.0\n","avg_envstep_per_sec: 8874.804825487423\n","avg_train_sample_per_sec: 212995.31581169815\n","avg_episode_per_sec: 0.12512590163812684\n","reward_mean: 302.91571044921875\n","reward_std: 0.0\n","reward_max: 302.91571044921875\n","reward_min: 302.91571044921875\n","total_envstep_count: 567416\n","total_train_sample_count: 13617984\n","total_episode_count: 602\n","INFO:root:Training: Train Iter(567200)\tEnv Step(567424)\tLoss(-26.818)\n","INFO:root:Evaluation: Train Iter(567296) Env Step(567488) Episode Return(301.026) \n","INFO:root:Training: Train Iter(567300)\tEnv Step(567552)\tLoss(-21.716)\n","INFO:root:Training: Train Iter(567400)\tEnv Step(567616)\tLoss(-15.599)\n","INFO:root:Training: Train Iter(567500)\tEnv Step(567744)\tLoss(-23.094)\n","INFO:root:Training: Train Iter(567600)\tEnv Step(567808)\tLoss(-27.115)\n","INFO:root:Training: Train Iter(567700)\tEnv Step(567936)\tLoss(-13.649)\n","INFO:root:Training: Train Iter(567800)\tEnv Step(568000)\tLoss(-24.472)\n","INFO:root:Training: Train Iter(567900)\tEnv Step(568128)\tLoss(-26.236)\n","INFO:root:Training: Train Iter(568000)\tEnv Step(568256)\tLoss(-23.614)\n","INFO:root:Training: Train Iter(568100)\tEnv Step(568320)\tLoss(-26.783)\n","INFO:root:Training: Train Iter(568200)\tEnv Step(568448)\tLoss(-23.380)\n","INFO:root:Training: Train Iter(568300)\tEnv Step(568512)\tLoss(-26.899)\n","INFO:root:Evaluation: Train Iter(568320) Env Step(568512) Episode Return(306.489) \n","INFO:root:Training: Train Iter(568400)\tEnv Step(568640)\tLoss(-25.759)\n","INFO:root:Training: Train Iter(568500)\tEnv Step(568704)\tLoss(-24.122)\n","INFO:root:Training: Train Iter(568600)\tEnv Step(568832)\tLoss(-17.935)\n","INFO:root:Training: Train Iter(568700)\tEnv Step(568896)\tLoss(-17.401)\n","INFO:root:Training: Train Iter(568800)\tEnv Step(569024)\tLoss(-20.366)\n","INFO:root:Training: Train Iter(568900)\tEnv Step(569152)\tLoss(-23.797)\n","INFO:root:Training: Train Iter(569000)\tEnv Step(569216)\tLoss(-18.182)\n","INFO:root:Training: Train Iter(569100)\tEnv Step(569344)\tLoss(-22.961)\n","INFO:root:Training: Train Iter(569200)\tEnv Step(569408)\tLoss(-23.948)\n","INFO:root:Training: Train Iter(569300)\tEnv Step(569536)\tLoss(-18.090)\n","INFO:root:Evaluation: Train Iter(569344) Env Step(569536) Episode Return(306.835) \n","INFO:root:Training: Train Iter(569400)\tEnv Step(569600)\tLoss(-26.008)\n","INFO:root:Training: Train Iter(569500)\tEnv Step(569728)\tLoss(-23.606)\n","INFO:root:Training: Train Iter(569600)\tEnv Step(569856)\tLoss(-18.053)\n","INFO:root:Training: Train Iter(569700)\tEnv Step(569920)\tLoss(-26.127)\n","INFO:root:Training: Train Iter(569800)\tEnv Step(570048)\tLoss(-22.047)\n","INFO:root:Training: Train Iter(569900)\tEnv Step(570112)\tLoss(-21.242)\n","INFO:root:Training: Train Iter(570000)\tEnv Step(570240)\tLoss(-24.660)\n","INFO:root:Training: Train Iter(570100)\tEnv Step(570304)\tLoss(-24.153)\n","INFO:root:Training: Train Iter(570200)\tEnv Step(570432)\tLoss(-25.841)\n","INFO:root:Training: Train Iter(570300)\tEnv Step(570496)\tLoss(-23.827)\n","INFO:root:Evaluation: Train Iter(570368) Env Step(570560) Episode Return(303.385) \n","INFO:root:Training: Train Iter(570400)\tEnv Step(570624)\tLoss(-20.478)\n","INFO:root:Training: Train Iter(570500)\tEnv Step(570752)\tLoss(-23.780)\n","INFO:root:Training: Train Iter(570600)\tEnv Step(570816)\tLoss(-25.015)\n","INFO:root:Training: Train Iter(570700)\tEnv Step(570944)\tLoss(-24.699)\n","INFO:root:Training: Train Iter(570800)\tEnv Step(571008)\tLoss(-23.508)\n","INFO:root:Training: Train Iter(570900)\tEnv Step(571136)\tLoss(-24.269)\n","INFO:root:Training: Train Iter(571000)\tEnv Step(571200)\tLoss(-22.085)\n","INFO:root:Training: Train Iter(571100)\tEnv Step(571328)\tLoss(-24.897)\n","INFO:root:Training: Train Iter(571200)\tEnv Step(571456)\tLoss(-24.872)\n","INFO:root:Training: Train Iter(571300)\tEnv Step(571520)\tLoss(-25.216)\n","INFO:root:Evaluation: Train Iter(571392) Env Step(571584) Episode Return(308.290) \n","INFO:root:Training: Train Iter(571400)\tEnv Step(571648)\tLoss(-25.081)\n","INFO:root:Training: Train Iter(571500)\tEnv Step(571712)\tLoss(-24.459)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71478\n","train_sample_count: 1715472\n","avg_envstep_per_episode: 71478.0\n","avg_sample_per_episode: 1715472.0\n","avg_envstep_per_sec: 6120.846758305838\n","avg_train_sample_per_sec: 146900.32219934012\n","avg_episode_per_sec: 0.08563259685925513\n","reward_mean: 303.69561767578125\n","reward_std: 0.0\n","reward_max: 303.69561767578125\n","reward_min: 303.69561767578125\n","total_envstep_count: 571824\n","total_train_sample_count: 13723776\n","total_episode_count: 603\n","INFO:root:Training: Train Iter(571600)\tEnv Step(571840)\tLoss(-24.217)\n","INFO:root:Training: Train Iter(571700)\tEnv Step(571904)\tLoss(-12.911)\n","INFO:root:collect end:\n","episode_count: 2\n","envstep_count: 142971\n","train_sample_count: 3431304\n","avg_envstep_per_episode: 71485.5\n","avg_sample_per_episode: 1715652.0\n","avg_envstep_per_sec: 8734.981343446088\n","avg_train_sample_per_sec: 209639.5522427061\n","avg_episode_per_sec: 0.12219235150409646\n","reward_mean: 302.363525390625\n","reward_std: 0.513641357421875\n","reward_max: 302.8771667480469\n","reward_min: 301.8498840332031\n","total_envstep_count: 571912\n","total_train_sample_count: 13725888\n","total_episode_count: 605\n","INFO:root:Training: Train Iter(571800)\tEnv Step(572032)\tLoss(-18.730)\n","INFO:root:Training: Train Iter(571900)\tEnv Step(572096)\tLoss(-22.345)\n","INFO:root:Training: Train Iter(572000)\tEnv Step(572224)\tLoss(-24.768)\n","INFO:root:Training: Train Iter(572100)\tEnv Step(572352)\tLoss(-25.827)\n","INFO:root:Training: Train Iter(572200)\tEnv Step(572416)\tLoss(-15.996)\n","INFO:root:Training: Train Iter(572300)\tEnv Step(572544)\tLoss(-28.007)\n","INFO:root:Training: Train Iter(572400)\tEnv Step(572608)\tLoss(-21.369)\n","INFO:root:Evaluation: Train Iter(572416) Env Step(572608) Episode Return(309.409) \n","INFO:root:Training: Train Iter(572500)\tEnv Step(572736)\tLoss(-24.851)\n","INFO:root:Training: Train Iter(572600)\tEnv Step(572800)\tLoss(-22.614)\n","INFO:root:Training: Train Iter(572700)\tEnv Step(572928)\tLoss(-26.816)\n","INFO:root:Training: Train Iter(572800)\tEnv Step(573056)\tLoss(-25.082)\n","INFO:root:Training: Train Iter(572900)\tEnv Step(573120)\tLoss(-24.941)\n","INFO:root:Training: Train Iter(573000)\tEnv Step(573248)\tLoss(-23.913)\n","INFO:root:Training: Train Iter(573100)\tEnv Step(573312)\tLoss(-22.281)\n","INFO:root:Training: Train Iter(573200)\tEnv Step(573440)\tLoss(-22.034)\n","INFO:root:Training: Train Iter(573300)\tEnv Step(573504)\tLoss(-17.931)\n","INFO:root:Training: Train Iter(573400)\tEnv Step(573632)\tLoss(-25.725)\n","INFO:root:Evaluation: Train Iter(573440) Env Step(573632) Episode Return(308.190) \n","INFO:root:Training: Train Iter(573500)\tEnv Step(573696)\tLoss(-23.574)\n","INFO:root:Training: Train Iter(573600)\tEnv Step(573824)\tLoss(-22.787)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71729\n","train_sample_count: 1721496\n","avg_envstep_per_episode: 71729.0\n","avg_sample_per_episode: 1721496.0\n","avg_envstep_per_sec: 8795.491068918254\n","avg_train_sample_per_sec: 211091.78565403813\n","avg_episode_per_sec: 0.12262113049001457\n","reward_mean: 299.89300537109375\n","reward_std: 0.0\n","reward_max: 299.89300537109375\n","reward_min: 299.89300537109375\n","total_envstep_count: 573832\n","total_train_sample_count: 13771968\n","total_episode_count: 606\n","INFO:root:Training: Train Iter(573700)\tEnv Step(573952)\tLoss(-20.790)\n","INFO:root:Training: Train Iter(573800)\tEnv Step(574016)\tLoss(-18.836)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71758\n","train_sample_count: 1722192\n","avg_envstep_per_episode: 71758.0\n","avg_sample_per_episode: 1722192.0\n","avg_envstep_per_sec: 8697.021376951086\n","avg_train_sample_per_sec: 208728.5130468261\n","avg_episode_per_sec: 0.12119932797668673\n","reward_mean: 302.1546325683594\n","reward_std: 0.0\n","reward_max: 302.1546325683594\n","reward_min: 302.1546325683594\n","total_envstep_count: 574064\n","total_train_sample_count: 13777536\n","total_episode_count: 607\n","INFO:root:Training: Train Iter(573900)\tEnv Step(574144)\tLoss(-19.080)\n","INFO:root:Training: Train Iter(574000)\tEnv Step(574208)\tLoss(-22.513)\n","INFO:root:Training: Train Iter(574100)\tEnv Step(574336)\tLoss(-25.240)\n","INFO:root:Training: Train Iter(574200)\tEnv Step(574400)\tLoss(-26.407)\n","INFO:root:Training: Train Iter(574300)\tEnv Step(574528)\tLoss(-21.069)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71826\n","train_sample_count: 1723824\n","avg_envstep_per_episode: 71826.0\n","avg_sample_per_episode: 1723824.0\n","avg_envstep_per_sec: 8018.861103604865\n","avg_train_sample_per_sec: 192452.66648651677\n","avg_episode_per_sec: 0.11164287449676809\n","reward_mean: 302.49884033203125\n","reward_std: 0.0\n","reward_max: 302.49884033203125\n","reward_min: 302.49884033203125\n","total_envstep_count: 574608\n","total_train_sample_count: 13790592\n","total_episode_count: 608\n","INFO:root:Training: Train Iter(574400)\tEnv Step(574656)\tLoss(-19.187)\n","INFO:root:Evaluation: Train Iter(574464) Env Step(574656) Episode Return(304.917) \n","INFO:root:Training: Train Iter(574500)\tEnv Step(574720)\tLoss(-23.964)\n","INFO:root:Training: Train Iter(574600)\tEnv Step(574848)\tLoss(-22.157)\n","INFO:root:Training: Train Iter(574700)\tEnv Step(574912)\tLoss(-22.847)\n","INFO:root:Training: Train Iter(574800)\tEnv Step(575040)\tLoss(-23.741)\n","INFO:root:Training: Train Iter(574900)\tEnv Step(575104)\tLoss(-24.422)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71897\n","train_sample_count: 1725528\n","avg_envstep_per_episode: 71897.0\n","avg_sample_per_episode: 1725528.0\n","avg_envstep_per_sec: 8527.69941954253\n","avg_train_sample_per_sec: 204664.78606902075\n","avg_episode_per_sec: 0.11860994783568898\n","reward_mean: 301.64105224609375\n","reward_std: 0.0\n","reward_max: 301.64105224609375\n","reward_min: 301.64105224609375\n","total_envstep_count: 575176\n","total_train_sample_count: 13804224\n","total_episode_count: 609\n","INFO:root:Training: Train Iter(575000)\tEnv Step(575232)\tLoss(-23.547)\n","INFO:root:Training: Train Iter(575100)\tEnv Step(575296)\tLoss(-20.745)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 71899\n","train_sample_count: 1725576\n","avg_envstep_per_episode: 71899.0\n","avg_sample_per_episode: 1725576.0\n","avg_envstep_per_sec: 8873.915281237803\n","avg_train_sample_per_sec: 212973.9667497073\n","avg_episode_per_sec: 0.1234219569289949\n","reward_mean: 301.87884521484375\n","reward_std: 0.0\n","reward_max: 301.87884521484375\n","reward_min: 301.87884521484375\n","total_envstep_count: 575304\n","total_train_sample_count: 13807296\n","total_episode_count: 610\n","INFO:root:Training: Train Iter(575200)\tEnv Step(575424)\tLoss(-23.015)\n","INFO:root:Training: Train Iter(575300)\tEnv Step(575552)\tLoss(-21.579)\n","INFO:root:Training: Train Iter(575400)\tEnv Step(575616)\tLoss(-22.057)\n","INFO:root:Evaluation: Train Iter(575488) Env Step(575680) Episode Return(305.398) \n","INFO:root:Training: Train Iter(575500)\tEnv Step(575744)\tLoss(-21.142)\n","INFO:root:Training: Train Iter(575600)\tEnv Step(575808)\tLoss(-26.008)\n","INFO:root:Training: Train Iter(575700)\tEnv Step(575936)\tLoss(-16.605)\n","INFO:root:Training: Train Iter(575800)\tEnv Step(576000)\tLoss(-23.467)\n","INFO:root:Training: Train Iter(575900)\tEnv Step(576128)\tLoss(-24.865)\n","INFO:root:Training: Train Iter(576000)\tEnv Step(576256)\tLoss(-18.355)\n","INFO:root:Training: Train Iter(576100)\tEnv Step(576320)\tLoss(-24.660)\n","INFO:root:Training: Train Iter(576200)\tEnv Step(576448)\tLoss(-25.152)\n","INFO:root:Training: Train Iter(576300)\tEnv Step(576512)\tLoss(-23.718)\n","INFO:root:Training: Train Iter(576400)\tEnv Step(576640)\tLoss(-22.799)\n","INFO:root:Training: Train Iter(576500)\tEnv Step(576704)\tLoss(-24.654)\n","INFO:root:Evaluation: Train Iter(576512) Env Step(576704) Episode Return(304.807) \n","INFO:root:Training: Train Iter(576600)\tEnv Step(576832)\tLoss(-17.501)\n","INFO:root:Training: Train Iter(576700)\tEnv Step(576896)\tLoss(-25.319)\n","INFO:root:Training: Train Iter(576800)\tEnv Step(577024)\tLoss(-18.291)\n","INFO:root:Training: Train Iter(576900)\tEnv Step(577152)\tLoss(-19.786)\n","INFO:root:Training: Train Iter(577000)\tEnv Step(577216)\tLoss(-12.898)\n","INFO:root:Training: Train Iter(577100)\tEnv Step(577344)\tLoss(-25.772)\n","INFO:root:Training: Train Iter(577200)\tEnv Step(577408)\tLoss(-23.843)\n","INFO:root:Training: Train Iter(577300)\tEnv Step(577536)\tLoss(-21.173)\n","INFO:root:Training: Train Iter(577400)\tEnv Step(577600)\tLoss(-23.169)\n","INFO:root:Training: Train Iter(577500)\tEnv Step(577728)\tLoss(-26.299)\n","INFO:root:Evaluation: Train Iter(577536) Env Step(577728) Episode Return(305.518) \n","INFO:root:Training: Train Iter(577600)\tEnv Step(577856)\tLoss(-19.712)\n","INFO:root:Training: Train Iter(577700)\tEnv Step(577920)\tLoss(-22.060)\n","INFO:root:Training: Train Iter(577800)\tEnv Step(578048)\tLoss(-24.132)\n","INFO:root:Training: Train Iter(577900)\tEnv Step(578112)\tLoss(-23.790)\n","INFO:root:Training: Train Iter(578000)\tEnv Step(578240)\tLoss(-18.072)\n","INFO:root:Training: Train Iter(578100)\tEnv Step(578304)\tLoss(-25.969)\n","INFO:root:Training: Train Iter(578200)\tEnv Step(578432)\tLoss(-23.382)\n","INFO:root:Training: Train Iter(578300)\tEnv Step(578496)\tLoss(-25.914)\n","INFO:root:Training: Train Iter(578400)\tEnv Step(578624)\tLoss(-21.789)\n","INFO:root:Training: Train Iter(578500)\tEnv Step(578752)\tLoss(-25.975)\n","INFO:root:Evaluation: Train Iter(578560) Env Step(578752) Episode Return(306.792) \n","INFO:root:Training: Train Iter(578600)\tEnv Step(578816)\tLoss(-23.650)\n","INFO:root:Training: Train Iter(578700)\tEnv Step(578944)\tLoss(-23.607)\n","INFO:root:Training: Train Iter(578800)\tEnv Step(579008)\tLoss(-24.497)\n","INFO:root:Training: Train Iter(578900)\tEnv Step(579136)\tLoss(-25.197)\n","INFO:root:Training: Train Iter(579000)\tEnv Step(579200)\tLoss(-25.122)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72410\n","train_sample_count: 1737840\n","avg_envstep_per_episode: 72410.0\n","avg_sample_per_episode: 1737840.0\n","avg_envstep_per_sec: 8724.618472698812\n","avg_train_sample_per_sec: 209390.84334477148\n","avg_episode_per_sec: 0.12048913786353835\n","reward_mean: 305.20611572265625\n","reward_std: 0.0\n","reward_max: 305.20611572265625\n","reward_min: 305.20611572265625\n","total_envstep_count: 579280\n","total_train_sample_count: 13902720\n","total_episode_count: 611\n","INFO:root:Training: Train Iter(579100)\tEnv Step(579328)\tLoss(-25.723)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72415\n","train_sample_count: 1737960\n","avg_envstep_per_episode: 72415.0\n","avg_sample_per_episode: 1737960.0\n","avg_envstep_per_sec: 8733.530841010164\n","avg_train_sample_per_sec: 209604.74018424394\n","avg_episode_per_sec: 0.12060389202527329\n","reward_mean: 303.2132873535156\n","reward_std: 0.0\n","reward_max: 303.2132873535156\n","reward_min: 303.2132873535156\n","total_envstep_count: 579400\n","total_train_sample_count: 13905600\n","total_episode_count: 612\n","INFO:root:Training: Train Iter(579200)\tEnv Step(579456)\tLoss(-20.304)\n","INFO:root:Training: Train Iter(579300)\tEnv Step(579520)\tLoss(-22.214)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72430\n","train_sample_count: 1738320\n","avg_envstep_per_episode: 72430.0\n","avg_sample_per_episode: 1738320.0\n","avg_envstep_per_sec: 6118.98010209593\n","avg_train_sample_per_sec: 146855.52245030232\n","avg_episode_per_sec: 0.08448129369178421\n","reward_mean: 304.1228332519531\n","reward_std: 0.0\n","reward_max: 304.1228332519531\n","reward_min: 304.1228332519531\n","total_envstep_count: 579528\n","total_train_sample_count: 13908672\n","total_episode_count: 613\n","INFO:root:Training: Train Iter(579400)\tEnv Step(579648)\tLoss(-25.642)\n","INFO:root:Training: Train Iter(579500)\tEnv Step(579712)\tLoss(-26.185)\n","INFO:root:Evaluation: Train Iter(579584) Env Step(579776) Episode Return(304.602) \n","INFO:root:Training: Train Iter(579600)\tEnv Step(579840)\tLoss(-24.996)\n","INFO:root:Training: Train Iter(579700)\tEnv Step(579904)\tLoss(-25.567)\n","INFO:root:Training: Train Iter(579800)\tEnv Step(580032)\tLoss(-21.967)\n","INFO:root:Training: Train Iter(579900)\tEnv Step(580096)\tLoss(-23.461)\n","INFO:root:Training: Train Iter(580000)\tEnv Step(580224)\tLoss(-23.352)\n","INFO:root:Training: Train Iter(580100)\tEnv Step(580352)\tLoss(-22.984)\n","INFO:root:Training: Train Iter(580200)\tEnv Step(580416)\tLoss(-21.470)\n","INFO:root:Training: Train Iter(580300)\tEnv Step(580544)\tLoss(-22.120)\n","INFO:root:Training: Train Iter(580400)\tEnv Step(580608)\tLoss(-26.013)\n","INFO:root:Training: Train Iter(580500)\tEnv Step(580736)\tLoss(-23.371)\n","INFO:root:Training: Train Iter(580600)\tEnv Step(580800)\tLoss(-25.181)\n","INFO:root:Evaluation: Train Iter(580608) Env Step(580800) Episode Return(306.226) \n","INFO:root:Training: Train Iter(580700)\tEnv Step(580928)\tLoss(-21.228)\n","INFO:root:Training: Train Iter(580800)\tEnv Step(581056)\tLoss(-24.221)\n","INFO:root:Training: Train Iter(580900)\tEnv Step(581120)\tLoss(-24.815)\n","INFO:root:Training: Train Iter(581000)\tEnv Step(581248)\tLoss(-23.304)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72664\n","train_sample_count: 1743936\n","avg_envstep_per_episode: 72664.0\n","avg_sample_per_episode: 1743936.0\n","avg_envstep_per_sec: 8788.110354995551\n","avg_train_sample_per_sec: 210914.64851989324\n","avg_episode_per_sec: 0.12094173669211096\n","reward_mean: 304.4784240722656\n","reward_std: 0.0\n","reward_max: 304.4784240722656\n","reward_min: 304.4784240722656\n","total_envstep_count: 581312\n","total_train_sample_count: 13951488\n","total_episode_count: 614\n","INFO:root:Training: Train Iter(581100)\tEnv Step(581312)\tLoss(-27.490)\n","INFO:root:Training: Train Iter(581200)\tEnv Step(581440)\tLoss(-20.570)\n","INFO:root:Training: Train Iter(581300)\tEnv Step(581504)\tLoss(-16.690)\n","INFO:root:Training: Train Iter(581400)\tEnv Step(581632)\tLoss(-23.550)\n","INFO:root:Training: Train Iter(581500)\tEnv Step(581696)\tLoss(-16.461)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72728\n","train_sample_count: 1745472\n","avg_envstep_per_episode: 72728.0\n","avg_sample_per_episode: 1745472.0\n","avg_envstep_per_sec: 8695.188012185528\n","avg_train_sample_per_sec: 208684.51229245268\n","avg_episode_per_sec: 0.11955763959115509\n","reward_mean: 302.3961181640625\n","reward_std: 0.0\n","reward_max: 302.3961181640625\n","reward_min: 302.3961181640625\n","total_envstep_count: 581824\n","total_train_sample_count: 13963776\n","total_episode_count: 615\n","INFO:root:Training: Train Iter(581600)\tEnv Step(581824)\tLoss(-25.668)\n","INFO:root:Evaluation: Train Iter(581632) Env Step(581824) Episode Return(305.849) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72736\n","train_sample_count: 1745664\n","avg_envstep_per_episode: 72736.0\n","avg_sample_per_episode: 1745664.0\n","avg_envstep_per_sec: 8013.427305610245\n","avg_train_sample_per_sec: 192322.25533464586\n","avg_episode_per_sec: 0.11017140488355484\n","reward_mean: 305.38519287109375\n","reward_std: 0.0\n","reward_max: 305.38519287109375\n","reward_min: 305.38519287109375\n","total_envstep_count: 581896\n","total_train_sample_count: 13965504\n","total_episode_count: 616\n","INFO:root:Training: Train Iter(581700)\tEnv Step(581952)\tLoss(-22.936)\n","INFO:root:Training: Train Iter(581800)\tEnv Step(582016)\tLoss(-23.958)\n","INFO:root:Training: Train Iter(581900)\tEnv Step(582144)\tLoss(-21.533)\n","INFO:root:Training: Train Iter(582000)\tEnv Step(582208)\tLoss(-23.910)\n","INFO:root:Training: Train Iter(582100)\tEnv Step(582336)\tLoss(-26.915)\n","INFO:root:Training: Train Iter(582200)\tEnv Step(582400)\tLoss(-26.176)\n","INFO:root:Training: Train Iter(582300)\tEnv Step(582528)\tLoss(-24.864)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72830\n","train_sample_count: 1747920\n","avg_envstep_per_episode: 72830.0\n","avg_sample_per_episode: 1747920.0\n","avg_envstep_per_sec: 8868.589329127259\n","avg_train_sample_per_sec: 212846.1438990542\n","avg_episode_per_sec: 0.12177110159449758\n","reward_mean: 303.7206115722656\n","reward_std: 0.0\n","reward_max: 303.7206115722656\n","reward_min: 303.7206115722656\n","total_envstep_count: 582640\n","total_train_sample_count: 13983360\n","total_episode_count: 617\n","INFO:root:Training: Train Iter(582400)\tEnv Step(582656)\tLoss(-16.701)\n","INFO:root:Training: Train Iter(582500)\tEnv Step(582720)\tLoss(-28.007)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 72843\n","train_sample_count: 1748232\n","avg_envstep_per_episode: 72843.0\n","avg_sample_per_episode: 1748232.0\n","avg_envstep_per_sec: 8524.360161149969\n","avg_train_sample_per_sec: 204584.64386759928\n","avg_episode_per_sec: 0.1170237381924134\n","reward_mean: 304.3370056152344\n","reward_std: 0.0\n","reward_max: 304.3370056152344\n","reward_min: 304.3370056152344\n","total_envstep_count: 582744\n","total_train_sample_count: 13985856\n","total_episode_count: 618\n","INFO:root:Training: Train Iter(582600)\tEnv Step(582848)\tLoss(-13.471)\n","INFO:root:Evaluation: Train Iter(582656) Env Step(582848) Episode Return(305.344) \n","INFO:root:Training: Train Iter(582700)\tEnv Step(582912)\tLoss(-25.211)\n","INFO:root:Training: Train Iter(582800)\tEnv Step(583040)\tLoss(-23.450)\n","INFO:root:Training: Train Iter(582900)\tEnv Step(583104)\tLoss(-21.923)\n","INFO:root:Training: Train Iter(583000)\tEnv Step(583232)\tLoss(-23.217)\n","INFO:root:Training: Train Iter(583100)\tEnv Step(583296)\tLoss(-18.476)\n","INFO:root:Training: Train Iter(583200)\tEnv Step(583424)\tLoss(-26.304)\n","INFO:root:Training: Train Iter(583300)\tEnv Step(583552)\tLoss(-23.758)\n","INFO:root:Training: Train Iter(583400)\tEnv Step(583616)\tLoss(-23.319)\n","INFO:root:Training: Train Iter(583500)\tEnv Step(583744)\tLoss(-19.980)\n","INFO:root:Training: Train Iter(583600)\tEnv Step(583808)\tLoss(-26.428)\n","INFO:root:Evaluation: Train Iter(583680) Env Step(583872) Episode Return(309.676) \n","INFO:root:Training: Train Iter(583700)\tEnv Step(583936)\tLoss(-25.721)\n","INFO:root:Training: Train Iter(583800)\tEnv Step(584000)\tLoss(-20.515)\n","INFO:root:Training: Train Iter(583900)\tEnv Step(584128)\tLoss(-26.488)\n","INFO:root:Training: Train Iter(584000)\tEnv Step(584256)\tLoss(-23.923)\n","INFO:root:Training: Train Iter(584100)\tEnv Step(584320)\tLoss(-24.122)\n","INFO:root:Training: Train Iter(584200)\tEnv Step(584448)\tLoss(-16.464)\n","INFO:root:Training: Train Iter(584300)\tEnv Step(584512)\tLoss(-16.166)\n","INFO:root:Training: Train Iter(584400)\tEnv Step(584640)\tLoss(-19.967)\n","INFO:root:Training: Train Iter(584500)\tEnv Step(584704)\tLoss(-17.677)\n","INFO:root:Training: Train Iter(584600)\tEnv Step(584832)\tLoss(-19.094)\n","INFO:root:Training: Train Iter(584700)\tEnv Step(584896)\tLoss(-21.462)\n","INFO:root:Evaluation: Train Iter(584704) Env Step(584896) Episode Return(308.603) \n","INFO:root:Training: Train Iter(584800)\tEnv Step(585024)\tLoss(-24.355)\n","INFO:root:Training: Train Iter(584900)\tEnv Step(585152)\tLoss(-19.071)\n","INFO:root:Training: Train Iter(585000)\tEnv Step(585216)\tLoss(-26.661)\n","INFO:root:Training: Train Iter(585100)\tEnv Step(585344)\tLoss(-26.596)\n","INFO:root:Training: Train Iter(585200)\tEnv Step(585408)\tLoss(-26.922)\n","INFO:root:Training: Train Iter(585300)\tEnv Step(585536)\tLoss(-19.770)\n","INFO:root:Training: Train Iter(585400)\tEnv Step(585600)\tLoss(-18.731)\n","INFO:root:Training: Train Iter(585500)\tEnv Step(585728)\tLoss(-24.229)\n","INFO:root:Training: Train Iter(585600)\tEnv Step(585856)\tLoss(-26.067)\n","INFO:root:Training: Train Iter(585700)\tEnv Step(585920)\tLoss(-27.362)\n","INFO:root:Evaluation: Train Iter(585728) Env Step(585920) Episode Return(309.769) \n","INFO:root:Training: Train Iter(585800)\tEnv Step(586048)\tLoss(-26.669)\n","INFO:root:Training: Train Iter(585900)\tEnv Step(586112)\tLoss(-20.555)\n","INFO:root:Training: Train Iter(586000)\tEnv Step(586240)\tLoss(-22.691)\n","INFO:root:Training: Train Iter(586100)\tEnv Step(586304)\tLoss(-23.576)\n","INFO:root:Training: Train Iter(586200)\tEnv Step(586432)\tLoss(-19.832)\n","INFO:root:Training: Train Iter(586300)\tEnv Step(586496)\tLoss(-25.837)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73328\n","train_sample_count: 1759872\n","avg_envstep_per_episode: 73328.0\n","avg_sample_per_episode: 1759872.0\n","avg_envstep_per_sec: 8725.819023905997\n","avg_train_sample_per_sec: 209419.65657374394\n","avg_episode_per_sec: 0.118997095569305\n","reward_mean: 305.6296081542969\n","reward_std: 0.0\n","reward_max: 305.6296081542969\n","reward_min: 305.6296081542969\n","total_envstep_count: 586624\n","total_train_sample_count: 14078976\n","total_episode_count: 619\n","INFO:root:Training: Train Iter(586400)\tEnv Step(586624)\tLoss(-23.179)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73337\n","train_sample_count: 1760088\n","avg_envstep_per_episode: 73337.0\n","avg_sample_per_episode: 1760088.0\n","avg_envstep_per_sec: 8733.49683084827\n","avg_train_sample_per_sec: 209603.92394035845\n","avg_episode_per_sec: 0.11908718424326423\n","reward_mean: 305.2280578613281\n","reward_std: 0.0\n","reward_max: 305.2280578613281\n","reward_min: 305.2280578613281\n","total_envstep_count: 586696\n","total_train_sample_count: 14080704\n","total_episode_count: 620\n","INFO:root:Training: Train Iter(586500)\tEnv Step(586752)\tLoss(-16.013)\n","INFO:root:Training: Train Iter(586600)\tEnv Step(586816)\tLoss(-23.015)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73343\n","train_sample_count: 1760232\n","avg_envstep_per_episode: 73343.0\n","avg_sample_per_episode: 1760232.0\n","avg_envstep_per_sec: 6121.318398504204\n","avg_train_sample_per_sec: 146911.6415641009\n","avg_episode_per_sec: 0.08346152186990176\n","reward_mean: 304.6270751953125\n","reward_std: 0.0\n","reward_max: 304.6270751953125\n","reward_min: 304.6270751953125\n","total_envstep_count: 586824\n","total_train_sample_count: 14083776\n","total_episode_count: 621\n","INFO:root:Training: Train Iter(586700)\tEnv Step(586944)\tLoss(-18.247)\n","INFO:root:Evaluation: Train Iter(586752) Env Step(586944) Episode Return(304.440) \n","INFO:root:Training: Train Iter(586800)\tEnv Step(587008)\tLoss(-24.636)\n","INFO:root:Training: Train Iter(586900)\tEnv Step(587136)\tLoss(-18.376)\n","INFO:root:Training: Train Iter(587000)\tEnv Step(587200)\tLoss(-25.230)\n","INFO:root:Training: Train Iter(587100)\tEnv Step(587328)\tLoss(-26.930)\n","INFO:root:Training: Train Iter(587200)\tEnv Step(587456)\tLoss(-25.988)\n","INFO:root:Training: Train Iter(587300)\tEnv Step(587520)\tLoss(-24.848)\n","INFO:root:Training: Train Iter(587400)\tEnv Step(587648)\tLoss(-26.770)\n","INFO:root:Training: Train Iter(587500)\tEnv Step(587712)\tLoss(-21.413)\n","INFO:root:Training: Train Iter(587600)\tEnv Step(587840)\tLoss(-26.667)\n","INFO:root:Training: Train Iter(587700)\tEnv Step(587904)\tLoss(-22.419)\n","INFO:root:Evaluation: Train Iter(587776) Env Step(587968) Episode Return(313.113) \n","INFO:root:Training: Train Iter(587800)\tEnv Step(588032)\tLoss(-23.629)\n","INFO:root:Training: Train Iter(587900)\tEnv Step(588096)\tLoss(-22.593)\n","INFO:root:Training: Train Iter(588000)\tEnv Step(588224)\tLoss(-23.586)\n","INFO:root:Training: Train Iter(588100)\tEnv Step(588352)\tLoss(-23.879)\n","INFO:root:Training: Train Iter(588200)\tEnv Step(588416)\tLoss(-23.154)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73567\n","train_sample_count: 1765608\n","avg_envstep_per_episode: 73567.0\n","avg_sample_per_episode: 1765608.0\n","avg_envstep_per_sec: 8785.003536888773\n","avg_train_sample_per_sec: 210840.08488533055\n","avg_episode_per_sec: 0.11941500315207597\n","reward_mean: 306.7801208496094\n","reward_std: 0.0\n","reward_max: 306.7801208496094\n","reward_min: 306.7801208496094\n","total_envstep_count: 588536\n","total_train_sample_count: 14124864\n","total_episode_count: 622\n","INFO:root:Training: Train Iter(588300)\tEnv Step(588544)\tLoss(-27.204)\n","INFO:root:Training: Train Iter(588400)\tEnv Step(588608)\tLoss(-23.887)\n","INFO:root:Training: Train Iter(588500)\tEnv Step(588736)\tLoss(-19.940)\n","INFO:root:Training: Train Iter(588600)\tEnv Step(588800)\tLoss(-20.124)\n","INFO:root:Training: Train Iter(588700)\tEnv Step(588928)\tLoss(-22.154)\n","INFO:root:Evaluation: Train Iter(588800) Env Step(588992) Episode Return(308.229) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73630\n","train_sample_count: 1767120\n","avg_envstep_per_episode: 73630.0\n","avg_sample_per_episode: 1767120.0\n","avg_envstep_per_sec: 8017.1252737997975\n","avg_train_sample_per_sec: 192411.00657119515\n","avg_episode_per_sec: 0.1088839504794214\n","reward_mean: 307.49212646484375\n","reward_std: 0.0\n","reward_max: 307.49212646484375\n","reward_min: 307.49212646484375\n","total_envstep_count: 589040\n","total_train_sample_count: 14136960\n","total_episode_count: 623\n","INFO:root:Training: Train Iter(588800)\tEnv Step(589056)\tLoss(-24.115)\n","INFO:root:Training: Train Iter(588900)\tEnv Step(589120)\tLoss(-13.399)\n","INFO:root:Training: Train Iter(589000)\tEnv Step(589248)\tLoss(-17.035)\n","INFO:root:Training: Train Iter(589100)\tEnv Step(589312)\tLoss(-26.182)\n","INFO:root:Training: Train Iter(589200)\tEnv Step(589440)\tLoss(-23.226)\n","INFO:root:Training: Train Iter(589300)\tEnv Step(589504)\tLoss(-22.636)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73690\n","train_sample_count: 1768560\n","avg_envstep_per_episode: 73690.0\n","avg_sample_per_episode: 1768560.0\n","avg_envstep_per_sec: 8697.712065657113\n","avg_train_sample_per_sec: 208745.08957577072\n","avg_episode_per_sec: 0.11803110416144814\n","reward_mean: 302.2707214355469\n","reward_std: 0.0\n","reward_max: 302.2707214355469\n","reward_min: 302.2707214355469\n","total_envstep_count: 589520\n","total_train_sample_count: 14148480\n","total_episode_count: 624\n","INFO:root:Training: Train Iter(589400)\tEnv Step(589632)\tLoss(-23.388)\n","INFO:root:Training: Train Iter(589500)\tEnv Step(589696)\tLoss(-19.645)\n","INFO:root:Training: Train Iter(589600)\tEnv Step(589824)\tLoss(-20.491)\n","INFO:root:Training: Train Iter(589700)\tEnv Step(589952)\tLoss(-24.664)\n","INFO:root:Training: Train Iter(589800)\tEnv Step(590016)\tLoss(-25.431)\n","INFO:root:Evaluation: Train Iter(589824) Env Step(590016) Episode Return(307.658) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73764\n","train_sample_count: 1770336\n","avg_envstep_per_episode: 73764.0\n","avg_sample_per_episode: 1770336.0\n","avg_envstep_per_sec: 8516.943068990759\n","avg_train_sample_per_sec: 204406.6336557782\n","avg_episode_per_sec: 0.11546205559610051\n","reward_mean: 305.422119140625\n","reward_std: 0.0\n","reward_max: 305.422119140625\n","reward_min: 305.422119140625\n","total_envstep_count: 590112\n","total_train_sample_count: 14162688\n","total_episode_count: 625\n","INFO:root:Training: Train Iter(589900)\tEnv Step(590144)\tLoss(-20.517)\n","INFO:root:Training: Train Iter(590000)\tEnv Step(590208)\tLoss(-22.748)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 73773\n","train_sample_count: 1770552\n","avg_envstep_per_episode: 73773.0\n","avg_sample_per_episode: 1770552.0\n","avg_envstep_per_sec: 8870.594683778967\n","avg_train_sample_per_sec: 212894.27241069518\n","avg_episode_per_sec: 0.12024175082725341\n","reward_mean: 304.7640686035156\n","reward_std: 0.0\n","reward_max: 304.7640686035156\n","reward_min: 304.7640686035156\n","total_envstep_count: 590216\n","total_train_sample_count: 14165184\n","total_episode_count: 626\n","INFO:root:Training: Train Iter(590100)\tEnv Step(590336)\tLoss(-14.493)\n","INFO:root:Training: Train Iter(590200)\tEnv Step(590400)\tLoss(-25.860)\n","INFO:root:Training: Train Iter(590300)\tEnv Step(590528)\tLoss(-23.255)\n","INFO:root:Training: Train Iter(590400)\tEnv Step(590656)\tLoss(-25.123)\n","INFO:root:Training: Train Iter(590500)\tEnv Step(590720)\tLoss(-19.818)\n","INFO:root:Training: Train Iter(590600)\tEnv Step(590848)\tLoss(-25.281)\n","INFO:root:Training: Train Iter(590700)\tEnv Step(590912)\tLoss(-24.457)\n","INFO:root:Training: Train Iter(590800)\tEnv Step(591040)\tLoss(-24.274)\n","INFO:root:Evaluation: Train Iter(590848) Env Step(591040) Episode Return(312.913) \n","INFO:root:Training: Train Iter(590900)\tEnv Step(591104)\tLoss(-26.156)\n","INFO:root:Training: Train Iter(591000)\tEnv Step(591232)\tLoss(-24.109)\n","INFO:root:Training: Train Iter(591100)\tEnv Step(591296)\tLoss(-23.872)\n","INFO:root:Training: Train Iter(591200)\tEnv Step(591424)\tLoss(-26.664)\n","INFO:root:Training: Train Iter(591300)\tEnv Step(591552)\tLoss(-25.483)\n","INFO:root:Training: Train Iter(591400)\tEnv Step(591616)\tLoss(-22.635)\n","INFO:root:Training: Train Iter(591500)\tEnv Step(591744)\tLoss(-23.526)\n","INFO:root:Training: Train Iter(591600)\tEnv Step(591808)\tLoss(-26.095)\n","INFO:root:Training: Train Iter(591700)\tEnv Step(591936)\tLoss(-24.144)\n","INFO:root:Training: Train Iter(591800)\tEnv Step(592000)\tLoss(-22.693)\n","INFO:root:Evaluation: Train Iter(591872) Env Step(592064) Episode Return(310.283) \n","INFO:root:Training: Train Iter(591900)\tEnv Step(592128)\tLoss(-14.733)\n","INFO:root:Training: Train Iter(592000)\tEnv Step(592256)\tLoss(-24.920)\n","INFO:root:Training: Train Iter(592100)\tEnv Step(592320)\tLoss(-24.940)\n","INFO:root:Training: Train Iter(592200)\tEnv Step(592448)\tLoss(-28.789)\n","INFO:root:Training: Train Iter(592300)\tEnv Step(592512)\tLoss(-26.081)\n","INFO:root:Training: Train Iter(592400)\tEnv Step(592640)\tLoss(-21.024)\n","INFO:root:Training: Train Iter(592500)\tEnv Step(592704)\tLoss(-24.176)\n","INFO:root:Training: Train Iter(592600)\tEnv Step(592832)\tLoss(-26.020)\n","INFO:root:Training: Train Iter(592700)\tEnv Step(592896)\tLoss(-24.374)\n","INFO:root:Training: Train Iter(592800)\tEnv Step(593024)\tLoss(-17.891)\n","INFO:root:Evaluation: Train Iter(592896) Env Step(593088) Episode Return(311.663) \n","INFO:root:Training: Train Iter(592900)\tEnv Step(593152)\tLoss(-26.348)\n","INFO:root:Training: Train Iter(593000)\tEnv Step(593216)\tLoss(-26.657)\n","INFO:root:Training: Train Iter(593100)\tEnv Step(593344)\tLoss(-12.129)\n","INFO:root:Training: Train Iter(593200)\tEnv Step(593408)\tLoss(-22.586)\n","INFO:root:Training: Train Iter(593300)\tEnv Step(593536)\tLoss(-26.990)\n","INFO:root:Training: Train Iter(593400)\tEnv Step(593600)\tLoss(-24.007)\n","INFO:root:Training: Train Iter(593500)\tEnv Step(593728)\tLoss(-24.374)\n","INFO:root:Training: Train Iter(593600)\tEnv Step(593856)\tLoss(-24.533)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74233\n","train_sample_count: 1781592\n","avg_envstep_per_episode: 74233.0\n","avg_sample_per_episode: 1781592.0\n","avg_envstep_per_sec: 8729.796181466245\n","avg_train_sample_per_sec: 209515.1083551899\n","avg_episode_per_sec: 0.11759993778328029\n","reward_mean: 307.9222106933594\n","reward_std: 0.0\n","reward_max: 307.9222106933594\n","reward_min: 307.9222106933594\n","total_envstep_count: 593864\n","total_train_sample_count: 14252736\n","total_episode_count: 627\n","INFO:root:Training: Train Iter(593700)\tEnv Step(593920)\tLoss(-18.054)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74234\n","train_sample_count: 1781616\n","avg_envstep_per_episode: 74234.0\n","avg_sample_per_episode: 1781616.0\n","avg_envstep_per_sec: 8722.91261445035\n","avg_train_sample_per_sec: 209349.9027468084\n","avg_episode_per_sec: 0.11750562564930288\n","reward_mean: 305.6587829589844\n","reward_std: 0.0\n","reward_max: 305.6587829589844\n","reward_min: 305.6587829589844\n","total_envstep_count: 593992\n","total_train_sample_count: 14255808\n","total_episode_count: 628\n","INFO:root:Training: Train Iter(593800)\tEnv Step(594048)\tLoss(-18.263)\n","INFO:root:Training: Train Iter(593900)\tEnv Step(594112)\tLoss(-25.307)\n","INFO:root:Evaluation: Train Iter(593920) Env Step(594112) Episode Return(308.585) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74279\n","train_sample_count: 1782696\n","avg_envstep_per_episode: 74279.0\n","avg_sample_per_episode: 1782696.0\n","avg_envstep_per_sec: 6119.904259126637\n","avg_train_sample_per_sec: 146877.7022190393\n","avg_episode_per_sec: 0.0823907734235334\n","reward_mean: 307.0151672363281\n","reward_std: 0.0\n","reward_max: 307.0151672363281\n","reward_min: 307.0151672363281\n","total_envstep_count: 594232\n","total_train_sample_count: 14261568\n","total_episode_count: 629\n","INFO:root:Training: Train Iter(594000)\tEnv Step(594240)\tLoss(-19.988)\n","INFO:root:Training: Train Iter(594100)\tEnv Step(594304)\tLoss(-26.641)\n","INFO:root:Training: Train Iter(594200)\tEnv Step(594432)\tLoss(-27.466)\n","INFO:root:Training: Train Iter(594300)\tEnv Step(594496)\tLoss(-20.116)\n","INFO:root:Training: Train Iter(594400)\tEnv Step(594624)\tLoss(-22.561)\n","INFO:root:Training: Train Iter(594500)\tEnv Step(594752)\tLoss(-23.506)\n","INFO:root:Training: Train Iter(594600)\tEnv Step(594816)\tLoss(-24.979)\n","INFO:root:Training: Train Iter(594700)\tEnv Step(594944)\tLoss(-22.581)\n","INFO:root:Training: Train Iter(594800)\tEnv Step(595008)\tLoss(-23.398)\n","INFO:root:Training: Train Iter(594900)\tEnv Step(595136)\tLoss(-23.392)\n","INFO:root:Evaluation: Train Iter(594944) Env Step(595136) Episode Return(309.419) \n","INFO:root:Training: Train Iter(595000)\tEnv Step(595200)\tLoss(-24.733)\n","INFO:root:Training: Train Iter(595100)\tEnv Step(595328)\tLoss(-26.447)\n","INFO:root:Training: Train Iter(595200)\tEnv Step(595456)\tLoss(-24.809)\n","INFO:root:Training: Train Iter(595300)\tEnv Step(595520)\tLoss(-22.664)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74453\n","train_sample_count: 1786872\n","avg_envstep_per_episode: 74453.0\n","avg_sample_per_episode: 1786872.0\n","avg_envstep_per_sec: 8783.174681503371\n","avg_train_sample_per_sec: 210796.19235608092\n","avg_episode_per_sec: 0.11796938580719879\n","reward_mean: 306.4301452636719\n","reward_std: 0.0\n","reward_max: 306.4301452636719\n","reward_min: 306.4301452636719\n","total_envstep_count: 595624\n","total_train_sample_count: 14294976\n","total_episode_count: 630\n","INFO:root:Training: Train Iter(595400)\tEnv Step(595648)\tLoss(-22.889)\n","INFO:root:Training: Train Iter(595500)\tEnv Step(595712)\tLoss(-21.097)\n","INFO:root:Training: Train Iter(595600)\tEnv Step(595840)\tLoss(-20.090)\n","INFO:root:Training: Train Iter(595700)\tEnv Step(595904)\tLoss(-24.626)\n","INFO:root:Training: Train Iter(595800)\tEnv Step(596032)\tLoss(-26.427)\n","INFO:root:Training: Train Iter(595900)\tEnv Step(596096)\tLoss(-23.953)\n","INFO:root:Evaluation: Train Iter(595968) Env Step(596160) Episode Return(311.234) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74527\n","train_sample_count: 1788648\n","avg_envstep_per_episode: 74527.0\n","avg_sample_per_episode: 1788648.0\n","avg_envstep_per_sec: 8018.481904543897\n","avg_train_sample_per_sec: 192443.56570905354\n","avg_episode_per_sec: 0.10759163664905198\n","reward_mean: 308.0817565917969\n","reward_std: 0.0\n","reward_max: 308.0817565917969\n","reward_min: 308.0817565917969\n","total_envstep_count: 596216\n","total_train_sample_count: 14309184\n","total_episode_count: 631\n","INFO:root:Training: Train Iter(596000)\tEnv Step(596224)\tLoss(-21.059)\n","INFO:root:Training: Train Iter(596100)\tEnv Step(596352)\tLoss(-24.222)\n","INFO:root:Training: Train Iter(596200)\tEnv Step(596416)\tLoss(-27.037)\n","INFO:root:Training: Train Iter(596300)\tEnv Step(596544)\tLoss(-17.415)\n","INFO:root:Training: Train Iter(596400)\tEnv Step(596608)\tLoss(-26.953)\n","INFO:root:Training: Train Iter(596500)\tEnv Step(596736)\tLoss(-23.032)\n","INFO:root:Training: Train Iter(596600)\tEnv Step(596800)\tLoss(-25.107)\n","INFO:root:Training: Train Iter(596700)\tEnv Step(596928)\tLoss(-18.774)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74629\n","train_sample_count: 1791096\n","avg_envstep_per_episode: 74629.0\n","avg_sample_per_episode: 1791096.0\n","avg_envstep_per_sec: 8694.155468381012\n","avg_train_sample_per_sec: 208659.73124114427\n","avg_episode_per_sec: 0.11649835142345484\n","reward_mean: 305.6444091796875\n","reward_std: 0.0\n","reward_max: 305.6444091796875\n","reward_min: 305.6444091796875\n","total_envstep_count: 597032\n","total_train_sample_count: 14328768\n","total_episode_count: 632\n","INFO:root:Training: Train Iter(596800)\tEnv Step(597056)\tLoss(-24.662)\n","INFO:root:Training: Train Iter(596900)\tEnv Step(597120)\tLoss(-18.937)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74641\n","train_sample_count: 1791384\n","avg_envstep_per_episode: 74641.0\n","avg_sample_per_episode: 1791384.0\n","avg_envstep_per_sec: 8518.371059785495\n","avg_train_sample_per_sec: 204440.90543485188\n","avg_episode_per_sec: 0.11412455701002794\n","reward_mean: 308.3924255371094\n","reward_std: 0.0\n","reward_max: 308.3924255371094\n","reward_min: 308.3924255371094\n","total_envstep_count: 597128\n","total_train_sample_count: 14331072\n","total_episode_count: 633\n","INFO:root:Evaluation: Train Iter(596992) Env Step(597184) Episode Return(132.793) \n","INFO:root:Training: Train Iter(597000)\tEnv Step(597248)\tLoss(-24.016)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74664\n","train_sample_count: 1791936\n","avg_envstep_per_episode: 74664.0\n","avg_sample_per_episode: 1791936.0\n","avg_envstep_per_sec: 8872.883381743535\n","avg_train_sample_per_sec: 212949.20116184483\n","avg_episode_per_sec: 0.11883750377348568\n","reward_mean: 308.7048645019531\n","reward_std: 0.0\n","reward_max: 308.7048645019531\n","reward_min: 308.7048645019531\n","total_envstep_count: 597312\n","total_train_sample_count: 14335488\n","total_episode_count: 634\n","INFO:root:Training: Train Iter(597100)\tEnv Step(597312)\tLoss(-23.275)\n","INFO:root:Training: Train Iter(597200)\tEnv Step(597440)\tLoss(-25.032)\n","INFO:root:Training: Train Iter(597300)\tEnv Step(597504)\tLoss(-24.733)\n","INFO:root:Training: Train Iter(597400)\tEnv Step(597632)\tLoss(-26.702)\n","INFO:root:Training: Train Iter(597500)\tEnv Step(597696)\tLoss(-20.010)\n","INFO:root:Training: Train Iter(597600)\tEnv Step(597824)\tLoss(-25.598)\n","INFO:root:Training: Train Iter(597700)\tEnv Step(597952)\tLoss(-26.941)\n","INFO:root:Training: Train Iter(597800)\tEnv Step(598016)\tLoss(-24.348)\n","INFO:root:Training: Train Iter(597900)\tEnv Step(598144)\tLoss(-25.165)\n","INFO:root:Training: Train Iter(598000)\tEnv Step(598208)\tLoss(-25.018)\n","INFO:root:Evaluation: Train Iter(598016) Env Step(598208) Episode Return(308.839) \n","INFO:root:Training: Train Iter(598100)\tEnv Step(598336)\tLoss(-23.719)\n","INFO:root:Training: Train Iter(598200)\tEnv Step(598400)\tLoss(-25.190)\n","INFO:root:Training: Train Iter(598300)\tEnv Step(598528)\tLoss(-22.085)\n","INFO:root:Training: Train Iter(598400)\tEnv Step(598656)\tLoss(-24.987)\n","INFO:root:Training: Train Iter(598500)\tEnv Step(598720)\tLoss(-26.171)\n","INFO:root:Training: Train Iter(598600)\tEnv Step(598848)\tLoss(-25.787)\n","INFO:root:Training: Train Iter(598700)\tEnv Step(598912)\tLoss(-20.143)\n","INFO:root:Training: Train Iter(598800)\tEnv Step(599040)\tLoss(-25.793)\n","INFO:root:Training: Train Iter(598900)\tEnv Step(599104)\tLoss(-26.875)\n","INFO:root:Training: Train Iter(599000)\tEnv Step(599232)\tLoss(-21.213)\n","INFO:root:Evaluation: Train Iter(599040) Env Step(599232) Episode Return(308.359) \n","INFO:root:Training: Train Iter(599100)\tEnv Step(599296)\tLoss(-15.871)\n","INFO:root:Training: Train Iter(599200)\tEnv Step(599424)\tLoss(-27.527)\n","INFO:root:Training: Train Iter(599300)\tEnv Step(599552)\tLoss(-26.874)\n","INFO:root:Training: Train Iter(599400)\tEnv Step(599616)\tLoss(-24.158)\n","INFO:root:Training: Train Iter(599500)\tEnv Step(599744)\tLoss(-20.238)\n","INFO:root:Training: Train Iter(599600)\tEnv Step(599808)\tLoss(-24.277)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 74991\n","train_sample_count: 1799784\n","avg_envstep_per_episode: 74991.0\n","avg_sample_per_episode: 1799784.0\n","avg_envstep_per_sec: 8872.060090360646\n","avg_train_sample_per_sec: 212929.4421686555\n","avg_episode_per_sec: 0.11830833153792651\n","reward_mean: -149.227783203125\n","reward_std: 0.0\n","reward_max: -149.227783203125\n","reward_min: -149.227783203125\n","total_envstep_count: 599928\n","total_train_sample_count: 14398272\n","total_episode_count: 635\n","INFO:root:Training: Train Iter(599700)\tEnv Step(599936)\tLoss(-23.196)\n","INFO:root:Training: Train Iter(599800)\tEnv Step(600000)\tLoss(-25.821)\n","INFO:root:Training: Train Iter(599900)\tEnv Step(600128)\tLoss(-26.337)\n","INFO:root:Training: Train Iter(600000)\tEnv Step(600256)\tLoss(-23.660)\n","INFO:root:Evaluation: Train Iter(600064) Env Step(600256) Episode Return(223.062) \n","INFO:root:Training: Train Iter(600100)\tEnv Step(600320)\tLoss(-19.175)\n","INFO:root:Training: Train Iter(600200)\tEnv Step(600448)\tLoss(-24.560)\n","INFO:root:Training: Train Iter(600300)\tEnv Step(600512)\tLoss(-27.741)\n","INFO:root:Training: Train Iter(600400)\tEnv Step(600640)\tLoss(-20.702)\n","INFO:root:Training: Train Iter(600500)\tEnv Step(600704)\tLoss(-25.074)\n","INFO:root:Training: Train Iter(600600)\tEnv Step(600832)\tLoss(-24.451)\n","INFO:root:Training: Train Iter(600700)\tEnv Step(600896)\tLoss(-25.078)\n","INFO:root:Training: Train Iter(600800)\tEnv Step(601024)\tLoss(-18.321)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75134\n","train_sample_count: 1803216\n","avg_envstep_per_episode: 75134.0\n","avg_sample_per_episode: 1803216.0\n","avg_envstep_per_sec: 8721.260825848845\n","avg_train_sample_per_sec: 209310.25982037227\n","avg_episode_per_sec: 0.11607608840004319\n","reward_mean: 307.556640625\n","reward_std: 0.0\n","reward_max: 307.556640625\n","reward_min: 307.556640625\n","total_envstep_count: 601072\n","total_train_sample_count: 14425728\n","total_episode_count: 636\n","INFO:root:Training: Train Iter(600900)\tEnv Step(601152)\tLoss(-24.698)\n","INFO:root:Training: Train Iter(601000)\tEnv Step(601216)\tLoss(-24.265)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75156\n","train_sample_count: 1803744\n","avg_envstep_per_episode: 75156.0\n","avg_sample_per_episode: 1803744.0\n","avg_envstep_per_sec: 8723.336423541734\n","avg_train_sample_per_sec: 209360.07416500163\n","avg_episode_per_sec: 0.11606972728114501\n","reward_mean: 306.9759521484375\n","reward_std: 0.0\n","reward_max: 306.9759521484375\n","reward_min: 306.9759521484375\n","total_envstep_count: 601248\n","total_train_sample_count: 14429952\n","total_episode_count: 637\n","INFO:root:Evaluation: Train Iter(601088) Env Step(601280) Episode Return(222.299) \n","INFO:root:Training: Train Iter(601100)\tEnv Step(601344)\tLoss(-20.954)\n","INFO:root:Training: Train Iter(601200)\tEnv Step(601408)\tLoss(-24.985)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75185\n","train_sample_count: 1804440\n","avg_envstep_per_episode: 75185.0\n","avg_sample_per_episode: 1804440.0\n","avg_envstep_per_sec: 6121.958157053834\n","avg_train_sample_per_sec: 146926.99576929203\n","avg_episode_per_sec: 0.08142525978657757\n","reward_mean: 306.7758483886719\n","reward_std: 0.0\n","reward_max: 306.7758483886719\n","reward_min: 306.7758483886719\n","total_envstep_count: 601480\n","total_train_sample_count: 14435520\n","total_episode_count: 638\n","INFO:root:Training: Train Iter(601300)\tEnv Step(601536)\tLoss(-16.314)\n","INFO:root:Training: Train Iter(601400)\tEnv Step(601600)\tLoss(-16.882)\n","INFO:root:Training: Train Iter(601500)\tEnv Step(601728)\tLoss(-23.028)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75226\n","train_sample_count: 1805424\n","avg_envstep_per_episode: 75226.0\n","avg_sample_per_episode: 1805424.0\n","avg_envstep_per_sec: 8722.950161995814\n","avg_train_sample_per_sec: 209350.80388789953\n","avg_episode_per_sec: 0.11595658631318712\n","reward_mean: -97.8112564086914\n","reward_std: 0.0\n","reward_max: -97.8112564086914\n","reward_min: -97.8112564086914\n","total_envstep_count: 601808\n","total_train_sample_count: 14443392\n","total_episode_count: 639\n","INFO:root:Training: Train Iter(601600)\tEnv Step(601856)\tLoss(-27.283)\n","INFO:root:Training: Train Iter(601700)\tEnv Step(601920)\tLoss(-24.885)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75232\n","train_sample_count: 1805568\n","avg_envstep_per_episode: 75232.0\n","avg_sample_per_episode: 1805568.0\n","avg_envstep_per_sec: 8720.350565309162\n","avg_train_sample_per_sec: 209288.41356741986\n","avg_episode_per_sec: 0.11591278399230595\n","reward_mean: -98.41551971435547\n","reward_std: 0.0\n","reward_max: -98.41551971435547\n","reward_min: -98.41551971435547\n","total_envstep_count: 601928\n","total_train_sample_count: 14446272\n","total_episode_count: 640\n","INFO:root:Training: Train Iter(601800)\tEnv Step(602048)\tLoss(-24.168)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75258\n","train_sample_count: 1806192\n","avg_envstep_per_episode: 75258.0\n","avg_sample_per_episode: 1806192.0\n","avg_envstep_per_sec: 6122.011579458782\n","avg_train_sample_per_sec: 146928.27790701075\n","avg_episode_per_sec: 0.08134698742271627\n","reward_mean: -95.8450698852539\n","reward_std: 0.0\n","reward_max: -95.8450698852539\n","reward_min: -95.8450698852539\n","total_envstep_count: 602064\n","total_train_sample_count: 14449536\n","total_episode_count: 641\n","INFO:root:Training: Train Iter(601900)\tEnv Step(602112)\tLoss(-21.262)\n","INFO:root:Training: Train Iter(602000)\tEnv Step(602240)\tLoss(-25.211)\n","INFO:root:Training: Train Iter(602100)\tEnv Step(602304)\tLoss(-18.660)\n","INFO:root:Evaluation: Train Iter(602112) Env Step(602304) Episode Return(209.824) \n","INFO:root:Training: Train Iter(602200)\tEnv Step(602432)\tLoss(-25.925)\n","INFO:root:Training: Train Iter(602300)\tEnv Step(602496)\tLoss(-23.614)\n","INFO:root:Training: Train Iter(602400)\tEnv Step(602624)\tLoss(-23.600)\n","INFO:root:Training: Train Iter(602500)\tEnv Step(602752)\tLoss(-17.652)\n","INFO:root:Training: Train Iter(602600)\tEnv Step(602816)\tLoss(-18.603)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75360\n","train_sample_count: 1808640\n","avg_envstep_per_episode: 75360.0\n","avg_sample_per_episode: 1808640.0\n","avg_envstep_per_sec: 8781.029470268963\n","avg_train_sample_per_sec: 210744.70728645512\n","avg_episode_per_sec: 0.11652109169677498\n","reward_mean: 307.6532897949219\n","reward_std: 0.0\n","reward_max: 307.6532897949219\n","reward_min: 307.6532897949219\n","total_envstep_count: 602880\n","total_train_sample_count: 14469120\n","total_episode_count: 642\n","INFO:root:Training: Train Iter(602700)\tEnv Step(602944)\tLoss(-23.668)\n","INFO:root:Training: Train Iter(602800)\tEnv Step(603008)\tLoss(-22.678)\n","INFO:root:Training: Train Iter(602900)\tEnv Step(603136)\tLoss(-18.552)\n","INFO:root:Training: Train Iter(603000)\tEnv Step(603200)\tLoss(-18.671)\n","INFO:root:Training: Train Iter(603100)\tEnv Step(603328)\tLoss(-26.248)\n","INFO:root:Evaluation: Train Iter(603136) Env Step(603328) Episode Return(306.430) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75417\n","train_sample_count: 1810008\n","avg_envstep_per_episode: 75417.0\n","avg_sample_per_episode: 1810008.0\n","avg_envstep_per_sec: 8016.3450750480815\n","avg_train_sample_per_sec: 192392.28180115396\n","avg_episode_per_sec: 0.1062936085371744\n","reward_mean: 307.337158203125\n","reward_std: 0.0\n","reward_max: 307.337158203125\n","reward_min: 307.337158203125\n","total_envstep_count: 603336\n","total_train_sample_count: 14480064\n","total_episode_count: 643\n","INFO:root:Training: Train Iter(603200)\tEnv Step(603456)\tLoss(-20.019)\n","INFO:root:Training: Train Iter(603300)\tEnv Step(603520)\tLoss(-23.287)\n","INFO:root:Training: Train Iter(603400)\tEnv Step(603648)\tLoss(-25.373)\n","INFO:root:Training: Train Iter(603500)\tEnv Step(603712)\tLoss(-21.457)\n","INFO:root:Training: Train Iter(603600)\tEnv Step(603840)\tLoss(-24.961)\n","INFO:root:Training: Train Iter(603700)\tEnv Step(603904)\tLoss(-24.393)\n","INFO:root:Training: Train Iter(603800)\tEnv Step(604032)\tLoss(-24.063)\n","INFO:root:Training: Train Iter(603900)\tEnv Step(604096)\tLoss(-23.248)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75513\n","train_sample_count: 1812312\n","avg_envstep_per_episode: 75513.0\n","avg_sample_per_episode: 1812312.0\n","avg_envstep_per_sec: 8691.532845918868\n","avg_train_sample_per_sec: 208596.78830205285\n","avg_episode_per_sec: 0.11509982183092803\n","reward_mean: 305.8268127441406\n","reward_std: 0.0\n","reward_max: 305.8268127441406\n","reward_min: 305.8268127441406\n","total_envstep_count: 604104\n","total_train_sample_count: 14498496\n","total_episode_count: 644\n","INFO:root:Training: Train Iter(604000)\tEnv Step(604224)\tLoss(-25.915)\n","INFO:root:Training: Train Iter(604100)\tEnv Step(604352)\tLoss(-26.706)\n","INFO:root:Evaluation: Train Iter(604160) Env Step(604352) Episode Return(311.296) \n","INFO:root:Training: Train Iter(604200)\tEnv Step(604416)\tLoss(-26.352)\n","INFO:root:Training: Train Iter(604300)\tEnv Step(604544)\tLoss(-25.301)\n","INFO:root:Training: Train Iter(604400)\tEnv Step(604608)\tLoss(-26.620)\n","INFO:root:Training: Train Iter(604500)\tEnv Step(604736)\tLoss(-19.286)\n","INFO:root:Training: Train Iter(604600)\tEnv Step(604800)\tLoss(-25.396)\n","INFO:root:Training: Train Iter(604700)\tEnv Step(604928)\tLoss(-20.738)\n","INFO:root:Training: Train Iter(604800)\tEnv Step(605056)\tLoss(-18.148)\n","INFO:root:Training: Train Iter(604900)\tEnv Step(605120)\tLoss(-24.166)\n","INFO:root:Training: Train Iter(605000)\tEnv Step(605248)\tLoss(-28.014)\n","INFO:root:Training: Train Iter(605100)\tEnv Step(605312)\tLoss(-25.075)\n","INFO:root:Evaluation: Train Iter(605184) Env Step(605376) Episode Return(312.264) \n","INFO:root:Training: Train Iter(605200)\tEnv Step(605440)\tLoss(-23.553)\n","INFO:root:Training: Train Iter(605300)\tEnv Step(605504)\tLoss(-24.321)\n","INFO:root:Training: Train Iter(605400)\tEnv Step(605632)\tLoss(-25.226)\n","INFO:root:Training: Train Iter(605500)\tEnv Step(605696)\tLoss(-25.220)\n","INFO:root:Training: Train Iter(605600)\tEnv Step(605824)\tLoss(-23.567)\n","INFO:root:Training: Train Iter(605700)\tEnv Step(605952)\tLoss(-25.531)\n","INFO:root:Training: Train Iter(605800)\tEnv Step(606016)\tLoss(-15.251)\n","INFO:root:Training: Train Iter(605900)\tEnv Step(606144)\tLoss(-24.976)\n","INFO:root:Training: Train Iter(606000)\tEnv Step(606208)\tLoss(-21.963)\n","INFO:root:Training: Train Iter(606100)\tEnv Step(606336)\tLoss(-8.593)\n","INFO:root:Training: Train Iter(606200)\tEnv Step(606400)\tLoss(-25.041)\n","INFO:root:Evaluation: Train Iter(606208) Env Step(606400) Episode Return(306.608) \n","INFO:root:Training: Train Iter(606300)\tEnv Step(606528)\tLoss(-21.979)\n","INFO:root:Training: Train Iter(606400)\tEnv Step(606656)\tLoss(-23.504)\n","INFO:root:Training: Train Iter(606500)\tEnv Step(606720)\tLoss(-24.969)\n","INFO:root:Training: Train Iter(606600)\tEnv Step(606848)\tLoss(-26.194)\n","INFO:root:Training: Train Iter(606700)\tEnv Step(606912)\tLoss(-26.401)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75874\n","train_sample_count: 1820976\n","avg_envstep_per_episode: 75874.0\n","avg_sample_per_episode: 1820976.0\n","avg_envstep_per_sec: 8517.249033333188\n","avg_train_sample_per_sec: 204413.97679999648\n","avg_episode_per_sec: 0.11225517348937959\n","reward_mean: 280.3374938964844\n","reward_std: 0.0\n","reward_max: 280.3374938964844\n","reward_min: 280.3374938964844\n","total_envstep_count: 606992\n","total_train_sample_count: 14567808\n","total_episode_count: 645\n","INFO:root:Training: Train Iter(606800)\tEnv Step(607040)\tLoss(-27.982)\n","INFO:root:Training: Train Iter(606900)\tEnv Step(607104)\tLoss(-26.786)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 75898\n","train_sample_count: 1821552\n","avg_envstep_per_episode: 75898.0\n","avg_sample_per_episode: 1821552.0\n","avg_envstep_per_sec: 8872.53231300236\n","avg_train_sample_per_sec: 212940.7755120567\n","avg_episode_per_sec: 0.11690073932122536\n","reward_mean: 307.0395202636719\n","reward_std: 0.0\n","reward_max: 307.0395202636719\n","reward_min: 307.0395202636719\n","total_envstep_count: 607184\n","total_train_sample_count: 14572416\n","total_episode_count: 646\n","INFO:root:Training: Train Iter(607000)\tEnv Step(607232)\tLoss(-22.966)\n","INFO:root:Training: Train Iter(607100)\tEnv Step(607296)\tLoss(-22.012)\n","INFO:root:Training: Train Iter(607200)\tEnv Step(607424)\tLoss(-24.393)\n","INFO:root:Evaluation: Train Iter(607232) Env Step(607424) Episode Return(308.395) \n","INFO:root:Training: Train Iter(607300)\tEnv Step(607552)\tLoss(-27.100)\n","INFO:root:Training: Train Iter(607400)\tEnv Step(607616)\tLoss(-24.352)\n","INFO:root:Training: Train Iter(607500)\tEnv Step(607744)\tLoss(-24.633)\n","INFO:root:Training: Train Iter(607600)\tEnv Step(607808)\tLoss(-22.618)\n","INFO:root:Training: Train Iter(607700)\tEnv Step(607936)\tLoss(-25.863)\n","INFO:root:Training: Train Iter(607800)\tEnv Step(608000)\tLoss(-19.836)\n","INFO:root:Training: Train Iter(607900)\tEnv Step(608128)\tLoss(-24.483)\n","INFO:root:Training: Train Iter(608000)\tEnv Step(608256)\tLoss(-25.056)\n","INFO:root:Training: Train Iter(608100)\tEnv Step(608320)\tLoss(-24.508)\n","INFO:root:Training: Train Iter(608200)\tEnv Step(608448)\tLoss(-26.261)\n","INFO:root:Evaluation: Train Iter(608256) Env Step(608448) Episode Return(310.436) \n","INFO:root:Training: Train Iter(608300)\tEnv Step(608512)\tLoss(-21.846)\n","INFO:root:Training: Train Iter(608400)\tEnv Step(608640)\tLoss(-22.400)\n","INFO:root:Training: Train Iter(608500)\tEnv Step(608704)\tLoss(-17.368)\n","INFO:root:Training: Train Iter(608600)\tEnv Step(608832)\tLoss(-26.651)\n","INFO:root:Training: Train Iter(608700)\tEnv Step(608896)\tLoss(-20.275)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76121\n","train_sample_count: 1826904\n","avg_envstep_per_episode: 76121.0\n","avg_sample_per_episode: 1826904.0\n","avg_envstep_per_sec: 8719.355450686426\n","avg_train_sample_per_sec: 209264.5308164742\n","avg_episode_per_sec: 0.1145459919166383\n","reward_mean: 307.9222717285156\n","reward_std: 0.0\n","reward_max: 307.9222717285156\n","reward_min: 307.9222717285156\n","total_envstep_count: 608968\n","total_train_sample_count: 14615232\n","total_episode_count: 647\n","INFO:root:Training: Train Iter(608800)\tEnv Step(609024)\tLoss(-22.157)\n","INFO:root:Training: Train Iter(608900)\tEnv Step(609152)\tLoss(-23.153)\n","INFO:root:Training: Train Iter(609000)\tEnv Step(609216)\tLoss(-20.768)\n","INFO:root:Training: Train Iter(609100)\tEnv Step(609344)\tLoss(-28.275)\n","INFO:root:Training: Train Iter(609200)\tEnv Step(609408)\tLoss(-27.224)\n","INFO:root:Evaluation: Train Iter(609280) Env Step(609472) Episode Return(314.072) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76189\n","train_sample_count: 1828536\n","avg_envstep_per_episode: 76189.0\n","avg_sample_per_episode: 1828536.0\n","avg_envstep_per_sec: 6123.573692536876\n","avg_train_sample_per_sec: 146965.768620885\n","avg_episode_per_sec: 0.08037346195037179\n","reward_mean: 305.37371826171875\n","reward_std: 0.0\n","reward_max: 305.37371826171875\n","reward_min: 305.37371826171875\n","total_envstep_count: 609512\n","total_train_sample_count: 14628288\n","total_episode_count: 648\n","INFO:root:Training: Train Iter(609300)\tEnv Step(609536)\tLoss(-27.509)\n","INFO:root:Training: Train Iter(609400)\tEnv Step(609600)\tLoss(-21.149)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76211\n","train_sample_count: 1829064\n","avg_envstep_per_episode: 76211.0\n","avg_sample_per_episode: 1829064.0\n","avg_envstep_per_sec: 8723.40399929761\n","avg_train_sample_per_sec: 209361.69598314265\n","avg_episode_per_sec: 0.11446384379285943\n","reward_mean: 301.0909423828125\n","reward_std: 0.0\n","reward_max: 301.0909423828125\n","reward_min: 301.0909423828125\n","total_envstep_count: 609688\n","total_train_sample_count: 14632512\n","total_episode_count: 649\n","INFO:root:Training: Train Iter(609500)\tEnv Step(609728)\tLoss(-25.506)\n","INFO:root:Training: Train Iter(609600)\tEnv Step(609856)\tLoss(-24.574)\n","INFO:root:Training: Train Iter(609700)\tEnv Step(609920)\tLoss(-26.803)\n","INFO:root:Training: Train Iter(609800)\tEnv Step(610048)\tLoss(-23.809)\n","INFO:root:Training: Train Iter(609900)\tEnv Step(610112)\tLoss(-27.135)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76272\n","train_sample_count: 1830528\n","avg_envstep_per_episode: 76272.0\n","avg_sample_per_episode: 1830528.0\n","avg_envstep_per_sec: 8782.368673187497\n","avg_train_sample_per_sec: 210776.84815649994\n","avg_episode_per_sec: 0.11514538327548114\n","reward_mean: 305.87298583984375\n","reward_std: 0.0\n","reward_max: 305.87298583984375\n","reward_min: 305.87298583984375\n","total_envstep_count: 610176\n","total_train_sample_count: 14644224\n","total_episode_count: 650\n","INFO:root:Training: Train Iter(610000)\tEnv Step(610240)\tLoss(-25.153)\n","INFO:root:Training: Train Iter(610100)\tEnv Step(610304)\tLoss(-24.628)\n","INFO:root:Training: Train Iter(610200)\tEnv Step(610432)\tLoss(-20.072)\n","INFO:root:Training: Train Iter(610300)\tEnv Step(610496)\tLoss(-22.865)\n","INFO:root:Evaluation: Train Iter(610304) Env Step(610496) Episode Return(309.321) \n","INFO:root:Training: Train Iter(610400)\tEnv Step(610624)\tLoss(-23.812)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76335\n","train_sample_count: 1832040\n","avg_envstep_per_episode: 76335.0\n","avg_sample_per_episode: 1832040.0\n","avg_envstep_per_sec: 8014.763864428673\n","avg_train_sample_per_sec: 192354.33274628816\n","avg_episode_per_sec: 0.10499461406207733\n","reward_mean: 306.3561706542969\n","reward_std: 0.0\n","reward_max: 306.3561706542969\n","reward_min: 306.3561706542969\n","total_envstep_count: 610680\n","total_train_sample_count: 14656320\n","total_episode_count: 651\n","INFO:root:Training: Train Iter(610500)\tEnv Step(610752)\tLoss(-20.284)\n","INFO:root:Training: Train Iter(610600)\tEnv Step(610816)\tLoss(-21.171)\n","INFO:root:Training: Train Iter(610700)\tEnv Step(610944)\tLoss(-21.844)\n","INFO:root:Training: Train Iter(610800)\tEnv Step(611008)\tLoss(-22.220)\n","INFO:root:Training: Train Iter(610900)\tEnv Step(611136)\tLoss(-20.350)\n","INFO:root:Training: Train Iter(611000)\tEnv Step(611200)\tLoss(-16.540)\n","INFO:root:Training: Train Iter(611100)\tEnv Step(611328)\tLoss(-15.915)\n","INFO:root:Training: Train Iter(611200)\tEnv Step(611456)\tLoss(-24.334)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76435\n","train_sample_count: 1834440\n","avg_envstep_per_episode: 76435.0\n","avg_sample_per_episode: 1834440.0\n","avg_envstep_per_sec: 8688.893902939386\n","avg_train_sample_per_sec: 208533.45367054528\n","avg_episode_per_sec: 0.1136769006729821\n","reward_mean: 306.2865295410156\n","reward_std: 0.0\n","reward_max: 306.2865295410156\n","reward_min: 306.2865295410156\n","total_envstep_count: 611480\n","total_train_sample_count: 14675520\n","total_episode_count: 652\n","INFO:root:Training: Train Iter(611300)\tEnv Step(611520)\tLoss(-26.336)\n","INFO:root:Evaluation: Train Iter(611328) Env Step(611520) Episode Return(162.360) \n","INFO:root:Training: Train Iter(611400)\tEnv Step(611648)\tLoss(-18.993)\n","INFO:root:Training: Train Iter(611500)\tEnv Step(611712)\tLoss(-26.137)\n","INFO:root:Training: Train Iter(611600)\tEnv Step(611840)\tLoss(-26.039)\n","INFO:root:Training: Train Iter(611700)\tEnv Step(611904)\tLoss(-24.481)\n","INFO:root:Training: Train Iter(611800)\tEnv Step(612032)\tLoss(-27.991)\n","INFO:root:Training: Train Iter(611900)\tEnv Step(612096)\tLoss(-26.376)\n","INFO:root:Training: Train Iter(612000)\tEnv Step(612224)\tLoss(-23.650)\n","INFO:root:Training: Train Iter(612100)\tEnv Step(612352)\tLoss(-24.351)\n","INFO:root:Training: Train Iter(612200)\tEnv Step(612416)\tLoss(-22.024)\n","INFO:root:Training: Train Iter(612300)\tEnv Step(612544)\tLoss(-25.957)\n","INFO:root:Evaluation: Train Iter(612352) Env Step(612544) Episode Return(314.167) \n","INFO:root:Training: Train Iter(612400)\tEnv Step(612608)\tLoss(-22.704)\n","INFO:root:Training: Train Iter(612500)\tEnv Step(612736)\tLoss(-26.106)\n","INFO:root:Training: Train Iter(612600)\tEnv Step(612800)\tLoss(-17.822)\n","INFO:root:Training: Train Iter(612700)\tEnv Step(612928)\tLoss(-25.514)\n","INFO:root:Training: Train Iter(612800)\tEnv Step(613056)\tLoss(-25.568)\n","INFO:root:Training: Train Iter(612900)\tEnv Step(613120)\tLoss(-23.646)\n","INFO:root:Training: Train Iter(613000)\tEnv Step(613248)\tLoss(-22.862)\n","INFO:root:Training: Train Iter(613100)\tEnv Step(613312)\tLoss(-26.320)\n","INFO:root:Training: Train Iter(613200)\tEnv Step(613440)\tLoss(-22.116)\n","INFO:root:Training: Train Iter(613300)\tEnv Step(613504)\tLoss(-19.588)\n","INFO:root:Evaluation: Train Iter(613376) Env Step(613568) Episode Return(311.071) \n","INFO:root:Training: Train Iter(613400)\tEnv Step(613632)\tLoss(-26.314)\n","INFO:root:Training: Train Iter(613500)\tEnv Step(613696)\tLoss(-25.158)\n","INFO:root:Training: Train Iter(613600)\tEnv Step(613824)\tLoss(-25.338)\n","INFO:root:Training: Train Iter(613700)\tEnv Step(613952)\tLoss(-21.353)\n","INFO:root:Training: Train Iter(613800)\tEnv Step(614016)\tLoss(-25.152)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76762\n","train_sample_count: 1842288\n","avg_envstep_per_episode: 76762.0\n","avg_sample_per_episode: 1842288.0\n","avg_envstep_per_sec: 8517.834388605066\n","avg_train_sample_per_sec: 204428.02532652157\n","avg_episode_per_sec: 0.11096420609943808\n","reward_mean: 308.0093688964844\n","reward_std: 0.0\n","reward_max: 308.0093688964844\n","reward_min: 308.0093688964844\n","total_envstep_count: 614096\n","total_train_sample_count: 14738304\n","total_episode_count: 653\n","INFO:root:Training: Train Iter(613900)\tEnv Step(614144)\tLoss(-23.954)\n","INFO:root:Training: Train Iter(614000)\tEnv Step(614208)\tLoss(-22.230)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 76792\n","train_sample_count: 1843008\n","avg_envstep_per_episode: 76792.0\n","avg_sample_per_episode: 1843008.0\n","avg_envstep_per_sec: 8875.139410648659\n","avg_train_sample_per_sec: 213003.3458555678\n","avg_episode_per_sec: 0.11557375000844695\n","reward_mean: 308.4915771484375\n","reward_std: 0.0\n","reward_max: 308.4915771484375\n","reward_min: 308.4915771484375\n","total_envstep_count: 614336\n","total_train_sample_count: 14744064\n","total_episode_count: 654\n","INFO:root:Training: Train Iter(614100)\tEnv Step(614336)\tLoss(-18.741)\n","INFO:root:Training: Train Iter(614200)\tEnv Step(614400)\tLoss(-21.261)\n","INFO:root:Training: Train Iter(614300)\tEnv Step(614528)\tLoss(-25.905)\n","INFO:root:Evaluation: Train Iter(614400) Env Step(614592) Episode Return(307.753) \n","INFO:root:Training: Train Iter(614400)\tEnv Step(614656)\tLoss(-25.350)\n","INFO:root:Training: Train Iter(614500)\tEnv Step(614720)\tLoss(-23.288)\n","INFO:root:Training: Train Iter(614600)\tEnv Step(614848)\tLoss(-12.119)\n","INFO:root:Training: Train Iter(614700)\tEnv Step(614912)\tLoss(-22.765)\n","INFO:root:Training: Train Iter(614800)\tEnv Step(615040)\tLoss(-24.825)\n","INFO:root:Training: Train Iter(614900)\tEnv Step(615104)\tLoss(-25.472)\n","INFO:root:Training: Train Iter(615000)\tEnv Step(615232)\tLoss(-11.458)\n","INFO:root:Training: Train Iter(615100)\tEnv Step(615296)\tLoss(-19.468)\n","INFO:root:Training: Train Iter(615200)\tEnv Step(615424)\tLoss(-23.583)\n","INFO:root:Training: Train Iter(615300)\tEnv Step(615552)\tLoss(-20.129)\n","INFO:root:Training: Train Iter(615400)\tEnv Step(615616)\tLoss(-25.820)\n","INFO:root:Evaluation: Train Iter(615424) Env Step(615616) Episode Return(312.390) \n","INFO:root:Training: Train Iter(615500)\tEnv Step(615744)\tLoss(-21.294)\n","INFO:root:Training: Train Iter(615600)\tEnv Step(615808)\tLoss(-22.370)\n","INFO:root:Training: Train Iter(615700)\tEnv Step(615936)\tLoss(-23.902)\n","INFO:root:Training: Train Iter(615800)\tEnv Step(616000)\tLoss(-24.326)\n","INFO:root:Training: Train Iter(615900)\tEnv Step(616128)\tLoss(-26.203)\n","INFO:root:Training: Train Iter(616000)\tEnv Step(616256)\tLoss(-25.534)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77033\n","train_sample_count: 1848792\n","avg_envstep_per_episode: 77033.0\n","avg_sample_per_episode: 1848792.0\n","avg_envstep_per_sec: 8715.138618259492\n","avg_train_sample_per_sec: 209163.3268382278\n","avg_episode_per_sec: 0.11313513193383994\n","reward_mean: 306.45458984375\n","reward_std: 0.0\n","reward_max: 306.45458984375\n","reward_min: 306.45458984375\n","total_envstep_count: 616264\n","total_train_sample_count: 14790336\n","total_episode_count: 655\n","INFO:root:Training: Train Iter(616100)\tEnv Step(616320)\tLoss(-26.504)\n","INFO:root:Training: Train Iter(616200)\tEnv Step(616448)\tLoss(-23.770)\n","INFO:root:Training: Train Iter(616300)\tEnv Step(616512)\tLoss(-24.900)\n","INFO:root:Training: Train Iter(616400)\tEnv Step(616640)\tLoss(-19.022)\n","INFO:root:Evaluation: Train Iter(616448) Env Step(616640) Episode Return(310.235) \n","INFO:root:Training: Train Iter(616500)\tEnv Step(616704)\tLoss(-24.482)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77093\n","train_sample_count: 1850232\n","avg_envstep_per_episode: 77093.0\n","avg_sample_per_episode: 1850232.0\n","avg_envstep_per_sec: 6124.254034630365\n","avg_train_sample_per_sec: 146982.09683112876\n","avg_episode_per_sec: 0.07943981988806202\n","reward_mean: 308.0492858886719\n","reward_std: 0.0\n","reward_max: 308.0492858886719\n","reward_min: 308.0492858886719\n","total_envstep_count: 616744\n","total_train_sample_count: 14801856\n","total_episode_count: 656\n","INFO:root:Training: Train Iter(616600)\tEnv Step(616832)\tLoss(-22.338)\n","INFO:root:Training: Train Iter(616700)\tEnv Step(616896)\tLoss(-22.644)\n","INFO:root:Training: Train Iter(616800)\tEnv Step(617024)\tLoss(-23.977)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77134\n","train_sample_count: 1851216\n","avg_envstep_per_episode: 77134.0\n","avg_sample_per_episode: 1851216.0\n","avg_envstep_per_sec: 8724.402411881356\n","avg_train_sample_per_sec: 209385.65788515256\n","avg_episode_per_sec: 0.11310709170899158\n","reward_mean: 306.4252624511719\n","reward_std: 0.0\n","reward_max: 306.4252624511719\n","reward_min: 306.4252624511719\n","total_envstep_count: 617072\n","total_train_sample_count: 14809728\n","total_episode_count: 657\n","INFO:root:Training: Train Iter(616900)\tEnv Step(617152)\tLoss(-21.502)\n","INFO:root:Training: Train Iter(617000)\tEnv Step(617216)\tLoss(-24.442)\n","INFO:root:Training: Train Iter(617100)\tEnv Step(617344)\tLoss(-26.043)\n","INFO:root:Training: Train Iter(617200)\tEnv Step(617408)\tLoss(-25.309)\n","INFO:root:Training: Train Iter(617300)\tEnv Step(617536)\tLoss(-26.124)\n","INFO:root:Training: Train Iter(617400)\tEnv Step(617600)\tLoss(-20.914)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77207\n","train_sample_count: 1852968\n","avg_envstep_per_episode: 77207.0\n","avg_sample_per_episode: 1852968.0\n","avg_envstep_per_sec: 8014.621312397326\n","avg_train_sample_per_sec: 192350.91149753582\n","avg_episode_per_sec: 0.10380692569841239\n","reward_mean: 309.6535949707031\n","reward_std: 0.0\n","reward_max: 309.6535949707031\n","reward_min: 309.6535949707031\n","total_envstep_count: 617656\n","total_train_sample_count: 14823744\n","total_episode_count: 658\n","INFO:root:Evaluation: Train Iter(617472) Env Step(617664) Episode Return(315.055) \n","INFO:root:Training: Train Iter(617500)\tEnv Step(617728)\tLoss(-23.967)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77211\n","train_sample_count: 1853064\n","avg_envstep_per_episode: 77211.0\n","avg_sample_per_episode: 1853064.0\n","avg_envstep_per_sec: 8779.16674033735\n","avg_train_sample_per_sec: 210700.0017680964\n","avg_episode_per_sec: 0.1137035751426267\n","reward_mean: 305.0268859863281\n","reward_std: 0.0\n","reward_max: 305.0268859863281\n","reward_min: 305.0268859863281\n","total_envstep_count: 617736\n","total_train_sample_count: 14825664\n","total_episode_count: 659\n","INFO:root:Training: Train Iter(617600)\tEnv Step(617856)\tLoss(-23.460)\n","INFO:root:Training: Train Iter(617700)\tEnv Step(617920)\tLoss(-26.143)\n","INFO:root:Training: Train Iter(617800)\tEnv Step(618048)\tLoss(-28.071)\n","INFO:root:Training: Train Iter(617900)\tEnv Step(618112)\tLoss(-25.348)\n","INFO:root:Training: Train Iter(618000)\tEnv Step(618240)\tLoss(-26.278)\n","INFO:root:Training: Train Iter(618100)\tEnv Step(618304)\tLoss(-21.511)\n","INFO:root:Training: Train Iter(618200)\tEnv Step(618432)\tLoss(-21.909)\n","INFO:root:Training: Train Iter(618300)\tEnv Step(618496)\tLoss(-26.096)\n","INFO:root:Training: Train Iter(618400)\tEnv Step(618624)\tLoss(-26.325)\n","INFO:root:Evaluation: Train Iter(618496) Env Step(618688) Episode Return(310.271) \n","INFO:root:Training: Train Iter(618500)\tEnv Step(618752)\tLoss(-23.353)\n","INFO:root:Training: Train Iter(618600)\tEnv Step(618816)\tLoss(-25.651)\n","INFO:root:Training: Train Iter(618700)\tEnv Step(618944)\tLoss(-23.368)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77374\n","train_sample_count: 1856976\n","avg_envstep_per_episode: 77374.0\n","avg_sample_per_episode: 1856976.0\n","avg_envstep_per_sec: 8687.718021840996\n","avg_train_sample_per_sec: 208505.2325241839\n","avg_episode_per_sec: 0.11228213640035407\n","reward_mean: 305.77655029296875\n","reward_std: 0.0\n","reward_max: 305.77655029296875\n","reward_min: 305.77655029296875\n","total_envstep_count: 618992\n","total_train_sample_count: 14855808\n","total_episode_count: 660\n","INFO:root:Training: Train Iter(618800)\tEnv Step(619008)\tLoss(-20.763)\n","INFO:root:Training: Train Iter(618900)\tEnv Step(619136)\tLoss(-24.248)\n","INFO:root:Training: Train Iter(619000)\tEnv Step(619200)\tLoss(-24.633)\n","INFO:root:Training: Train Iter(619100)\tEnv Step(619328)\tLoss(-24.690)\n","INFO:root:Training: Train Iter(619200)\tEnv Step(619456)\tLoss(-21.129)\n","INFO:root:Training: Train Iter(619300)\tEnv Step(619520)\tLoss(-22.967)\n","INFO:root:Training: Train Iter(619400)\tEnv Step(619648)\tLoss(-23.275)\n","INFO:root:Training: Train Iter(619500)\tEnv Step(619712)\tLoss(-24.171)\n","INFO:root:Evaluation: Train Iter(619520) Env Step(619712) Episode Return(310.818) \n","INFO:root:Training: Train Iter(619600)\tEnv Step(619840)\tLoss(-18.814)\n","INFO:root:Training: Train Iter(619700)\tEnv Step(619904)\tLoss(-26.373)\n","INFO:root:Training: Train Iter(619800)\tEnv Step(620032)\tLoss(-25.660)\n","INFO:root:Training: Train Iter(619900)\tEnv Step(620096)\tLoss(-25.720)\n","INFO:root:Training: Train Iter(620000)\tEnv Step(620224)\tLoss(-24.384)\n","INFO:root:Training: Train Iter(620100)\tEnv Step(620352)\tLoss(-26.070)\n","INFO:root:Training: Train Iter(620200)\tEnv Step(620416)\tLoss(-23.394)\n","INFO:root:Training: Train Iter(620300)\tEnv Step(620544)\tLoss(-26.404)\n","INFO:root:Training: Train Iter(620400)\tEnv Step(620608)\tLoss(-24.939)\n","INFO:root:Training: Train Iter(620500)\tEnv Step(620736)\tLoss(-20.797)\n","INFO:root:Evaluation: Train Iter(620544) Env Step(620736) Episode Return(311.718) \n","INFO:root:Training: Train Iter(620600)\tEnv Step(620800)\tLoss(-21.221)\n","INFO:root:Training: Train Iter(620700)\tEnv Step(620928)\tLoss(-27.939)\n","INFO:root:Training: Train Iter(620800)\tEnv Step(621056)\tLoss(-26.177)\n","INFO:root:Training: Train Iter(620900)\tEnv Step(621120)\tLoss(-22.874)\n","INFO:root:Training: Train Iter(621000)\tEnv Step(621248)\tLoss(-24.364)\n","INFO:root:Training: Train Iter(621100)\tEnv Step(621312)\tLoss(-22.865)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77676\n","train_sample_count: 1864224\n","avg_envstep_per_episode: 77676.0\n","avg_sample_per_episode: 1864224.0\n","avg_envstep_per_sec: 8512.280432266005\n","avg_train_sample_per_sec: 204294.7303743841\n","avg_episode_per_sec: 0.10958700798529795\n","reward_mean: 307.24676513671875\n","reward_std: 0.0\n","reward_max: 307.24676513671875\n","reward_min: 307.24676513671875\n","total_envstep_count: 621408\n","total_train_sample_count: 14913792\n","total_episode_count: 661\n","INFO:root:Training: Train Iter(621200)\tEnv Step(621440)\tLoss(-24.319)\n","INFO:root:Training: Train Iter(621300)\tEnv Step(621504)\tLoss(-26.478)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77703\n","train_sample_count: 1864872\n","avg_envstep_per_episode: 77703.0\n","avg_sample_per_episode: 1864872.0\n","avg_envstep_per_sec: 8868.866338379827\n","avg_train_sample_per_sec: 212852.79212111584\n","avg_episode_per_sec: 0.11413801704412735\n","reward_mean: 307.40753173828125\n","reward_std: 0.0\n","reward_max: 307.40753173828125\n","reward_min: 307.40753173828125\n","total_envstep_count: 621624\n","total_train_sample_count: 14918976\n","total_episode_count: 662\n","INFO:root:Training: Train Iter(621400)\tEnv Step(621632)\tLoss(-27.323)\n","INFO:root:Training: Train Iter(621500)\tEnv Step(621696)\tLoss(-27.854)\n","INFO:root:Evaluation: Train Iter(621568) Env Step(621760) Episode Return(309.315) \n","INFO:root:Training: Train Iter(621600)\tEnv Step(621824)\tLoss(-25.047)\n","INFO:root:Training: Train Iter(621700)\tEnv Step(621952)\tLoss(-27.759)\n","INFO:root:Training: Train Iter(621800)\tEnv Step(622016)\tLoss(-25.689)\n","INFO:root:Training: Train Iter(621900)\tEnv Step(622144)\tLoss(-23.330)\n","INFO:root:Training: Train Iter(622000)\tEnv Step(622208)\tLoss(-22.696)\n","INFO:root:Training: Train Iter(622100)\tEnv Step(622336)\tLoss(-26.361)\n","INFO:root:Training: Train Iter(622200)\tEnv Step(622400)\tLoss(-23.718)\n","INFO:root:Training: Train Iter(622300)\tEnv Step(622528)\tLoss(-24.235)\n","INFO:root:Training: Train Iter(622400)\tEnv Step(622656)\tLoss(-25.462)\n","INFO:root:Training: Train Iter(622500)\tEnv Step(622720)\tLoss(-26.772)\n","INFO:root:Evaluation: Train Iter(622592) Env Step(622784) Episode Return(310.420) \n","INFO:root:Training: Train Iter(622600)\tEnv Step(622848)\tLoss(-20.146)\n","INFO:root:Training: Train Iter(622700)\tEnv Step(622912)\tLoss(-27.288)\n","INFO:root:Training: Train Iter(622800)\tEnv Step(623040)\tLoss(-26.068)\n","INFO:root:Training: Train Iter(622900)\tEnv Step(623104)\tLoss(-25.973)\n","INFO:root:Training: Train Iter(623000)\tEnv Step(623232)\tLoss(-20.773)\n","INFO:root:Training: Train Iter(623100)\tEnv Step(623296)\tLoss(-21.455)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77923\n","train_sample_count: 1870152\n","avg_envstep_per_episode: 77923.0\n","avg_sample_per_episode: 1870152.0\n","avg_envstep_per_sec: 8707.950759562904\n","avg_train_sample_per_sec: 208990.8182295097\n","avg_episode_per_sec: 0.11175071236429429\n","reward_mean: 308.9713134765625\n","reward_std: 0.0\n","reward_max: 308.9713134765625\n","reward_min: 308.9713134765625\n","total_envstep_count: 623384\n","total_train_sample_count: 14961216\n","total_episode_count: 663\n","INFO:root:Training: Train Iter(623200)\tEnv Step(623424)\tLoss(-14.587)\n","INFO:root:Training: Train Iter(623300)\tEnv Step(623552)\tLoss(-24.222)\n","INFO:root:Training: Train Iter(623400)\tEnv Step(623616)\tLoss(-24.517)\n","INFO:root:Training: Train Iter(623500)\tEnv Step(623744)\tLoss(-26.002)\n","INFO:root:Training: Train Iter(623600)\tEnv Step(623808)\tLoss(-25.980)\n","INFO:root:Evaluation: Train Iter(623616) Env Step(623808) Episode Return(309.762) \n","INFO:root:Training: Train Iter(623700)\tEnv Step(623936)\tLoss(-26.740)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 77995\n","train_sample_count: 1871880\n","avg_envstep_per_episode: 77995.0\n","avg_sample_per_episode: 1871880.0\n","avg_envstep_per_sec: 6115.698972967881\n","avg_train_sample_per_sec: 146776.77535122912\n","avg_episode_per_sec: 0.07841142346263069\n","reward_mean: 308.3876037597656\n","reward_std: 0.0\n","reward_max: 308.3876037597656\n","reward_min: 308.3876037597656\n","total_envstep_count: 623960\n","total_train_sample_count: 14975040\n","total_episode_count: 664\n","INFO:root:Training: Train Iter(623800)\tEnv Step(624000)\tLoss(-27.583)\n","INFO:root:Training: Train Iter(623900)\tEnv Step(624128)\tLoss(-25.661)\n","INFO:root:Training: Train Iter(624000)\tEnv Step(624256)\tLoss(-22.941)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78036\n","train_sample_count: 1872864\n","avg_envstep_per_episode: 78036.0\n","avg_sample_per_episode: 1872864.0\n","avg_envstep_per_sec: 8708.620181108206\n","avg_train_sample_per_sec: 209006.88434659695\n","avg_episode_per_sec: 0.11159747015618697\n","reward_mean: 308.113525390625\n","reward_std: 0.0\n","reward_max: 308.113525390625\n","reward_min: 308.113525390625\n","total_envstep_count: 624288\n","total_train_sample_count: 14982912\n","total_episode_count: 665\n","INFO:root:Training: Train Iter(624100)\tEnv Step(624320)\tLoss(-22.288)\n","INFO:root:Training: Train Iter(624200)\tEnv Step(624448)\tLoss(-25.562)\n","INFO:root:Training: Train Iter(624300)\tEnv Step(624512)\tLoss(-26.540)\n","INFO:root:Training: Train Iter(624400)\tEnv Step(624640)\tLoss(-25.803)\n","INFO:root:Training: Train Iter(624500)\tEnv Step(624704)\tLoss(-14.008)\n","INFO:root:Training: Train Iter(624600)\tEnv Step(624832)\tLoss(-27.041)\n","INFO:root:Evaluation: Train Iter(624640) Env Step(624832) Episode Return(309.226) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78110\n","train_sample_count: 1874640\n","avg_envstep_per_episode: 78110.0\n","avg_sample_per_episode: 1874640.0\n","avg_envstep_per_sec: 8005.066450151669\n","avg_train_sample_per_sec: 192121.59480364007\n","avg_episode_per_sec: 0.10248452759123888\n","reward_mean: 308.20159912109375\n","reward_std: 0.0\n","reward_max: 308.20159912109375\n","reward_min: 308.20159912109375\n","total_envstep_count: 624880\n","total_train_sample_count: 14997120\n","total_episode_count: 666\n","INFO:root:Training: Train Iter(624700)\tEnv Step(624896)\tLoss(-23.714)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78120\n","train_sample_count: 1874880\n","avg_envstep_per_episode: 78120.0\n","avg_sample_per_episode: 1874880.0\n","avg_envstep_per_sec: 8774.86640526759\n","avg_train_sample_per_sec: 210596.79372642218\n","avg_episode_per_sec: 0.11232547881806952\n","reward_mean: 308.558837890625\n","reward_std: 0.0\n","reward_max: 308.558837890625\n","reward_min: 308.558837890625\n","total_envstep_count: 624968\n","total_train_sample_count: 14999232\n","total_episode_count: 667\n","INFO:root:Training: Train Iter(624800)\tEnv Step(625024)\tLoss(-15.061)\n","INFO:root:Training: Train Iter(624900)\tEnv Step(625152)\tLoss(-22.540)\n","INFO:root:Training: Train Iter(625000)\tEnv Step(625216)\tLoss(-26.028)\n","INFO:root:Training: Train Iter(625100)\tEnv Step(625344)\tLoss(-25.124)\n","INFO:root:Training: Train Iter(625200)\tEnv Step(625408)\tLoss(-26.796)\n","INFO:root:Training: Train Iter(625300)\tEnv Step(625536)\tLoss(-23.064)\n","INFO:root:Training: Train Iter(625400)\tEnv Step(625600)\tLoss(-24.652)\n","INFO:root:Training: Train Iter(625500)\tEnv Step(625728)\tLoss(-24.128)\n","INFO:root:Training: Train Iter(625600)\tEnv Step(625856)\tLoss(-27.258)\n","INFO:root:Evaluation: Train Iter(625664) Env Step(625856) Episode Return(313.514) \n","INFO:root:Training: Train Iter(625700)\tEnv Step(625920)\tLoss(-19.397)\n","INFO:root:Training: Train Iter(625800)\tEnv Step(626048)\tLoss(-23.505)\n","INFO:root:Training: Train Iter(625900)\tEnv Step(626112)\tLoss(-23.053)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78267\n","train_sample_count: 1878408\n","avg_envstep_per_episode: 78267.0\n","avg_sample_per_episode: 1878408.0\n","avg_envstep_per_sec: 8684.21563800121\n","avg_train_sample_per_sec: 208421.175312029\n","avg_episode_per_sec: 0.1109562860209438\n","reward_mean: 308.3717041015625\n","reward_std: 0.0\n","reward_max: 308.3717041015625\n","reward_min: 308.3717041015625\n","total_envstep_count: 626136\n","total_train_sample_count: 15027264\n","total_episode_count: 668\n","INFO:root:Training: Train Iter(626000)\tEnv Step(626240)\tLoss(-19.534)\n","INFO:root:Training: Train Iter(626100)\tEnv Step(626304)\tLoss(-21.707)\n","INFO:root:Training: Train Iter(626200)\tEnv Step(626432)\tLoss(-27.743)\n","INFO:root:Training: Train Iter(626300)\tEnv Step(626496)\tLoss(-19.940)\n","INFO:root:Training: Train Iter(626400)\tEnv Step(626624)\tLoss(-27.607)\n","INFO:root:Training: Train Iter(626500)\tEnv Step(626752)\tLoss(-26.180)\n","INFO:root:Training: Train Iter(626600)\tEnv Step(626816)\tLoss(-19.160)\n","INFO:root:Evaluation: Train Iter(626688) Env Step(626880) Episode Return(312.464) \n","INFO:root:Training: Train Iter(626700)\tEnv Step(626944)\tLoss(-13.121)\n","INFO:root:Training: Train Iter(626800)\tEnv Step(627008)\tLoss(-25.059)\n","INFO:root:Training: Train Iter(626900)\tEnv Step(627136)\tLoss(-17.087)\n","INFO:root:Training: Train Iter(627000)\tEnv Step(627200)\tLoss(-23.956)\n","INFO:root:Training: Train Iter(627100)\tEnv Step(627328)\tLoss(-12.403)\n","INFO:root:Training: Train Iter(627200)\tEnv Step(627456)\tLoss(-25.440)\n","INFO:root:Training: Train Iter(627300)\tEnv Step(627520)\tLoss(-27.055)\n","INFO:root:Training: Train Iter(627400)\tEnv Step(627648)\tLoss(-23.836)\n","INFO:root:Training: Train Iter(627500)\tEnv Step(627712)\tLoss(-23.431)\n","INFO:root:Training: Train Iter(627600)\tEnv Step(627840)\tLoss(-20.412)\n","INFO:root:Training: Train Iter(627700)\tEnv Step(627904)\tLoss(-25.671)\n","INFO:root:Evaluation: Train Iter(627712) Env Step(627904) Episode Return(313.567) \n","INFO:root:Training: Train Iter(627800)\tEnv Step(628032)\tLoss(-26.791)\n","INFO:root:Training: Train Iter(627900)\tEnv Step(628096)\tLoss(-20.338)\n","INFO:root:Training: Train Iter(628000)\tEnv Step(628224)\tLoss(-18.372)\n","INFO:root:Training: Train Iter(628100)\tEnv Step(628352)\tLoss(-23.626)\n","INFO:root:Training: Train Iter(628200)\tEnv Step(628416)\tLoss(-20.375)\n","INFO:root:Training: Train Iter(628300)\tEnv Step(628544)\tLoss(-25.219)\n","INFO:root:Training: Train Iter(628400)\tEnv Step(628608)\tLoss(-21.264)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78586\n","train_sample_count: 1886064\n","avg_envstep_per_episode: 78586.0\n","avg_sample_per_episode: 1886064.0\n","avg_envstep_per_sec: 8510.022898411315\n","avg_train_sample_per_sec: 204240.54956187156\n","avg_episode_per_sec: 0.10828929960058171\n","reward_mean: 307.764404296875\n","reward_std: 0.0\n","reward_max: 307.764404296875\n","reward_min: 307.764404296875\n","total_envstep_count: 628688\n","total_train_sample_count: 15088512\n","total_episode_count: 669\n","INFO:root:Training: Train Iter(628500)\tEnv Step(628736)\tLoss(-23.647)\n","INFO:root:Training: Train Iter(628600)\tEnv Step(628800)\tLoss(-25.260)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78598\n","train_sample_count: 1886352\n","avg_envstep_per_episode: 78598.0\n","avg_sample_per_episode: 1886352.0\n","avg_envstep_per_sec: 8865.395480381327\n","avg_train_sample_per_sec: 212769.49152915183\n","avg_episode_per_sec: 0.1127941611794362\n","reward_mean: 307.7129211425781\n","reward_std: 0.0\n","reward_max: 307.7129211425781\n","reward_min: 307.7129211425781\n","total_envstep_count: 628808\n","total_train_sample_count: 15091392\n","total_episode_count: 670\n","INFO:root:Training: Train Iter(628700)\tEnv Step(628928)\tLoss(-24.285)\n","INFO:root:Evaluation: Train Iter(628736) Env Step(628928) Episode Return(314.711) \n","INFO:root:Training: Train Iter(628800)\tEnv Step(629056)\tLoss(-25.976)\n","INFO:root:Training: Train Iter(628900)\tEnv Step(629120)\tLoss(-24.726)\n","INFO:root:Training: Train Iter(629000)\tEnv Step(629248)\tLoss(-25.168)\n","INFO:root:Training: Train Iter(629100)\tEnv Step(629312)\tLoss(-26.546)\n","INFO:root:Training: Train Iter(629200)\tEnv Step(629440)\tLoss(-25.587)\n","INFO:root:Training: Train Iter(629300)\tEnv Step(629504)\tLoss(-24.330)\n","INFO:root:Training: Train Iter(629400)\tEnv Step(629632)\tLoss(-20.819)\n","INFO:root:Training: Train Iter(629500)\tEnv Step(629696)\tLoss(-23.776)\n","INFO:root:Training: Train Iter(629600)\tEnv Step(629824)\tLoss(-23.627)\n","INFO:root:Training: Train Iter(629700)\tEnv Step(629952)\tLoss(-14.931)\n","INFO:root:Evaluation: Train Iter(629760) Env Step(629952) Episode Return(309.505) \n","INFO:root:Training: Train Iter(629800)\tEnv Step(630016)\tLoss(-23.688)\n","INFO:root:Training: Train Iter(629900)\tEnv Step(630144)\tLoss(-16.484)\n","INFO:root:Training: Train Iter(630000)\tEnv Step(630208)\tLoss(-24.066)\n","INFO:root:Training: Train Iter(630100)\tEnv Step(630336)\tLoss(-25.936)\n","INFO:root:Training: Train Iter(630200)\tEnv Step(630400)\tLoss(-19.218)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78816\n","train_sample_count: 1891584\n","avg_envstep_per_episode: 78816.0\n","avg_sample_per_episode: 1891584.0\n","avg_envstep_per_sec: 8707.925525557815\n","avg_train_sample_per_sec: 208990.21261338756\n","avg_episode_per_sec: 0.11048423575870146\n","reward_mean: 307.3199462890625\n","reward_std: 0.0\n","reward_max: 307.3199462890625\n","reward_min: 307.3199462890625\n","total_envstep_count: 630528\n","total_train_sample_count: 15132672\n","total_episode_count: 671\n","INFO:root:Training: Train Iter(630300)\tEnv Step(630528)\tLoss(-26.836)\n","INFO:root:Training: Train Iter(630400)\tEnv Step(630656)\tLoss(-25.046)\n","INFO:root:Training: Train Iter(630500)\tEnv Step(630720)\tLoss(-26.347)\n","INFO:root:Training: Train Iter(630600)\tEnv Step(630848)\tLoss(-25.401)\n","INFO:root:Training: Train Iter(630700)\tEnv Step(630912)\tLoss(-25.937)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78867\n","train_sample_count: 1892808\n","avg_envstep_per_episode: 78867.0\n","avg_sample_per_episode: 1892808.0\n","avg_envstep_per_sec: 6114.851042714318\n","avg_train_sample_per_sec: 146756.42502514366\n","avg_episode_per_sec: 0.07753370919033713\n","reward_mean: 308.875244140625\n","reward_std: 0.0\n","reward_max: 308.875244140625\n","reward_min: 308.875244140625\n","total_envstep_count: 630936\n","total_train_sample_count: 15142464\n","total_episode_count: 672\n","INFO:root:Evaluation: Train Iter(630784) Env Step(630976) Episode Return(312.358) \n","INFO:root:Training: Train Iter(630800)\tEnv Step(631040)\tLoss(-23.438)\n","INFO:root:Training: Train Iter(630900)\tEnv Step(631104)\tLoss(-17.016)\n","INFO:root:Training: Train Iter(631000)\tEnv Step(631232)\tLoss(-20.421)\n","INFO:root:Training: Train Iter(631100)\tEnv Step(631296)\tLoss(-22.997)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78920\n","train_sample_count: 1894080\n","avg_envstep_per_episode: 78920.0\n","avg_sample_per_episode: 1894080.0\n","avg_envstep_per_sec: 8704.471287599104\n","avg_train_sample_per_sec: 208907.3109023785\n","avg_episode_per_sec: 0.11029487186516858\n","reward_mean: 307.6416931152344\n","reward_std: 0.0\n","reward_max: 307.6416931152344\n","reward_min: 307.6416931152344\n","total_envstep_count: 631360\n","total_train_sample_count: 15152640\n","total_episode_count: 673\n","INFO:root:Training: Train Iter(631200)\tEnv Step(631424)\tLoss(-24.181)\n","INFO:root:Training: Train Iter(631300)\tEnv Step(631552)\tLoss(-27.506)\n","INFO:root:Training: Train Iter(631400)\tEnv Step(631616)\tLoss(-28.379)\n","INFO:root:Training: Train Iter(631500)\tEnv Step(631744)\tLoss(-24.419)\n","INFO:root:Training: Train Iter(631600)\tEnv Step(631808)\tLoss(-23.571)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78992\n","train_sample_count: 1895808\n","avg_envstep_per_episode: 78992.0\n","avg_sample_per_episode: 1895808.0\n","avg_envstep_per_sec: 8766.40148166966\n","avg_train_sample_per_sec: 210393.63556007185\n","avg_episode_per_sec: 0.11097834567639331\n","reward_mean: 308.5785827636719\n","reward_std: 0.0\n","reward_max: 308.5785827636719\n","reward_min: 308.5785827636719\n","total_envstep_count: 631936\n","total_train_sample_count: 15166464\n","total_episode_count: 674\n","INFO:root:Training: Train Iter(631700)\tEnv Step(631936)\tLoss(-26.404)\n","INFO:root:Training: Train Iter(631800)\tEnv Step(632000)\tLoss(-26.098)\n","INFO:root:Evaluation: Train Iter(631808) Env Step(632000) Episode Return(310.092) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 78994\n","train_sample_count: 1895856\n","avg_envstep_per_episode: 78994.0\n","avg_sample_per_episode: 1895856.0\n","avg_envstep_per_sec: 8003.1099281935385\n","avg_train_sample_per_sec: 192074.63827664492\n","avg_episode_per_sec: 0.10131288361386356\n","reward_mean: 308.60113525390625\n","reward_std: 0.0\n","reward_max: 308.60113525390625\n","reward_min: 308.60113525390625\n","total_envstep_count: 632008\n","total_train_sample_count: 15168192\n","total_episode_count: 675\n","INFO:root:Training: Train Iter(631900)\tEnv Step(632128)\tLoss(-26.650)\n","INFO:root:Training: Train Iter(632000)\tEnv Step(632256)\tLoss(-26.397)\n","INFO:root:Training: Train Iter(632100)\tEnv Step(632320)\tLoss(-27.896)\n","INFO:root:Training: Train Iter(632200)\tEnv Step(632448)\tLoss(-24.124)\n","INFO:root:Training: Train Iter(632300)\tEnv Step(632512)\tLoss(-23.323)\n","INFO:root:Training: Train Iter(632400)\tEnv Step(632640)\tLoss(-23.591)\n","INFO:root:Training: Train Iter(632500)\tEnv Step(632704)\tLoss(-17.922)\n","INFO:root:Training: Train Iter(632600)\tEnv Step(632832)\tLoss(-17.322)\n","INFO:root:Training: Train Iter(632700)\tEnv Step(632896)\tLoss(-17.804)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79122\n","train_sample_count: 1898928\n","avg_envstep_per_episode: 79122.0\n","avg_sample_per_episode: 1898928.0\n","avg_envstep_per_sec: 8681.924112159313\n","avg_train_sample_per_sec: 208366.1786918235\n","avg_episode_per_sec: 0.10972831971081763\n","reward_mean: 309.372802734375\n","reward_std: 0.0\n","reward_max: 309.372802734375\n","reward_min: 309.372802734375\n","total_envstep_count: 632976\n","total_train_sample_count: 15191424\n","total_episode_count: 676\n","INFO:root:Training: Train Iter(632800)\tEnv Step(633024)\tLoss(-24.165)\n","INFO:root:Evaluation: Train Iter(632832) Env Step(633024) Episode Return(306.813) \n","INFO:root:Training: Train Iter(632900)\tEnv Step(633152)\tLoss(-26.095)\n","INFO:root:Training: Train Iter(633000)\tEnv Step(633216)\tLoss(-25.768)\n","INFO:root:Training: Train Iter(633100)\tEnv Step(633344)\tLoss(-20.680)\n","INFO:root:Training: Train Iter(633200)\tEnv Step(633408)\tLoss(-24.680)\n","INFO:root:Training: Train Iter(633300)\tEnv Step(633536)\tLoss(-24.995)\n","INFO:root:Training: Train Iter(633400)\tEnv Step(633600)\tLoss(-26.486)\n","INFO:root:Training: Train Iter(633500)\tEnv Step(633728)\tLoss(-26.949)\n","INFO:root:Training: Train Iter(633600)\tEnv Step(633856)\tLoss(-26.585)\n","INFO:root:Training: Train Iter(633700)\tEnv Step(633920)\tLoss(-26.216)\n","INFO:root:Training: Train Iter(633800)\tEnv Step(634048)\tLoss(-24.133)\n","INFO:root:Evaluation: Train Iter(633856) Env Step(634048) Episode Return(315.712) \n","INFO:root:Training: Train Iter(633900)\tEnv Step(634112)\tLoss(-24.074)\n","INFO:root:Training: Train Iter(634000)\tEnv Step(634240)\tLoss(-21.533)\n","INFO:root:Training: Train Iter(634100)\tEnv Step(634304)\tLoss(-26.243)\n","INFO:root:Training: Train Iter(634200)\tEnv Step(634432)\tLoss(-23.567)\n","INFO:root:Training: Train Iter(634300)\tEnv Step(634496)\tLoss(-25.470)\n","INFO:root:Training: Train Iter(634400)\tEnv Step(634624)\tLoss(-27.668)\n","INFO:root:Training: Train Iter(634500)\tEnv Step(634752)\tLoss(-21.224)\n","INFO:root:Training: Train Iter(634600)\tEnv Step(634816)\tLoss(-27.139)\n","INFO:root:Training: Train Iter(634700)\tEnv Step(634944)\tLoss(-25.005)\n","INFO:root:Training: Train Iter(634800)\tEnv Step(635008)\tLoss(-26.216)\n","INFO:root:Evaluation: Train Iter(634880) Env Step(635072) Episode Return(307.941) \n","INFO:root:Training: Train Iter(634900)\tEnv Step(635136)\tLoss(-25.639)\n","INFO:root:Training: Train Iter(635000)\tEnv Step(635200)\tLoss(-24.979)\n","INFO:root:Training: Train Iter(635100)\tEnv Step(635328)\tLoss(-23.970)\n","INFO:root:Training: Train Iter(635200)\tEnv Step(635456)\tLoss(-23.721)\n","INFO:root:Training: Train Iter(635300)\tEnv Step(635520)\tLoss(-26.333)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79442\n","train_sample_count: 1906608\n","avg_envstep_per_episode: 79442.0\n","avg_sample_per_episode: 1906608.0\n","avg_envstep_per_sec: 8858.334473351806\n","avg_train_sample_per_sec: 212600.02736044332\n","avg_episode_per_sec: 0.1115069418362051\n","reward_mean: 308.5883483886719\n","reward_std: 0.0\n","reward_max: 308.5883483886719\n","reward_min: 308.5883483886719\n","total_envstep_count: 635536\n","total_train_sample_count: 15252864\n","total_episode_count: 677\n","INFO:root:Training: Train Iter(635400)\tEnv Step(635648)\tLoss(-26.364)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79454\n","train_sample_count: 1906896\n","avg_envstep_per_episode: 79454.0\n","avg_sample_per_episode: 1906896.0\n","avg_envstep_per_sec: 8506.16680876701\n","avg_train_sample_per_sec: 204148.00341040822\n","avg_episode_per_sec: 0.10705775428256613\n","reward_mean: 309.0558776855469\n","reward_std: 0.0\n","reward_max: 309.0558776855469\n","reward_min: 309.0558776855469\n","total_envstep_count: 635656\n","total_train_sample_count: 15255744\n","total_episode_count: 678\n","INFO:root:Training: Train Iter(635500)\tEnv Step(635712)\tLoss(-27.660)\n","INFO:root:Training: Train Iter(635600)\tEnv Step(635840)\tLoss(-25.482)\n","INFO:root:Training: Train Iter(635700)\tEnv Step(635904)\tLoss(-23.699)\n","INFO:root:Training: Train Iter(635800)\tEnv Step(636032)\tLoss(-21.535)\n","INFO:root:Training: Train Iter(635900)\tEnv Step(636096)\tLoss(-23.603)\n","INFO:root:Evaluation: Train Iter(635904) Env Step(636096) Episode Return(305.929) \n","INFO:root:Training: Train Iter(636000)\tEnv Step(636224)\tLoss(-24.301)\n","INFO:root:Training: Train Iter(636100)\tEnv Step(636352)\tLoss(-21.217)\n","INFO:root:Training: Train Iter(636200)\tEnv Step(636416)\tLoss(-25.616)\n","INFO:root:Training: Train Iter(636300)\tEnv Step(636544)\tLoss(-25.330)\n","INFO:root:Training: Train Iter(636400)\tEnv Step(636608)\tLoss(-24.655)\n","INFO:root:Training: Train Iter(636500)\tEnv Step(636736)\tLoss(-21.090)\n","INFO:root:Training: Train Iter(636600)\tEnv Step(636800)\tLoss(-26.937)\n","INFO:root:Training: Train Iter(636700)\tEnv Step(636928)\tLoss(-26.257)\n","INFO:root:Training: Train Iter(636800)\tEnv Step(637056)\tLoss(-20.354)\n","INFO:root:Training: Train Iter(636900)\tEnv Step(637120)\tLoss(-22.442)\n","INFO:root:Evaluation: Train Iter(636928) Env Step(637120) Episode Return(311.198) \n","INFO:root:Training: Train Iter(637000)\tEnv Step(637248)\tLoss(-24.304)\n","INFO:root:Training: Train Iter(637100)\tEnv Step(637312)\tLoss(-23.397)\n","INFO:root:Training: Train Iter(637200)\tEnv Step(637440)\tLoss(-28.150)\n","INFO:root:Training: Train Iter(637300)\tEnv Step(637504)\tLoss(-22.564)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79692\n","train_sample_count: 1912608\n","avg_envstep_per_episode: 79692.0\n","avg_sample_per_episode: 1912608.0\n","avg_envstep_per_sec: 8704.295488036445\n","avg_train_sample_per_sec: 208903.09171287465\n","avg_episode_per_sec: 0.10922420679662255\n","reward_mean: 304.3893737792969\n","reward_std: 0.0\n","reward_max: 304.3893737792969\n","reward_min: 304.3893737792969\n","total_envstep_count: 637536\n","total_train_sample_count: 15300864\n","total_episode_count: 679\n","INFO:root:Training: Train Iter(637400)\tEnv Step(637632)\tLoss(-25.354)\n","INFO:root:Training: Train Iter(637500)\tEnv Step(637696)\tLoss(-27.628)\n","INFO:root:Training: Train Iter(637600)\tEnv Step(637824)\tLoss(-24.509)\n","INFO:root:Training: Train Iter(637700)\tEnv Step(637952)\tLoss(-24.799)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79748\n","train_sample_count: 1913952\n","avg_envstep_per_episode: 79748.0\n","avg_sample_per_episode: 1913952.0\n","avg_envstep_per_sec: 6113.7805571440895\n","avg_train_sample_per_sec: 146730.73337145816\n","avg_episode_per_sec: 0.07666374776977591\n","reward_mean: 307.6004638671875\n","reward_std: 0.0\n","reward_max: 307.6004638671875\n","reward_min: 307.6004638671875\n","total_envstep_count: 637984\n","total_train_sample_count: 15311616\n","total_episode_count: 680\n","INFO:root:Training: Train Iter(637800)\tEnv Step(638016)\tLoss(-18.123)\n","INFO:root:Training: Train Iter(637900)\tEnv Step(638144)\tLoss(-20.614)\n","INFO:root:Evaluation: Train Iter(637952) Env Step(638144) Episode Return(306.236) \n","INFO:root:Training: Train Iter(638000)\tEnv Step(638208)\tLoss(-15.219)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79783\n","train_sample_count: 1914792\n","avg_envstep_per_episode: 79783.0\n","avg_sample_per_episode: 1914792.0\n","avg_envstep_per_sec: 8704.37229092509\n","avg_train_sample_per_sec: 208904.93498220216\n","avg_episode_per_sec: 0.10910058898418323\n","reward_mean: 307.4222412109375\n","reward_std: 0.0\n","reward_max: 307.4222412109375\n","reward_min: 307.4222412109375\n","total_envstep_count: 638264\n","total_train_sample_count: 15318336\n","total_episode_count: 681\n","INFO:root:Training: Train Iter(638100)\tEnv Step(638336)\tLoss(-27.303)\n","INFO:root:Training: Train Iter(638200)\tEnv Step(638400)\tLoss(-20.672)\n","INFO:root:Training: Train Iter(638300)\tEnv Step(638528)\tLoss(-25.972)\n","INFO:root:Training: Train Iter(638400)\tEnv Step(638656)\tLoss(-21.924)\n","INFO:root:Training: Train Iter(638500)\tEnv Step(638720)\tLoss(-22.142)\n","INFO:root:Training: Train Iter(638600)\tEnv Step(638848)\tLoss(-18.079)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79861\n","train_sample_count: 1916664\n","avg_envstep_per_episode: 79861.0\n","avg_sample_per_episode: 1916664.0\n","avg_envstep_per_sec: 8002.74853600236\n","avg_train_sample_per_sec: 192065.96486405664\n","avg_episode_per_sec: 0.10020846891476891\n","reward_mean: 307.66571044921875\n","reward_std: 0.0\n","reward_max: 307.66571044921875\n","reward_min: 307.66571044921875\n","total_envstep_count: 638888\n","total_train_sample_count: 15333312\n","total_episode_count: 682\n","INFO:root:Training: Train Iter(638700)\tEnv Step(638912)\tLoss(-29.301)\n","INFO:root:Training: Train Iter(638800)\tEnv Step(639040)\tLoss(-13.329)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79884\n","train_sample_count: 1917216\n","avg_envstep_per_episode: 79884.0\n","avg_sample_per_episode: 1917216.0\n","avg_envstep_per_sec: 8764.783954943956\n","avg_train_sample_per_sec: 210354.81491865494\n","avg_episode_per_sec: 0.10971889182995288\n","reward_mean: 305.37994384765625\n","reward_std: 0.0\n","reward_max: 305.37994384765625\n","reward_min: 305.37994384765625\n","total_envstep_count: 639072\n","total_train_sample_count: 15337728\n","total_episode_count: 683\n","INFO:root:Training: Train Iter(638900)\tEnv Step(639104)\tLoss(-26.797)\n","INFO:root:Evaluation: Train Iter(638976) Env Step(639168) Episode Return(310.005) \n","INFO:root:Training: Train Iter(639000)\tEnv Step(639232)\tLoss(-20.595)\n","INFO:root:Training: Train Iter(639100)\tEnv Step(639296)\tLoss(-26.709)\n","INFO:root:Training: Train Iter(639200)\tEnv Step(639424)\tLoss(-25.051)\n","INFO:root:Training: Train Iter(639300)\tEnv Step(639552)\tLoss(-20.585)\n","INFO:root:Training: Train Iter(639400)\tEnv Step(639616)\tLoss(-24.255)\n","INFO:root:Training: Train Iter(639500)\tEnv Step(639744)\tLoss(-29.130)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 79972\n","train_sample_count: 1919328\n","avg_envstep_per_episode: 79972.0\n","avg_sample_per_episode: 1919328.0\n","avg_envstep_per_sec: 8677.419842481597\n","avg_train_sample_per_sec: 208258.07621955834\n","avg_episode_per_sec: 0.10850572503478215\n","reward_mean: 307.857666015625\n","reward_std: 0.0\n","reward_max: 307.857666015625\n","reward_min: 307.857666015625\n","total_envstep_count: 639776\n","total_train_sample_count: 15354624\n","total_episode_count: 684\n","INFO:root:Training: Train Iter(639600)\tEnv Step(639808)\tLoss(-25.314)\n","INFO:root:Training: Train Iter(639700)\tEnv Step(639936)\tLoss(-30.189)\n","INFO:root:Training: Train Iter(639800)\tEnv Step(640000)\tLoss(-25.410)\n","INFO:root:Training: Train Iter(639900)\tEnv Step(640128)\tLoss(-26.303)\n","INFO:root:Evaluation: Train Iter(640000) Env Step(640192) Episode Return(310.876) \n","INFO:root:Training: Train Iter(640000)\tEnv Step(640256)\tLoss(-28.452)\n","INFO:root:Training: Train Iter(640100)\tEnv Step(640320)\tLoss(-27.525)\n","INFO:root:Training: Train Iter(640200)\tEnv Step(640448)\tLoss(-24.598)\n","INFO:root:Training: Train Iter(640300)\tEnv Step(640512)\tLoss(-27.573)\n","INFO:root:Training: Train Iter(640400)\tEnv Step(640640)\tLoss(-28.882)\n","INFO:root:Training: Train Iter(640500)\tEnv Step(640704)\tLoss(-23.134)\n","INFO:root:Training: Train Iter(640600)\tEnv Step(640832)\tLoss(-27.234)\n","INFO:root:Training: Train Iter(640700)\tEnv Step(640896)\tLoss(-26.828)\n","INFO:root:Training: Train Iter(640800)\tEnv Step(641024)\tLoss(-26.636)\n","INFO:root:Training: Train Iter(640900)\tEnv Step(641152)\tLoss(-18.205)\n","INFO:root:Training: Train Iter(641000)\tEnv Step(641216)\tLoss(-26.525)\n","INFO:root:Evaluation: Train Iter(641024) Env Step(641216) Episode Return(310.017) \n","INFO:root:Training: Train Iter(641100)\tEnv Step(641344)\tLoss(-19.337)\n","INFO:root:Training: Train Iter(641200)\tEnv Step(641408)\tLoss(-24.641)\n","INFO:root:Training: Train Iter(641300)\tEnv Step(641536)\tLoss(-26.775)\n","INFO:root:Training: Train Iter(641400)\tEnv Step(641600)\tLoss(-27.655)\n","INFO:root:Training: Train Iter(641500)\tEnv Step(641728)\tLoss(-19.290)\n","INFO:root:Training: Train Iter(641600)\tEnv Step(641856)\tLoss(-25.798)\n","INFO:root:Training: Train Iter(641700)\tEnv Step(641920)\tLoss(-27.799)\n","INFO:root:Training: Train Iter(641800)\tEnv Step(642048)\tLoss(-26.718)\n","INFO:root:Training: Train Iter(641900)\tEnv Step(642112)\tLoss(-26.914)\n","INFO:root:Training: Train Iter(642000)\tEnv Step(642240)\tLoss(-22.754)\n","INFO:root:Evaluation: Train Iter(642048) Env Step(642240) Episode Return(310.425) \n","INFO:root:Training: Train Iter(642100)\tEnv Step(642304)\tLoss(-25.380)\n","INFO:root:Training: Train Iter(642200)\tEnv Step(642432)\tLoss(-24.229)\n","INFO:root:Training: Train Iter(642300)\tEnv Step(642496)\tLoss(-27.113)\n","INFO:root:Training: Train Iter(642400)\tEnv Step(642624)\tLoss(-25.763)\n","INFO:root:Training: Train Iter(642500)\tEnv Step(642752)\tLoss(-26.568)\n","INFO:root:Training: Train Iter(642600)\tEnv Step(642816)\tLoss(-26.862)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80355\n","train_sample_count: 1928520\n","avg_envstep_per_episode: 80355.0\n","avg_sample_per_episode: 1928520.0\n","avg_envstep_per_sec: 8859.566204368024\n","avg_train_sample_per_sec: 212629.58890483258\n","avg_episode_per_sec: 0.11025531957399072\n","reward_mean: 305.7569580078125\n","reward_std: 0.0\n","reward_max: 305.7569580078125\n","reward_min: 305.7569580078125\n","total_envstep_count: 642840\n","total_train_sample_count: 15428160\n","total_episode_count: 685\n","INFO:root:Training: Train Iter(642700)\tEnv Step(642944)\tLoss(-25.628)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80360\n","train_sample_count: 1928640\n","avg_envstep_per_episode: 80360.0\n","avg_sample_per_episode: 1928640.0\n","avg_envstep_per_sec: 8506.71692208838\n","avg_train_sample_per_sec: 204161.20613012114\n","avg_episode_per_sec: 0.10585760231568418\n","reward_mean: 304.73907470703125\n","reward_std: 0.0\n","reward_max: 304.73907470703125\n","reward_min: 304.73907470703125\n","total_envstep_count: 642952\n","total_train_sample_count: 15430848\n","total_episode_count: 686\n","INFO:root:Training: Train Iter(642800)\tEnv Step(643008)\tLoss(-28.482)\n","INFO:root:Training: Train Iter(642900)\tEnv Step(643136)\tLoss(-27.012)\n","INFO:root:Training: Train Iter(643000)\tEnv Step(643200)\tLoss(-24.958)\n","INFO:root:Evaluation: Train Iter(643072) Env Step(643264) Episode Return(309.980) \n","INFO:root:Training: Train Iter(643100)\tEnv Step(643328)\tLoss(-26.697)\n","INFO:root:Training: Train Iter(643200)\tEnv Step(643456)\tLoss(-26.123)\n","INFO:root:Training: Train Iter(643300)\tEnv Step(643520)\tLoss(-25.457)\n","INFO:root:Training: Train Iter(643400)\tEnv Step(643648)\tLoss(-19.004)\n","INFO:root:Training: Train Iter(643500)\tEnv Step(643712)\tLoss(-23.677)\n","INFO:root:Training: Train Iter(643600)\tEnv Step(643840)\tLoss(-27.503)\n","INFO:root:Training: Train Iter(643700)\tEnv Step(643904)\tLoss(-20.916)\n","INFO:root:Training: Train Iter(643800)\tEnv Step(644032)\tLoss(-26.068)\n","INFO:root:Training: Train Iter(643900)\tEnv Step(644096)\tLoss(-19.460)\n","INFO:root:Training: Train Iter(644000)\tEnv Step(644224)\tLoss(-25.926)\n","INFO:root:Evaluation: Train Iter(644096) Env Step(644288) Episode Return(283.893) \n","INFO:root:Training: Train Iter(644100)\tEnv Step(644352)\tLoss(-18.224)\n","INFO:root:Training: Train Iter(644200)\tEnv Step(644416)\tLoss(-19.100)\n","INFO:root:Training: Train Iter(644300)\tEnv Step(644544)\tLoss(-25.556)\n","INFO:root:Training: Train Iter(644400)\tEnv Step(644608)\tLoss(-27.507)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80587\n","train_sample_count: 1934088\n","avg_envstep_per_episode: 80587.0\n","avg_sample_per_episode: 1934088.0\n","avg_envstep_per_sec: 8703.553200176844\n","avg_train_sample_per_sec: 208885.27680424426\n","avg_episode_per_sec: 0.1080019506890298\n","reward_mean: 307.04388427734375\n","reward_std: 0.0\n","reward_max: 307.04388427734375\n","reward_min: 307.04388427734375\n","total_envstep_count: 644696\n","total_train_sample_count: 15472704\n","total_episode_count: 687\n","INFO:root:Training: Train Iter(644500)\tEnv Step(644736)\tLoss(-22.715)\n","INFO:root:Training: Train Iter(644600)\tEnv Step(644800)\tLoss(-28.380)\n","INFO:root:Training: Train Iter(644700)\tEnv Step(644928)\tLoss(-21.221)\n","INFO:root:Training: Train Iter(644800)\tEnv Step(645056)\tLoss(-28.690)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80639\n","train_sample_count: 1935336\n","avg_envstep_per_episode: 80639.0\n","avg_sample_per_episode: 1935336.0\n","avg_envstep_per_sec: 6114.8317247391005\n","avg_train_sample_per_sec: 146755.96139373843\n","avg_episode_per_sec: 0.07582970677636257\n","reward_mean: 307.908447265625\n","reward_std: 0.0\n","reward_max: 307.908447265625\n","reward_min: 307.908447265625\n","total_envstep_count: 645112\n","total_train_sample_count: 15482688\n","total_episode_count: 688\n","INFO:root:Training: Train Iter(644900)\tEnv Step(645120)\tLoss(-25.614)\n","INFO:root:Training: Train Iter(645000)\tEnv Step(645248)\tLoss(-24.230)\n","INFO:root:Training: Train Iter(645100)\tEnv Step(645312)\tLoss(-25.961)\n","INFO:root:Evaluation: Train Iter(645120) Env Step(645312) Episode Return(314.918) \n","INFO:root:Training: Train Iter(645200)\tEnv Step(645440)\tLoss(-25.109)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80681\n","train_sample_count: 1936344\n","avg_envstep_per_episode: 80681.0\n","avg_sample_per_episode: 1936344.0\n","avg_envstep_per_sec: 8705.24705443147\n","avg_train_sample_per_sec: 208925.92930635528\n","avg_episode_per_sec: 0.10789711399748973\n","reward_mean: 307.6571350097656\n","reward_std: 0.0\n","reward_max: 307.6571350097656\n","reward_min: 307.6571350097656\n","total_envstep_count: 645448\n","total_train_sample_count: 15490752\n","total_episode_count: 689\n","INFO:root:Training: Train Iter(645300)\tEnv Step(645504)\tLoss(-25.579)\n","INFO:root:Training: Train Iter(645400)\tEnv Step(645632)\tLoss(-27.483)\n","INFO:root:Training: Train Iter(645500)\tEnv Step(645696)\tLoss(-27.857)\n","INFO:root:Training: Train Iter(645600)\tEnv Step(645824)\tLoss(-20.517)\n","INFO:root:Training: Train Iter(645700)\tEnv Step(645952)\tLoss(-23.852)\n","INFO:root:Training: Train Iter(645800)\tEnv Step(646016)\tLoss(-23.367)\n","INFO:root:Training: Train Iter(645900)\tEnv Step(646144)\tLoss(-26.611)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80773\n","train_sample_count: 1938552\n","avg_envstep_per_episode: 80773.0\n","avg_sample_per_episode: 1938552.0\n","avg_envstep_per_sec: 8005.283568129073\n","avg_train_sample_per_sec: 192126.80563509773\n","avg_episode_per_sec: 0.09910840959391223\n","reward_mean: 306.5967712402344\n","reward_std: 0.0\n","reward_max: 306.5967712402344\n","reward_min: 306.5967712402344\n","total_envstep_count: 646184\n","total_train_sample_count: 15508416\n","total_episode_count: 690\n","INFO:root:Training: Train Iter(646000)\tEnv Step(646208)\tLoss(-24.228)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80785\n","train_sample_count: 1938840\n","avg_envstep_per_episode: 80785.0\n","avg_sample_per_episode: 1938840.0\n","avg_envstep_per_sec: 8766.736886088951\n","avg_train_sample_per_sec: 210401.68526613482\n","avg_episode_per_sec: 0.10851936480892432\n","reward_mean: 307.9190979003906\n","reward_std: 0.0\n","reward_max: 307.9190979003906\n","reward_min: 307.9190979003906\n","total_envstep_count: 646280\n","total_train_sample_count: 15510720\n","total_episode_count: 691\n","INFO:root:Training: Train Iter(646100)\tEnv Step(646336)\tLoss(-26.114)\n","INFO:root:Evaluation: Train Iter(646144) Env Step(646336) Episode Return(309.033) \n","INFO:root:Training: Train Iter(646200)\tEnv Step(646400)\tLoss(-24.120)\n","INFO:root:Training: Train Iter(646300)\tEnv Step(646528)\tLoss(-21.526)\n","INFO:root:Training: Train Iter(646400)\tEnv Step(646656)\tLoss(-27.395)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 80836\n","train_sample_count: 1940064\n","avg_envstep_per_episode: 80836.0\n","avg_sample_per_episode: 1940064.0\n","avg_envstep_per_sec: 8675.021693492645\n","avg_train_sample_per_sec: 208200.52064382346\n","avg_episode_per_sec: 0.10731631566990751\n","reward_mean: 310.31585693359375\n","reward_std: 0.0\n","reward_max: 310.31585693359375\n","reward_min: 310.31585693359375\n","total_envstep_count: 646688\n","total_train_sample_count: 15520512\n","total_episode_count: 692\n","INFO:root:Training: Train Iter(646500)\tEnv Step(646720)\tLoss(-26.811)\n","INFO:root:Training: Train Iter(646600)\tEnv Step(646848)\tLoss(-25.444)\n","INFO:root:Training: Train Iter(646700)\tEnv Step(646912)\tLoss(-25.985)\n","INFO:root:Training: Train Iter(646800)\tEnv Step(647040)\tLoss(-23.959)\n","INFO:root:Training: Train Iter(646900)\tEnv Step(647104)\tLoss(-21.608)\n","INFO:root:Training: Train Iter(647000)\tEnv Step(647232)\tLoss(-25.839)\n","INFO:root:Training: Train Iter(647100)\tEnv Step(647296)\tLoss(-23.539)\n","INFO:root:Evaluation: Train Iter(647168) Env Step(647360) Episode Return(309.827) \n","INFO:root:Training: Train Iter(647200)\tEnv Step(647424)\tLoss(-26.400)\n","INFO:root:Training: Train Iter(647300)\tEnv Step(647552)\tLoss(-27.983)\n","INFO:root:Training: Train Iter(647400)\tEnv Step(647616)\tLoss(-21.331)\n","INFO:root:Training: Train Iter(647500)\tEnv Step(647744)\tLoss(-18.787)\n","INFO:root:Training: Train Iter(647600)\tEnv Step(647808)\tLoss(-8.051)\n","INFO:root:Training: Train Iter(647700)\tEnv Step(647936)\tLoss(-28.381)\n","INFO:root:Training: Train Iter(647800)\tEnv Step(648000)\tLoss(-24.298)\n","INFO:root:Training: Train Iter(647900)\tEnv Step(648128)\tLoss(-22.486)\n","INFO:root:Training: Train Iter(648000)\tEnv Step(648256)\tLoss(-19.960)\n","INFO:root:Training: Train Iter(648100)\tEnv Step(648320)\tLoss(-24.042)\n","INFO:root:Evaluation: Train Iter(648192) Env Step(648384) Episode Return(305.361) \n","INFO:root:Training: Train Iter(648200)\tEnv Step(648448)\tLoss(-23.334)\n","INFO:root:Training: Train Iter(648300)\tEnv Step(648512)\tLoss(-28.148)\n","INFO:root:Training: Train Iter(648400)\tEnv Step(648640)\tLoss(-26.630)\n","INFO:root:Training: Train Iter(648500)\tEnv Step(648704)\tLoss(-25.902)\n","INFO:root:Training: Train Iter(648600)\tEnv Step(648832)\tLoss(-27.112)\n","INFO:root:Training: Train Iter(648700)\tEnv Step(648896)\tLoss(-26.338)\n","INFO:root:Training: Train Iter(648800)\tEnv Step(649024)\tLoss(-26.998)\n","INFO:root:Training: Train Iter(648900)\tEnv Step(649152)\tLoss(-19.119)\n","INFO:root:Training: Train Iter(649000)\tEnv Step(649216)\tLoss(-26.891)\n","INFO:root:Training: Train Iter(649100)\tEnv Step(649344)\tLoss(-25.723)\n","INFO:root:Training: Train Iter(649200)\tEnv Step(649408)\tLoss(-28.926)\n","INFO:root:Evaluation: Train Iter(649216) Env Step(649408) Episode Return(309.418) \n","INFO:root:Training: Train Iter(649300)\tEnv Step(649536)\tLoss(-24.869)\n","INFO:root:Training: Train Iter(649400)\tEnv Step(649600)\tLoss(-26.601)\n","INFO:root:Training: Train Iter(649500)\tEnv Step(649728)\tLoss(-28.115)\n","INFO:root:Training: Train Iter(649600)\tEnv Step(649856)\tLoss(-27.235)\n","INFO:root:Training: Train Iter(649700)\tEnv Step(649920)\tLoss(-27.394)\n","INFO:root:Training: Train Iter(649800)\tEnv Step(650048)\tLoss(-20.081)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81263\n","train_sample_count: 1950312\n","avg_envstep_per_episode: 81263.0\n","avg_sample_per_episode: 1950312.0\n","avg_envstep_per_sec: 8862.620918290411\n","avg_train_sample_per_sec: 212702.90203896986\n","avg_episode_per_sec: 0.10906096154818812\n","reward_mean: 306.4380798339844\n","reward_std: 0.0\n","reward_max: 306.4380798339844\n","reward_min: 306.4380798339844\n","total_envstep_count: 650104\n","total_train_sample_count: 15602496\n","total_episode_count: 693\n","INFO:root:Training: Train Iter(649900)\tEnv Step(650112)\tLoss(-25.193)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81279\n","train_sample_count: 1950696\n","avg_envstep_per_episode: 81279.0\n","avg_sample_per_episode: 1950696.0\n","avg_envstep_per_sec: 8508.8750230619\n","avg_train_sample_per_sec: 204213.00055348565\n","avg_episode_per_sec: 0.10468725037293644\n","reward_mean: 306.0950622558594\n","reward_std: 0.0\n","reward_max: 306.0950622558594\n","reward_min: 306.0950622558594\n","total_envstep_count: 650232\n","total_train_sample_count: 15605568\n","total_episode_count: 694\n","INFO:root:Training: Train Iter(650000)\tEnv Step(650240)\tLoss(-28.857)\n","INFO:root:Training: Train Iter(650100)\tEnv Step(650304)\tLoss(-19.404)\n","INFO:root:Training: Train Iter(650200)\tEnv Step(650432)\tLoss(-27.701)\n","INFO:root:Evaluation: Train Iter(650240) Env Step(650432) Episode Return(311.058) \n","INFO:root:Training: Train Iter(650300)\tEnv Step(650496)\tLoss(-27.882)\n","INFO:root:Training: Train Iter(650400)\tEnv Step(650624)\tLoss(-26.491)\n","INFO:root:Training: Train Iter(650500)\tEnv Step(650752)\tLoss(-26.443)\n","INFO:root:Training: Train Iter(650600)\tEnv Step(650816)\tLoss(-25.850)\n","INFO:root:Training: Train Iter(650700)\tEnv Step(650944)\tLoss(-28.265)\n","INFO:root:Training: Train Iter(650800)\tEnv Step(651008)\tLoss(-28.381)\n","INFO:root:Training: Train Iter(650900)\tEnv Step(651136)\tLoss(-27.159)\n","INFO:root:Training: Train Iter(651000)\tEnv Step(651200)\tLoss(-28.290)\n","INFO:root:Training: Train Iter(651100)\tEnv Step(651328)\tLoss(-27.161)\n","INFO:root:Training: Train Iter(651200)\tEnv Step(651456)\tLoss(-26.857)\n","INFO:root:Evaluation: Train Iter(651264) Env Step(651456) Episode Return(311.803) \n","INFO:root:Training: Train Iter(651300)\tEnv Step(651520)\tLoss(-30.206)\n","INFO:root:Training: Train Iter(651400)\tEnv Step(651648)\tLoss(-23.560)\n","INFO:root:Training: Train Iter(651500)\tEnv Step(651712)\tLoss(-19.650)\n","INFO:root:Training: Train Iter(651600)\tEnv Step(651840)\tLoss(-26.692)\n","INFO:root:Training: Train Iter(651700)\tEnv Step(651904)\tLoss(-28.297)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81490\n","train_sample_count: 1955760\n","avg_envstep_per_episode: 81490.0\n","avg_sample_per_episode: 1955760.0\n","avg_envstep_per_sec: 8707.133571027638\n","avg_train_sample_per_sec: 208971.2057046633\n","avg_episode_per_sec: 0.10684910505617422\n","reward_mean: 306.4801025390625\n","reward_std: 0.0\n","reward_max: 306.4801025390625\n","reward_min: 306.4801025390625\n","total_envstep_count: 651920\n","total_train_sample_count: 15646080\n","total_episode_count: 695\n","INFO:root:Training: Train Iter(651800)\tEnv Step(652032)\tLoss(-26.372)\n","INFO:root:Training: Train Iter(651900)\tEnv Step(652096)\tLoss(-26.306)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81528\n","train_sample_count: 1956672\n","avg_envstep_per_episode: 81528.0\n","avg_sample_per_episode: 1956672.0\n","avg_envstep_per_sec: 6116.953077022735\n","avg_train_sample_per_sec: 146806.87384854563\n","avg_episode_per_sec: 0.07502886219486231\n","reward_mean: 307.01385498046875\n","reward_std: 0.0\n","reward_max: 307.01385498046875\n","reward_min: 307.01385498046875\n","total_envstep_count: 652224\n","total_train_sample_count: 15653376\n","total_episode_count: 696\n","INFO:root:Training: Train Iter(652000)\tEnv Step(652224)\tLoss(-26.260)\n","INFO:root:Training: Train Iter(652100)\tEnv Step(652352)\tLoss(-26.302)\n","INFO:root:Training: Train Iter(652200)\tEnv Step(652416)\tLoss(-27.937)\n","INFO:root:Evaluation: Train Iter(652288) Env Step(652480) Episode Return(308.862) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81567\n","train_sample_count: 1957608\n","avg_envstep_per_episode: 81567.0\n","avg_sample_per_episode: 1957608.0\n","avg_envstep_per_sec: 8707.34338259172\n","avg_train_sample_per_sec: 208976.2411822013\n","avg_episode_per_sec: 0.10675081077631543\n","reward_mean: 307.10394287109375\n","reward_std: 0.0\n","reward_max: 307.10394287109375\n","reward_min: 307.10394287109375\n","total_envstep_count: 652536\n","total_train_sample_count: 15660864\n","total_episode_count: 697\n","INFO:root:Training: Train Iter(652300)\tEnv Step(652544)\tLoss(-26.294)\n","INFO:root:Training: Train Iter(652400)\tEnv Step(652608)\tLoss(-27.520)\n","INFO:root:Training: Train Iter(652500)\tEnv Step(652736)\tLoss(-26.316)\n","INFO:root:Training: Train Iter(652600)\tEnv Step(652800)\tLoss(-23.755)\n","INFO:root:Training: Train Iter(652700)\tEnv Step(652928)\tLoss(-25.298)\n","INFO:root:Training: Train Iter(652800)\tEnv Step(653056)\tLoss(-26.258)\n","INFO:root:Training: Train Iter(652900)\tEnv Step(653120)\tLoss(-24.582)\n","INFO:root:Training: Train Iter(653000)\tEnv Step(653248)\tLoss(-24.092)\n","INFO:root:Training: Train Iter(653100)\tEnv Step(653312)\tLoss(-25.990)\n","INFO:root:Training: Train Iter(653200)\tEnv Step(653440)\tLoss(-21.697)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81682\n","train_sample_count: 1960368\n","avg_envstep_per_episode: 81682.0\n","avg_sample_per_episode: 1960368.0\n","avg_envstep_per_sec: 8006.18042737295\n","avg_train_sample_per_sec: 192148.3302569508\n","avg_episode_per_sec: 0.09801645928568045\n","reward_mean: 304.33544921875\n","reward_std: 0.0\n","reward_max: 304.33544921875\n","reward_min: 304.33544921875\n","total_envstep_count: 653456\n","total_train_sample_count: 15682944\n","total_episode_count: 698\n","INFO:root:Training: Train Iter(653300)\tEnv Step(653504)\tLoss(-27.587)\n","INFO:root:Evaluation: Train Iter(653312) Env Step(653504) Episode Return(260.048) \n","INFO:root:Training: Train Iter(653400)\tEnv Step(653632)\tLoss(-28.628)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81709\n","train_sample_count: 1961016\n","avg_envstep_per_episode: 81709.0\n","avg_sample_per_episode: 1961016.0\n","avg_envstep_per_sec: 8771.75175960756\n","avg_train_sample_per_sec: 210522.04223058143\n","avg_episode_per_sec: 0.10735355664134379\n","reward_mean: 193.16412353515625\n","reward_std: 0.0\n","reward_max: 193.16412353515625\n","reward_min: 193.16412353515625\n","total_envstep_count: 653672\n","total_train_sample_count: 15688128\n","total_episode_count: 699\n","INFO:root:Training: Train Iter(653500)\tEnv Step(653696)\tLoss(-28.092)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 81725\n","train_sample_count: 1961400\n","avg_envstep_per_episode: 81725.0\n","avg_sample_per_episode: 1961400.0\n","avg_envstep_per_sec: 8674.506336690054\n","avg_train_sample_per_sec: 208188.15208056133\n","avg_episode_per_sec: 0.10614262877565071\n","reward_mean: 306.45379638671875\n","reward_std: 0.0\n","reward_max: 306.45379638671875\n","reward_min: 306.45379638671875\n","total_envstep_count: 653800\n","total_train_sample_count: 15691200\n","total_episode_count: 700\n","INFO:root:Training: Train Iter(653600)\tEnv Step(653824)\tLoss(-25.979)\n","INFO:root:Training: Train Iter(653700)\tEnv Step(653952)\tLoss(-26.523)\n","INFO:root:Training: Train Iter(653800)\tEnv Step(654016)\tLoss(-21.100)\n","INFO:root:Training: Train Iter(653900)\tEnv Step(654144)\tLoss(-19.774)\n","INFO:root:Training: Train Iter(654000)\tEnv Step(654208)\tLoss(-26.972)\n","INFO:root:Training: Train Iter(654100)\tEnv Step(654336)\tLoss(-23.664)\n","INFO:root:Training: Train Iter(654200)\tEnv Step(654400)\tLoss(-26.399)\n","INFO:root:Training: Train Iter(654300)\tEnv Step(654528)\tLoss(-19.890)\n","INFO:root:Evaluation: Train Iter(654336) Env Step(654528) Episode Return(305.479) \n","INFO:root:Training: Train Iter(654400)\tEnv Step(654656)\tLoss(-25.480)\n","INFO:root:Training: Train Iter(654500)\tEnv Step(654720)\tLoss(-21.341)\n","INFO:root:Training: Train Iter(654600)\tEnv Step(654848)\tLoss(-18.488)\n","INFO:root:Training: Train Iter(654700)\tEnv Step(654912)\tLoss(-24.575)\n","INFO:root:Training: Train Iter(654800)\tEnv Step(655040)\tLoss(-26.756)\n","INFO:root:Training: Train Iter(654900)\tEnv Step(655104)\tLoss(-29.890)\n","INFO:root:Training: Train Iter(655000)\tEnv Step(655232)\tLoss(-27.045)\n","INFO:root:Training: Train Iter(655100)\tEnv Step(655296)\tLoss(-25.856)\n","INFO:root:Training: Train Iter(655200)\tEnv Step(655424)\tLoss(-24.837)\n","INFO:root:Training: Train Iter(655300)\tEnv Step(655552)\tLoss(-25.352)\n","INFO:root:Evaluation: Train Iter(655360) Env Step(655552) Episode Return(306.057) \n","INFO:root:Training: Train Iter(655400)\tEnv Step(655616)\tLoss(-22.672)\n","INFO:root:Training: Train Iter(655500)\tEnv Step(655744)\tLoss(-27.687)\n","INFO:root:Training: Train Iter(655600)\tEnv Step(655808)\tLoss(-28.336)\n","INFO:root:Training: Train Iter(655700)\tEnv Step(655936)\tLoss(-25.435)\n","INFO:root:Training: Train Iter(655800)\tEnv Step(656000)\tLoss(-26.645)\n","INFO:root:Training: Train Iter(655900)\tEnv Step(656128)\tLoss(-24.582)\n","INFO:root:Training: Train Iter(656000)\tEnv Step(656256)\tLoss(-28.155)\n","INFO:root:Training: Train Iter(656100)\tEnv Step(656320)\tLoss(-28.436)\n","INFO:root:Training: Train Iter(656200)\tEnv Step(656448)\tLoss(-17.634)\n","INFO:root:Training: Train Iter(656300)\tEnv Step(656512)\tLoss(-24.073)\n","INFO:root:Evaluation: Train Iter(656384) Env Step(656576) Episode Return(313.299) \n","INFO:root:Training: Train Iter(656400)\tEnv Step(656640)\tLoss(-26.114)\n","INFO:root:Training: Train Iter(656500)\tEnv Step(656704)\tLoss(-25.180)\n","INFO:root:Training: Train Iter(656600)\tEnv Step(656832)\tLoss(-20.019)\n","INFO:root:Training: Train Iter(656700)\tEnv Step(656896)\tLoss(-18.658)\n","INFO:root:Training: Train Iter(656800)\tEnv Step(657024)\tLoss(-26.267)\n","INFO:root:Training: Train Iter(656900)\tEnv Step(657152)\tLoss(-28.103)\n","INFO:root:Training: Train Iter(657000)\tEnv Step(657216)\tLoss(-24.397)\n","INFO:root:Training: Train Iter(657100)\tEnv Step(657344)\tLoss(-20.848)\n","INFO:root:Training: Train Iter(657200)\tEnv Step(657408)\tLoss(-25.502)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82177\n","train_sample_count: 1972248\n","avg_envstep_per_episode: 82177.0\n","avg_sample_per_episode: 1972248.0\n","avg_envstep_per_sec: 8510.589282379007\n","avg_train_sample_per_sec: 204254.1427770962\n","avg_episode_per_sec: 0.1035641272178226\n","reward_mean: 306.8870849609375\n","reward_std: 0.0\n","reward_max: 306.8870849609375\n","reward_min: 306.8870849609375\n","total_envstep_count: 657416\n","total_train_sample_count: 15777984\n","total_episode_count: 701\n","INFO:root:Training: Train Iter(657300)\tEnv Step(657536)\tLoss(-24.481)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82193\n","train_sample_count: 1972632\n","avg_envstep_per_episode: 82193.0\n","avg_sample_per_episode: 1972632.0\n","avg_envstep_per_sec: 8861.216016625276\n","avg_train_sample_per_sec: 212669.18439900663\n","avg_episode_per_sec: 0.10780986235598258\n","reward_mean: 303.94842529296875\n","reward_std: 0.0\n","reward_max: 303.94842529296875\n","reward_min: 303.94842529296875\n","total_envstep_count: 657544\n","total_train_sample_count: 15781056\n","total_episode_count: 702\n","INFO:root:Training: Train Iter(657400)\tEnv Step(657600)\tLoss(-25.540)\n","INFO:root:Evaluation: Train Iter(657408) Env Step(657600) Episode Return(309.581) \n","INFO:root:Training: Train Iter(657500)\tEnv Step(657728)\tLoss(-24.990)\n","INFO:root:Training: Train Iter(657600)\tEnv Step(657856)\tLoss(-28.187)\n","INFO:root:Training: Train Iter(657700)\tEnv Step(657920)\tLoss(-25.974)\n","INFO:root:Training: Train Iter(657800)\tEnv Step(658048)\tLoss(-26.384)\n","INFO:root:Training: Train Iter(657900)\tEnv Step(658112)\tLoss(-25.729)\n","INFO:root:Training: Train Iter(658000)\tEnv Step(658240)\tLoss(-23.639)\n","INFO:root:Training: Train Iter(658100)\tEnv Step(658304)\tLoss(-25.106)\n","INFO:root:Training: Train Iter(658200)\tEnv Step(658432)\tLoss(-26.499)\n","INFO:root:Training: Train Iter(658300)\tEnv Step(658496)\tLoss(-26.515)\n","INFO:root:Training: Train Iter(658400)\tEnv Step(658624)\tLoss(-21.205)\n","INFO:root:Evaluation: Train Iter(658432) Env Step(658624) Episode Return(221.300) \n","INFO:root:Training: Train Iter(658500)\tEnv Step(658752)\tLoss(-30.074)\n","INFO:root:Training: Train Iter(658600)\tEnv Step(658816)\tLoss(-30.343)\n","INFO:root:Training: Train Iter(658700)\tEnv Step(658944)\tLoss(-26.878)\n","INFO:root:Training: Train Iter(658800)\tEnv Step(659008)\tLoss(-21.021)\n","INFO:root:Training: Train Iter(658900)\tEnv Step(659136)\tLoss(-28.493)\n","INFO:root:Training: Train Iter(659000)\tEnv Step(659200)\tLoss(-27.210)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82403\n","train_sample_count: 1977672\n","avg_envstep_per_episode: 82403.0\n","avg_sample_per_episode: 1977672.0\n","avg_envstep_per_sec: 8705.38260186208\n","avg_train_sample_per_sec: 208929.1824446899\n","avg_episode_per_sec: 0.10564400084780991\n","reward_mean: 304.60882568359375\n","reward_std: 0.0\n","reward_max: 304.60882568359375\n","reward_min: 304.60882568359375\n","total_envstep_count: 659224\n","total_train_sample_count: 15821376\n","total_episode_count: 703\n","INFO:root:Training: Train Iter(659100)\tEnv Step(659328)\tLoss(-26.636)\n","INFO:root:Training: Train Iter(659200)\tEnv Step(659456)\tLoss(-25.325)\n","INFO:root:Training: Train Iter(659300)\tEnv Step(659520)\tLoss(-17.168)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82443\n","train_sample_count: 1978632\n","avg_envstep_per_episode: 82443.0\n","avg_sample_per_episode: 1978632.0\n","avg_envstep_per_sec: 8709.46820730191\n","avg_train_sample_per_sec: 209027.23697524585\n","avg_episode_per_sec: 0.10564230082968731\n","reward_mean: 307.0732727050781\n","reward_std: 0.0\n","reward_max: 307.0732727050781\n","reward_min: 307.0732727050781\n","total_envstep_count: 659544\n","total_train_sample_count: 15829056\n","total_episode_count: 704\n","INFO:root:Training: Train Iter(659400)\tEnv Step(659648)\tLoss(-27.921)\n","INFO:root:Evaluation: Train Iter(659456) Env Step(659648) Episode Return(309.323) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82449\n","train_sample_count: 1978776\n","avg_envstep_per_episode: 82449.0\n","avg_sample_per_episode: 1978776.0\n","avg_envstep_per_sec: 6114.699446960271\n","avg_train_sample_per_sec: 146752.78672704654\n","avg_episode_per_sec: 0.07416341552911826\n","reward_mean: 304.7687683105469\n","reward_std: 0.0\n","reward_max: 304.7687683105469\n","reward_min: 304.7687683105469\n","total_envstep_count: 659656\n","total_train_sample_count: 15831744\n","total_episode_count: 705\n","INFO:root:Training: Train Iter(659500)\tEnv Step(659712)\tLoss(-20.210)\n","INFO:root:Training: Train Iter(659600)\tEnv Step(659840)\tLoss(-26.688)\n","INFO:root:Training: Train Iter(659700)\tEnv Step(659904)\tLoss(-23.396)\n","INFO:root:Training: Train Iter(659800)\tEnv Step(660032)\tLoss(-25.384)\n","INFO:root:Training: Train Iter(659900)\tEnv Step(660096)\tLoss(-27.815)\n","INFO:root:Training: Train Iter(660000)\tEnv Step(660224)\tLoss(-27.657)\n","INFO:root:Training: Train Iter(660100)\tEnv Step(660352)\tLoss(-19.346)\n","INFO:root:Training: Train Iter(660200)\tEnv Step(660416)\tLoss(-23.557)\n","INFO:root:Training: Train Iter(660300)\tEnv Step(660544)\tLoss(-27.530)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82569\n","train_sample_count: 1981656\n","avg_envstep_per_episode: 82569.0\n","avg_sample_per_episode: 1981656.0\n","avg_envstep_per_sec: 8002.509951105084\n","avg_train_sample_per_sec: 192060.238826522\n","avg_episode_per_sec: 0.09691906104113025\n","reward_mean: 306.3739929199219\n","reward_std: 0.0\n","reward_max: 306.3739929199219\n","reward_min: 306.3739929199219\n","total_envstep_count: 660552\n","total_train_sample_count: 15853248\n","total_episode_count: 706\n","INFO:root:Training: Train Iter(660400)\tEnv Step(660608)\tLoss(-25.134)\n","INFO:root:Evaluation: Train Iter(660480) Env Step(660672) Episode Return(308.368) \n","INFO:root:Training: Train Iter(660500)\tEnv Step(660736)\tLoss(-29.437)\n","INFO:root:Training: Train Iter(660600)\tEnv Step(660800)\tLoss(-5.209)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82607\n","train_sample_count: 1982568\n","avg_envstep_per_episode: 82607.0\n","avg_sample_per_episode: 1982568.0\n","avg_envstep_per_sec: 8773.357634282771\n","avg_train_sample_per_sec: 210560.5832227865\n","avg_episode_per_sec: 0.10620598295886269\n","reward_mean: 306.7138977050781\n","reward_std: 0.0\n","reward_max: 306.7138977050781\n","reward_min: 306.7138977050781\n","total_envstep_count: 660856\n","total_train_sample_count: 15860544\n","total_episode_count: 707\n","INFO:root:Training: Train Iter(660700)\tEnv Step(660928)\tLoss(-27.036)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 82628\n","train_sample_count: 1983072\n","avg_envstep_per_episode: 82628.0\n","avg_sample_per_episode: 1983072.0\n","avg_envstep_per_sec: 8669.51097961793\n","avg_train_sample_per_sec: 208068.26351083032\n","avg_episode_per_sec: 0.10492219319864851\n","reward_mean: 305.6465148925781\n","reward_std: 0.0\n","reward_max: 305.6465148925781\n","reward_min: 305.6465148925781\n","total_envstep_count: 661024\n","total_train_sample_count: 15864576\n","total_episode_count: 708\n","INFO:root:Training: Train Iter(660800)\tEnv Step(661056)\tLoss(-21.370)\n","INFO:root:Training: Train Iter(660900)\tEnv Step(661120)\tLoss(-27.649)\n","INFO:root:Training: Train Iter(661000)\tEnv Step(661248)\tLoss(-21.466)\n","INFO:root:Training: Train Iter(661100)\tEnv Step(661312)\tLoss(-26.311)\n","INFO:root:Training: Train Iter(661200)\tEnv Step(661440)\tLoss(-22.280)\n","INFO:root:Training: Train Iter(661300)\tEnv Step(661504)\tLoss(-28.679)\n","INFO:root:Training: Train Iter(661400)\tEnv Step(661632)\tLoss(-26.001)\n","INFO:root:Training: Train Iter(661500)\tEnv Step(661696)\tLoss(-11.863)\n","INFO:root:Evaluation: Train Iter(661504) Env Step(661696) Episode Return(309.799) \n","INFO:root:Training: Train Iter(661600)\tEnv Step(661824)\tLoss(-28.380)\n","INFO:root:Training: Train Iter(661700)\tEnv Step(661952)\tLoss(-27.113)\n","INFO:root:Training: Train Iter(661800)\tEnv Step(662016)\tLoss(-28.921)\n","INFO:root:Training: Train Iter(661900)\tEnv Step(662144)\tLoss(-23.232)\n","INFO:root:Training: Train Iter(662000)\tEnv Step(662208)\tLoss(-27.243)\n","INFO:root:Training: Train Iter(662100)\tEnv Step(662336)\tLoss(-24.716)\n","INFO:root:Training: Train Iter(662200)\tEnv Step(662400)\tLoss(-28.260)\n","INFO:root:Training: Train Iter(662300)\tEnv Step(662528)\tLoss(-26.235)\n","INFO:root:Training: Train Iter(662400)\tEnv Step(662656)\tLoss(-26.047)\n","INFO:root:Training: Train Iter(662500)\tEnv Step(662720)\tLoss(-18.537)\n","INFO:root:Evaluation: Train Iter(662528) Env Step(662720) Episode Return(308.649) \n","INFO:root:Training: Train Iter(662600)\tEnv Step(662848)\tLoss(-26.876)\n","INFO:root:Training: Train Iter(662700)\tEnv Step(662912)\tLoss(-26.441)\n","INFO:root:Training: Train Iter(662800)\tEnv Step(663040)\tLoss(-26.954)\n","INFO:root:Training: Train Iter(662900)\tEnv Step(663104)\tLoss(-29.473)\n","INFO:root:Training: Train Iter(663000)\tEnv Step(663232)\tLoss(-26.420)\n","INFO:root:Training: Train Iter(663100)\tEnv Step(663296)\tLoss(-26.269)\n","INFO:root:Training: Train Iter(663200)\tEnv Step(663424)\tLoss(-21.673)\n","INFO:root:Training: Train Iter(663300)\tEnv Step(663552)\tLoss(-27.683)\n","INFO:root:Training: Train Iter(663400)\tEnv Step(663616)\tLoss(-26.141)\n","INFO:root:Training: Train Iter(663500)\tEnv Step(663744)\tLoss(-29.074)\n","INFO:root:Evaluation: Train Iter(663552) Env Step(663744) Episode Return(311.842) \n","INFO:root:Training: Train Iter(663600)\tEnv Step(663808)\tLoss(-26.187)\n","INFO:root:Training: Train Iter(663700)\tEnv Step(663936)\tLoss(-24.059)\n","INFO:root:Training: Train Iter(663800)\tEnv Step(664000)\tLoss(-25.350)\n","INFO:root:Training: Train Iter(663900)\tEnv Step(664128)\tLoss(-13.406)\n","INFO:root:Training: Train Iter(664000)\tEnv Step(664256)\tLoss(-25.867)\n","INFO:root:Training: Train Iter(664100)\tEnv Step(664320)\tLoss(-13.369)\n","INFO:root:Training: Train Iter(664200)\tEnv Step(664448)\tLoss(-26.465)\n","INFO:root:Training: Train Iter(664300)\tEnv Step(664512)\tLoss(-24.629)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83070\n","train_sample_count: 1993680\n","avg_envstep_per_episode: 83070.0\n","avg_sample_per_episode: 1993680.0\n","avg_envstep_per_sec: 8862.906984629484\n","avg_train_sample_per_sec: 212709.76763110762\n","avg_episode_per_sec: 0.10669203063235204\n","reward_mean: 307.3304748535156\n","reward_std: 0.0\n","reward_max: 307.3304748535156\n","reward_min: 307.3304748535156\n","total_envstep_count: 664560\n","total_train_sample_count: 15949440\n","total_episode_count: 709\n","INFO:root:Training: Train Iter(664400)\tEnv Step(664640)\tLoss(-28.258)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83073\n","train_sample_count: 1993752\n","avg_envstep_per_episode: 83073.0\n","avg_sample_per_episode: 1993752.0\n","avg_envstep_per_sec: 8512.163660001945\n","avg_train_sample_per_sec: 204291.92784004664\n","avg_episode_per_sec: 0.10246606791619352\n","reward_mean: 306.4349365234375\n","reward_std: 0.0\n","reward_max: 306.4349365234375\n","reward_min: 306.4349365234375\n","total_envstep_count: 664648\n","total_train_sample_count: 15951552\n","total_episode_count: 710\n","INFO:root:Training: Train Iter(664500)\tEnv Step(664704)\tLoss(-27.749)\n","INFO:root:Evaluation: Train Iter(664576) Env Step(664768) Episode Return(313.958) \n","INFO:root:Training: Train Iter(664600)\tEnv Step(664832)\tLoss(-28.605)\n","INFO:root:Training: Train Iter(664700)\tEnv Step(664896)\tLoss(-26.827)\n","INFO:root:Training: Train Iter(664800)\tEnv Step(665024)\tLoss(-26.679)\n","INFO:root:Training: Train Iter(664900)\tEnv Step(665152)\tLoss(-30.137)\n","INFO:root:Training: Train Iter(665000)\tEnv Step(665216)\tLoss(-24.832)\n","INFO:root:Training: Train Iter(665100)\tEnv Step(665344)\tLoss(-25.006)\n","INFO:root:Training: Train Iter(665200)\tEnv Step(665408)\tLoss(-27.181)\n","INFO:root:Training: Train Iter(665300)\tEnv Step(665536)\tLoss(-22.818)\n","INFO:root:Training: Train Iter(665400)\tEnv Step(665600)\tLoss(-28.788)\n","INFO:root:Training: Train Iter(665500)\tEnv Step(665728)\tLoss(-21.317)\n","INFO:root:Evaluation: Train Iter(665600) Env Step(665792) Episode Return(310.642) \n","INFO:root:Training: Train Iter(665600)\tEnv Step(665856)\tLoss(-15.843)\n","INFO:root:Training: Train Iter(665700)\tEnv Step(665920)\tLoss(-24.991)\n","INFO:root:Training: Train Iter(665800)\tEnv Step(666048)\tLoss(-23.823)\n","INFO:root:Training: Train Iter(665900)\tEnv Step(666112)\tLoss(-25.282)\n","INFO:root:Training: Train Iter(666000)\tEnv Step(666240)\tLoss(-28.948)\n","INFO:root:Training: Train Iter(666100)\tEnv Step(666304)\tLoss(-25.543)\n","INFO:root:Training: Train Iter(666200)\tEnv Step(666432)\tLoss(-25.972)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83309\n","train_sample_count: 1999416\n","avg_envstep_per_episode: 83309.0\n","avg_sample_per_episode: 1999416.0\n","avg_envstep_per_sec: 8707.311691072411\n","avg_train_sample_per_sec: 208975.48058573785\n","avg_episode_per_sec: 0.10451825962467934\n","reward_mean: 307.61114501953125\n","reward_std: 0.0\n","reward_max: 307.61114501953125\n","reward_min: 307.61114501953125\n","total_envstep_count: 666472\n","total_train_sample_count: 15995328\n","total_episode_count: 711\n","INFO:root:Training: Train Iter(666300)\tEnv Step(666496)\tLoss(-29.081)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83321\n","train_sample_count: 1999704\n","avg_envstep_per_episode: 83321.0\n","avg_sample_per_episode: 1999704.0\n","avg_envstep_per_sec: 6117.543387499704\n","avg_train_sample_per_sec: 146821.04129999288\n","avg_episode_per_sec: 0.0734213870152747\n","reward_mean: 308.18365478515625\n","reward_std: 0.0\n","reward_max: 308.18365478515625\n","reward_min: 308.18365478515625\n","total_envstep_count: 666568\n","total_train_sample_count: 15997632\n","total_episode_count: 712\n","INFO:root:Training: Train Iter(666400)\tEnv Step(666624)\tLoss(-25.947)\n","INFO:root:Training: Train Iter(666500)\tEnv Step(666752)\tLoss(-27.949)\n","INFO:root:Training: Train Iter(666600)\tEnv Step(666816)\tLoss(-27.432)\n","INFO:root:Evaluation: Train Iter(666624) Env Step(666816) Episode Return(312.434) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83355\n","train_sample_count: 2000520\n","avg_envstep_per_episode: 83355.0\n","avg_sample_per_episode: 2000520.0\n","avg_envstep_per_sec: 8703.3918232929\n","avg_train_sample_per_sec: 208881.40375902964\n","avg_episode_per_sec: 0.10441355435538242\n","reward_mean: 306.173828125\n","reward_std: 0.0\n","reward_max: 306.173828125\n","reward_min: 306.173828125\n","total_envstep_count: 666840\n","total_train_sample_count: 16004160\n","total_episode_count: 713\n","INFO:root:Training: Train Iter(666700)\tEnv Step(666944)\tLoss(-20.964)\n","INFO:root:Training: Train Iter(666800)\tEnv Step(667008)\tLoss(-27.515)\n","INFO:root:Training: Train Iter(666900)\tEnv Step(667136)\tLoss(-27.184)\n","INFO:root:Training: Train Iter(667000)\tEnv Step(667200)\tLoss(-21.741)\n","INFO:root:Training: Train Iter(667100)\tEnv Step(667328)\tLoss(-20.979)\n","INFO:root:Training: Train Iter(667200)\tEnv Step(667456)\tLoss(-21.669)\n","INFO:root:Training: Train Iter(667300)\tEnv Step(667520)\tLoss(-30.117)\n","INFO:root:Training: Train Iter(667400)\tEnv Step(667648)\tLoss(-27.160)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83464\n","train_sample_count: 2003136\n","avg_envstep_per_episode: 83464.0\n","avg_sample_per_episode: 2003136.0\n","avg_envstep_per_sec: 8001.302849774621\n","avg_train_sample_per_sec: 192031.2683945909\n","avg_episode_per_sec: 0.09586531737964417\n","reward_mean: 307.0867004394531\n","reward_std: 0.0\n","reward_max: 307.0867004394531\n","reward_min: 307.0867004394531\n","total_envstep_count: 667712\n","total_train_sample_count: 16025088\n","total_episode_count: 714\n","INFO:root:Training: Train Iter(667500)\tEnv Step(667712)\tLoss(-26.438)\n","INFO:root:Training: Train Iter(667600)\tEnv Step(667840)\tLoss(-29.390)\n","INFO:root:Evaluation: Train Iter(667648) Env Step(667840) Episode Return(314.383) \n","INFO:root:Training: Train Iter(667700)\tEnv Step(667904)\tLoss(-22.865)\n","INFO:root:collect end:\n","episode_count: 2\n","envstep_count: 166984\n","train_sample_count: 4007616\n","avg_envstep_per_episode: 83492.0\n","avg_sample_per_episode: 2003808.0\n","avg_envstep_per_sec: 8724.420017967392\n","avg_train_sample_per_sec: 209386.0804312174\n","avg_episode_per_sec: 0.10449408348066151\n","reward_mean: 309.17857360839844\n","reward_std: 0.1602020263671875\n","reward_max: 309.3387756347656\n","reward_min: 309.01837158203125\n","total_envstep_count: 667936\n","total_train_sample_count: 16030464\n","total_episode_count: 716\n","INFO:root:Training: Train Iter(667800)\tEnv Step(668032)\tLoss(-29.004)\n","INFO:root:Training: Train Iter(667900)\tEnv Step(668096)\tLoss(-26.810)\n","INFO:root:Training: Train Iter(668000)\tEnv Step(668224)\tLoss(-26.699)\n","INFO:root:Training: Train Iter(668100)\tEnv Step(668352)\tLoss(-29.044)\n","INFO:root:Training: Train Iter(668200)\tEnv Step(668416)\tLoss(-23.340)\n","INFO:root:Training: Train Iter(668300)\tEnv Step(668544)\tLoss(-29.936)\n","INFO:root:Training: Train Iter(668400)\tEnv Step(668608)\tLoss(-26.809)\n","INFO:root:Training: Train Iter(668500)\tEnv Step(668736)\tLoss(-25.777)\n","INFO:root:Training: Train Iter(668600)\tEnv Step(668800)\tLoss(-17.920)\n","INFO:root:Evaluation: Train Iter(668672) Env Step(668864) Episode Return(314.706) \n","INFO:root:Training: Train Iter(668700)\tEnv Step(668928)\tLoss(-28.618)\n","INFO:root:Training: Train Iter(668800)\tEnv Step(669056)\tLoss(-24.407)\n","INFO:root:Training: Train Iter(668900)\tEnv Step(669120)\tLoss(-24.333)\n","INFO:root:Training: Train Iter(669000)\tEnv Step(669248)\tLoss(-25.491)\n","INFO:root:Training: Train Iter(669100)\tEnv Step(669312)\tLoss(-20.437)\n","INFO:root:Training: Train Iter(669200)\tEnv Step(669440)\tLoss(-30.780)\n","INFO:root:Training: Train Iter(669300)\tEnv Step(669504)\tLoss(-29.131)\n","INFO:root:Training: Train Iter(669400)\tEnv Step(669632)\tLoss(-26.580)\n","INFO:root:Training: Train Iter(669500)\tEnv Step(669696)\tLoss(-21.644)\n","INFO:root:Training: Train Iter(669600)\tEnv Step(669824)\tLoss(-28.015)\n","INFO:root:Evaluation: Train Iter(669696) Env Step(669888) Episode Return(312.003) \n","INFO:root:Training: Train Iter(669700)\tEnv Step(669952)\tLoss(-27.356)\n","INFO:root:Training: Train Iter(669800)\tEnv Step(670016)\tLoss(-27.657)\n","INFO:root:Training: Train Iter(669900)\tEnv Step(670144)\tLoss(-25.052)\n","INFO:root:Training: Train Iter(670000)\tEnv Step(670208)\tLoss(-25.510)\n","INFO:root:Training: Train Iter(670100)\tEnv Step(670336)\tLoss(-27.131)\n","INFO:root:Training: Train Iter(670200)\tEnv Step(670400)\tLoss(-18.948)\n","INFO:root:Training: Train Iter(670300)\tEnv Step(670528)\tLoss(-28.348)\n","INFO:root:Training: Train Iter(670400)\tEnv Step(670656)\tLoss(-25.917)\n","INFO:root:Training: Train Iter(670500)\tEnv Step(670720)\tLoss(-27.252)\n","INFO:root:Training: Train Iter(670600)\tEnv Step(670848)\tLoss(-26.238)\n","INFO:root:Training: Train Iter(670700)\tEnv Step(670912)\tLoss(-26.993)\n","INFO:root:Evaluation: Train Iter(670720) Env Step(670912) Episode Return(-27.706) \n","INFO:root:Training: Train Iter(670800)\tEnv Step(671040)\tLoss(-28.745)\n","INFO:root:Training: Train Iter(670900)\tEnv Step(671104)\tLoss(-25.858)\n","INFO:root:Training: Train Iter(671000)\tEnv Step(671232)\tLoss(-27.786)\n","INFO:root:Training: Train Iter(671100)\tEnv Step(671296)\tLoss(-26.760)\n","INFO:root:Training: Train Iter(671200)\tEnv Step(671424)\tLoss(-28.759)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83942\n","train_sample_count: 2014608\n","avg_envstep_per_episode: 83942.0\n","avg_sample_per_episode: 2014608.0\n","avg_envstep_per_sec: 8515.100628406095\n","avg_train_sample_per_sec: 204362.41508174627\n","avg_episode_per_sec: 0.10144028767966089\n","reward_mean: 308.46173095703125\n","reward_std: 0.0\n","reward_max: 308.46173095703125\n","reward_min: 308.46173095703125\n","total_envstep_count: 671536\n","total_train_sample_count: 16116864\n","total_episode_count: 717\n","INFO:root:Training: Train Iter(671300)\tEnv Step(671552)\tLoss(-25.277)\n","INFO:root:Training: Train Iter(671400)\tEnv Step(671616)\tLoss(-26.160)\n","INFO:root:Training: Train Iter(671500)\tEnv Step(671744)\tLoss(-23.196)\n","INFO:root:Training: Train Iter(671600)\tEnv Step(671808)\tLoss(-25.755)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 83978\n","train_sample_count: 2015472\n","avg_envstep_per_episode: 83978.0\n","avg_sample_per_episode: 2015472.0\n","avg_envstep_per_sec: 8865.146477117694\n","avg_train_sample_per_sec: 212763.51545082466\n","avg_episode_per_sec: 0.10556510606489432\n","reward_mean: 308.7333068847656\n","reward_std: 0.0\n","reward_max: 308.7333068847656\n","reward_min: 308.7333068847656\n","total_envstep_count: 671824\n","total_train_sample_count: 16123776\n","total_episode_count: 718\n","INFO:root:Training: Train Iter(671700)\tEnv Step(671936)\tLoss(-25.739)\n","INFO:root:Evaluation: Train Iter(671744) Env Step(671936) Episode Return(311.254) \n","INFO:root:Training: Train Iter(671800)\tEnv Step(672000)\tLoss(-26.069)\n","INFO:root:Training: Train Iter(671900)\tEnv Step(672128)\tLoss(-29.016)\n","INFO:root:Training: Train Iter(672000)\tEnv Step(672256)\tLoss(-28.513)\n","INFO:root:Training: Train Iter(672100)\tEnv Step(672320)\tLoss(-22.527)\n","INFO:root:Training: Train Iter(672200)\tEnv Step(672448)\tLoss(-28.577)\n","INFO:root:Training: Train Iter(672300)\tEnv Step(672512)\tLoss(-27.675)\n","INFO:root:Training: Train Iter(672400)\tEnv Step(672640)\tLoss(-27.011)\n","INFO:root:Training: Train Iter(672500)\tEnv Step(672704)\tLoss(-19.838)\n","INFO:root:Training: Train Iter(672600)\tEnv Step(672832)\tLoss(-27.123)\n","INFO:root:Training: Train Iter(672700)\tEnv Step(672896)\tLoss(-25.671)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84116\n","train_sample_count: 2018784\n","avg_envstep_per_episode: 84116.0\n","avg_sample_per_episode: 2018784.0\n","avg_envstep_per_sec: 8866.154297512043\n","avg_train_sample_per_sec: 212787.70314028906\n","avg_episode_per_sec: 0.10540389815863858\n","reward_mean: -113.74601745605469\n","reward_std: 0.0\n","reward_max: -113.74601745605469\n","reward_min: -113.74601745605469\n","total_envstep_count: 672928\n","total_train_sample_count: 16150272\n","total_episode_count: 719\n","INFO:root:Evaluation: Train Iter(672768) Env Step(672960) Episode Return(314.491) \n","INFO:root:Training: Train Iter(672800)\tEnv Step(673024)\tLoss(-25.088)\n","INFO:root:Training: Train Iter(672900)\tEnv Step(673152)\tLoss(-24.433)\n","INFO:root:Training: Train Iter(673000)\tEnv Step(673216)\tLoss(-27.004)\n","INFO:root:Training: Train Iter(673100)\tEnv Step(673344)\tLoss(-30.539)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84173\n","train_sample_count: 2020152\n","avg_envstep_per_episode: 84173.0\n","avg_sample_per_episode: 2020152.0\n","avg_envstep_per_sec: 8708.894647121033\n","avg_train_sample_per_sec: 209013.47153090476\n","avg_episode_per_sec: 0.10346423018213717\n","reward_mean: 309.75299072265625\n","reward_std: 0.0\n","reward_max: 309.75299072265625\n","reward_min: 309.75299072265625\n","total_envstep_count: 673384\n","total_train_sample_count: 16161216\n","total_episode_count: 720\n","INFO:root:Training: Train Iter(673200)\tEnv Step(673408)\tLoss(-28.427)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84189\n","train_sample_count: 2020536\n","avg_envstep_per_episode: 84189.0\n","avg_sample_per_episode: 2020536.0\n","avg_envstep_per_sec: 6119.522550639506\n","avg_train_sample_per_sec: 146868.54121534814\n","avg_episode_per_sec: 0.07268791113612831\n","reward_mean: 309.6748962402344\n","reward_std: 0.0\n","reward_max: 309.6748962402344\n","reward_min: 309.6748962402344\n","total_envstep_count: 673512\n","total_train_sample_count: 16164288\n","total_episode_count: 721\n","INFO:root:Training: Train Iter(673300)\tEnv Step(673536)\tLoss(-24.017)\n","INFO:root:Training: Train Iter(673400)\tEnv Step(673600)\tLoss(-26.295)\n","INFO:root:Training: Train Iter(673500)\tEnv Step(673728)\tLoss(-18.323)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84229\n","train_sample_count: 2021496\n","avg_envstep_per_episode: 84229.0\n","avg_sample_per_episode: 2021496.0\n","avg_envstep_per_sec: 8707.313807973715\n","avg_train_sample_per_sec: 208975.53139136915\n","avg_episode_per_sec: 0.10337667321200197\n","reward_mean: 308.7265319824219\n","reward_std: 0.0\n","reward_max: 308.7265319824219\n","reward_min: 308.7265319824219\n","total_envstep_count: 673832\n","total_train_sample_count: 16171968\n","total_episode_count: 722\n","INFO:root:Training: Train Iter(673600)\tEnv Step(673856)\tLoss(-26.427)\n","INFO:root:Training: Train Iter(673700)\tEnv Step(673920)\tLoss(-26.073)\n","INFO:root:Evaluation: Train Iter(673792) Env Step(673984) Episode Return(308.399) \n","INFO:root:Training: Train Iter(673800)\tEnv Step(674048)\tLoss(-25.666)\n","INFO:root:Training: Train Iter(673900)\tEnv Step(674112)\tLoss(-24.058)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84280\n","train_sample_count: 2022720\n","avg_envstep_per_episode: 84280.0\n","avg_sample_per_episode: 2022720.0\n","avg_envstep_per_sec: 6119.345563387668\n","avg_train_sample_per_sec: 146864.29352130403\n","avg_episode_per_sec: 0.07260732752002455\n","reward_mean: -108.782958984375\n","reward_std: 0.0\n","reward_max: -108.782958984375\n","reward_min: -108.782958984375\n","total_envstep_count: 674240\n","total_train_sample_count: 16181760\n","total_episode_count: 723\n","INFO:root:Training: Train Iter(674000)\tEnv Step(674240)\tLoss(-25.548)\n","INFO:root:Training: Train Iter(674100)\tEnv Step(674304)\tLoss(-20.046)\n","INFO:root:Training: Train Iter(674200)\tEnv Step(674432)\tLoss(-28.986)\n","INFO:root:Training: Train Iter(674300)\tEnv Step(674496)\tLoss(-30.799)\n","INFO:root:Training: Train Iter(674400)\tEnv Step(674624)\tLoss(-24.457)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84337\n","train_sample_count: 2024088\n","avg_envstep_per_episode: 84337.0\n","avg_sample_per_episode: 2024088.0\n","avg_envstep_per_sec: 8003.201150094669\n","avg_train_sample_per_sec: 192076.82760227207\n","avg_episode_per_sec: 0.0948954924895914\n","reward_mean: 308.6777038574219\n","reward_std: 0.0\n","reward_max: 308.6777038574219\n","reward_min: 308.6777038574219\n","total_envstep_count: 674696\n","total_train_sample_count: 16192704\n","total_episode_count: 724\n","INFO:root:Training: Train Iter(674500)\tEnv Step(674752)\tLoss(-19.271)\n","INFO:root:Training: Train Iter(674600)\tEnv Step(674816)\tLoss(-28.490)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84363\n","train_sample_count: 2024712\n","avg_envstep_per_episode: 84363.0\n","avg_sample_per_episode: 2024712.0\n","avg_envstep_per_sec: 8779.515410418877\n","avg_train_sample_per_sec: 210708.36985005302\n","avg_episode_per_sec: 0.10406831680261341\n","reward_mean: 308.9016418457031\n","reward_std: 0.0\n","reward_max: 308.9016418457031\n","reward_min: 308.9016418457031\n","total_envstep_count: 674904\n","total_train_sample_count: 16197696\n","total_episode_count: 725\n","INFO:root:Training: Train Iter(674700)\tEnv Step(674944)\tLoss(-27.315)\n","INFO:root:Training: Train Iter(674800)\tEnv Step(675008)\tLoss(-15.863)\n","INFO:root:Evaluation: Train Iter(674816) Env Step(675008) Episode Return(311.409) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84373\n","train_sample_count: 2024952\n","avg_envstep_per_episode: 84373.0\n","avg_sample_per_episode: 2024952.0\n","avg_envstep_per_sec: 8671.81732606062\n","avg_train_sample_per_sec: 208123.61582545485\n","avg_episode_per_sec: 0.10277953049033008\n","reward_mean: 307.6784362792969\n","reward_std: 0.0\n","reward_max: 307.6784362792969\n","reward_min: 307.6784362792969\n","total_envstep_count: 675016\n","total_train_sample_count: 16200384\n","total_episode_count: 726\n","INFO:root:Training: Train Iter(674900)\tEnv Step(675136)\tLoss(-29.112)\n","INFO:root:Training: Train Iter(675000)\tEnv Step(675200)\tLoss(-25.961)\n","INFO:root:Training: Train Iter(675100)\tEnv Step(675328)\tLoss(-24.656)\n","INFO:root:Training: Train Iter(675200)\tEnv Step(675456)\tLoss(-27.213)\n","INFO:root:Training: Train Iter(675300)\tEnv Step(675520)\tLoss(-24.185)\n","INFO:root:Training: Train Iter(675400)\tEnv Step(675648)\tLoss(-21.466)\n","INFO:root:Training: Train Iter(675500)\tEnv Step(675712)\tLoss(-29.334)\n","INFO:root:Training: Train Iter(675600)\tEnv Step(675840)\tLoss(-25.256)\n","INFO:root:Training: Train Iter(675700)\tEnv Step(675904)\tLoss(-28.086)\n","INFO:root:Training: Train Iter(675800)\tEnv Step(676032)\tLoss(-27.795)\n","INFO:root:Evaluation: Train Iter(675840) Env Step(676032) Episode Return(314.398) \n","INFO:root:Training: Train Iter(675900)\tEnv Step(676096)\tLoss(-20.237)\n","INFO:root:Training: Train Iter(676000)\tEnv Step(676224)\tLoss(-26.416)\n","INFO:root:Training: Train Iter(676100)\tEnv Step(676352)\tLoss(-25.372)\n","INFO:root:Training: Train Iter(676200)\tEnv Step(676416)\tLoss(-28.710)\n","INFO:root:Training: Train Iter(676300)\tEnv Step(676544)\tLoss(-26.250)\n","INFO:root:Training: Train Iter(676400)\tEnv Step(676608)\tLoss(-25.082)\n","INFO:root:Training: Train Iter(676500)\tEnv Step(676736)\tLoss(-28.956)\n","INFO:root:Training: Train Iter(676600)\tEnv Step(676800)\tLoss(-20.550)\n","INFO:root:Training: Train Iter(676700)\tEnv Step(676928)\tLoss(-26.574)\n","INFO:root:Training: Train Iter(676800)\tEnv Step(677056)\tLoss(-28.662)\n","INFO:root:Evaluation: Train Iter(676864) Env Step(677056) Episode Return(311.087) \n","INFO:root:Training: Train Iter(676900)\tEnv Step(677120)\tLoss(-29.041)\n","INFO:root:Training: Train Iter(677000)\tEnv Step(677248)\tLoss(-24.561)\n","INFO:root:Training: Train Iter(677100)\tEnv Step(677312)\tLoss(-24.986)\n","INFO:root:Training: Train Iter(677200)\tEnv Step(677440)\tLoss(-21.673)\n","INFO:root:Training: Train Iter(677300)\tEnv Step(677504)\tLoss(-27.261)\n","INFO:root:Training: Train Iter(677400)\tEnv Step(677632)\tLoss(-26.773)\n","INFO:root:Training: Train Iter(677500)\tEnv Step(677696)\tLoss(-26.609)\n","INFO:root:Training: Train Iter(677600)\tEnv Step(677824)\tLoss(-25.240)\n","INFO:root:Training: Train Iter(677700)\tEnv Step(677952)\tLoss(-28.466)\n","INFO:root:Training: Train Iter(677800)\tEnv Step(678016)\tLoss(-30.744)\n","INFO:root:Evaluation: Train Iter(677888) Env Step(678080) Episode Return(316.557) \n","INFO:root:Training: Train Iter(677900)\tEnv Step(678144)\tLoss(-26.928)\n","INFO:root:Training: Train Iter(678000)\tEnv Step(678208)\tLoss(-27.068)\n","INFO:root:Training: Train Iter(678100)\tEnv Step(678336)\tLoss(-26.134)\n","INFO:root:Training: Train Iter(678200)\tEnv Step(678400)\tLoss(-18.424)\n","INFO:root:Training: Train Iter(678300)\tEnv Step(678528)\tLoss(-23.056)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84824\n","train_sample_count: 2035776\n","avg_envstep_per_episode: 84824.0\n","avg_sample_per_episode: 2035776.0\n","avg_envstep_per_sec: 8518.540898590052\n","avg_train_sample_per_sec: 204444.98156616127\n","avg_episode_per_sec: 0.10042606925622528\n","reward_mean: 308.51904296875\n","reward_std: 0.0\n","reward_max: 308.51904296875\n","reward_min: 308.51904296875\n","total_envstep_count: 678592\n","total_train_sample_count: 16286208\n","total_episode_count: 727\n","INFO:root:Training: Train Iter(678400)\tEnv Step(678656)\tLoss(-21.293)\n","INFO:root:Training: Train Iter(678500)\tEnv Step(678720)\tLoss(-24.390)\n","INFO:root:Training: Train Iter(678600)\tEnv Step(678848)\tLoss(-26.391)\n","INFO:root:Training: Train Iter(678700)\tEnv Step(678912)\tLoss(-24.647)\n","INFO:root:Training: Train Iter(678800)\tEnv Step(679040)\tLoss(-31.393)\n","INFO:root:Training: Train Iter(678900)\tEnv Step(679104)\tLoss(-22.779)\n","INFO:root:Evaluation: Train Iter(678912) Env Step(679104) Episode Return(312.189) \n","INFO:root:Training: Train Iter(679000)\tEnv Step(679232)\tLoss(-27.630)\n","INFO:root:Training: Train Iter(679100)\tEnv Step(679296)\tLoss(-22.234)\n","INFO:root:Training: Train Iter(679200)\tEnv Step(679424)\tLoss(-26.167)\n","INFO:root:Training: Train Iter(679300)\tEnv Step(679552)\tLoss(-23.165)\n","INFO:root:Training: Train Iter(679400)\tEnv Step(679616)\tLoss(-25.238)\n","INFO:root:Training: Train Iter(679500)\tEnv Step(679744)\tLoss(-27.349)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 84976\n","train_sample_count: 2039424\n","avg_envstep_per_episode: 84976.0\n","avg_sample_per_episode: 2039424.0\n","avg_envstep_per_sec: 8863.839681338546\n","avg_train_sample_per_sec: 212732.15235212512\n","avg_episode_per_sec: 0.10430991905171515\n","reward_mean: 309.3419189453125\n","reward_std: 0.0\n","reward_max: 309.3419189453125\n","reward_min: 309.3419189453125\n","total_envstep_count: 679808\n","total_train_sample_count: 16315392\n","total_episode_count: 728\n","INFO:root:Training: Train Iter(679600)\tEnv Step(679808)\tLoss(-26.509)\n","INFO:root:Training: Train Iter(679700)\tEnv Step(679936)\tLoss(-26.535)\n","INFO:root:Training: Train Iter(679800)\tEnv Step(680000)\tLoss(-25.128)\n","INFO:root:Training: Train Iter(679900)\tEnv Step(680128)\tLoss(-28.053)\n","INFO:root:Evaluation: Train Iter(679936) Env Step(680128) Episode Return(310.016) \n","INFO:root:Training: Train Iter(680000)\tEnv Step(680256)\tLoss(-27.994)\n","INFO:root:Training: Train Iter(680100)\tEnv Step(680320)\tLoss(-24.617)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85049\n","train_sample_count: 2041176\n","avg_envstep_per_episode: 85049.0\n","avg_sample_per_episode: 2041176.0\n","avg_envstep_per_sec: 8711.807671225577\n","avg_train_sample_per_sec: 209083.38410941383\n","avg_episode_per_sec: 0.10243280545597923\n","reward_mean: 309.146728515625\n","reward_std: 0.0\n","reward_max: 309.146728515625\n","reward_min: 309.146728515625\n","total_envstep_count: 680392\n","total_train_sample_count: 16329408\n","total_episode_count: 729\n","INFO:root:Training: Train Iter(680200)\tEnv Step(680448)\tLoss(-16.589)\n","INFO:root:Training: Train Iter(680300)\tEnv Step(680512)\tLoss(-29.098)\n","INFO:root:Training: Train Iter(680400)\tEnv Step(680640)\tLoss(-25.605)\n","INFO:root:Training: Train Iter(680500)\tEnv Step(680704)\tLoss(-29.103)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85092\n","train_sample_count: 2042208\n","avg_envstep_per_episode: 85092.0\n","avg_sample_per_episode: 2042208.0\n","avg_envstep_per_sec: 8711.486680234382\n","avg_train_sample_per_sec: 209075.68032562517\n","avg_episode_per_sec: 0.10237727025142648\n","reward_mean: 310.93011474609375\n","reward_std: 0.0\n","reward_max: 310.93011474609375\n","reward_min: 310.93011474609375\n","total_envstep_count: 680736\n","total_train_sample_count: 16337664\n","total_episode_count: 730\n","INFO:root:Training: Train Iter(680600)\tEnv Step(680832)\tLoss(-27.030)\n","INFO:root:Training: Train Iter(680700)\tEnv Step(680896)\tLoss(-27.998)\n","INFO:root:Training: Train Iter(680800)\tEnv Step(681024)\tLoss(-25.575)\n","INFO:root:Training: Train Iter(680900)\tEnv Step(681152)\tLoss(-27.377)\n","INFO:root:Evaluation: Train Iter(680960) Env Step(681152) Episode Return(314.668) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85148\n","train_sample_count: 2043552\n","avg_envstep_per_episode: 85148.0\n","avg_sample_per_episode: 2043552.0\n","avg_envstep_per_sec: 6122.07782272149\n","avg_train_sample_per_sec: 146929.86774531577\n","avg_episode_per_sec: 0.07189925568094953\n","reward_mean: 309.2196044921875\n","reward_std: 0.0\n","reward_max: 309.2196044921875\n","reward_min: 309.2196044921875\n","total_envstep_count: 681184\n","total_train_sample_count: 16348416\n","total_episode_count: 731\n","INFO:root:Training: Train Iter(681000)\tEnv Step(681216)\tLoss(-25.301)\n","INFO:root:Training: Train Iter(681100)\tEnv Step(681344)\tLoss(-21.759)\n","INFO:root:Training: Train Iter(681200)\tEnv Step(681408)\tLoss(-26.853)\n","INFO:root:Training: Train Iter(681300)\tEnv Step(681536)\tLoss(-27.950)\n","INFO:root:Training: Train Iter(681400)\tEnv Step(681600)\tLoss(-24.407)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85210\n","train_sample_count: 2045040\n","avg_envstep_per_episode: 85210.0\n","avg_sample_per_episode: 2045040.0\n","avg_envstep_per_sec: 8005.953898313492\n","avg_train_sample_per_sec: 192142.8935595238\n","avg_episode_per_sec: 0.09395556740187175\n","reward_mean: 309.6062927246094\n","reward_std: 0.0\n","reward_max: 309.6062927246094\n","reward_min: 309.6062927246094\n","total_envstep_count: 681680\n","total_train_sample_count: 16360320\n","total_episode_count: 732\n","INFO:root:Training: Train Iter(681500)\tEnv Step(681728)\tLoss(-16.817)\n","INFO:root:Training: Train Iter(681600)\tEnv Step(681856)\tLoss(-26.751)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85236\n","train_sample_count: 2045664\n","avg_envstep_per_episode: 85236.0\n","avg_sample_per_episode: 2045664.0\n","avg_envstep_per_sec: 8782.26725328728\n","avg_train_sample_per_sec: 210774.41407889474\n","avg_episode_per_sec: 0.10303471835007838\n","reward_mean: 308.93438720703125\n","reward_std: 0.0\n","reward_max: 308.93438720703125\n","reward_min: 308.93438720703125\n","total_envstep_count: 681888\n","total_train_sample_count: 16365312\n","total_episode_count: 733\n","INFO:root:Training: Train Iter(681700)\tEnv Step(681920)\tLoss(-30.439)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85242\n","train_sample_count: 2045808\n","avg_envstep_per_episode: 85242.0\n","avg_sample_per_episode: 2045808.0\n","avg_envstep_per_sec: 8675.418482262863\n","avg_train_sample_per_sec: 208210.04357430871\n","avg_episode_per_sec: 0.10177399031302484\n","reward_mean: 309.6855163574219\n","reward_std: 0.0\n","reward_max: 309.6855163574219\n","reward_min: 309.6855163574219\n","total_envstep_count: 681992\n","total_train_sample_count: 16367808\n","total_episode_count: 734\n","INFO:root:Training: Train Iter(681800)\tEnv Step(682048)\tLoss(-28.693)\n","INFO:root:Training: Train Iter(681900)\tEnv Step(682112)\tLoss(-23.634)\n","INFO:root:Evaluation: Train Iter(681984) Env Step(682176) Episode Return(309.572) \n","INFO:root:Training: Train Iter(682000)\tEnv Step(682240)\tLoss(-27.929)\n","INFO:root:Training: Train Iter(682100)\tEnv Step(682304)\tLoss(-24.656)\n","INFO:root:Training: Train Iter(682200)\tEnv Step(682432)\tLoss(-28.340)\n","INFO:root:Training: Train Iter(682300)\tEnv Step(682496)\tLoss(-26.290)\n","INFO:root:Training: Train Iter(682400)\tEnv Step(682624)\tLoss(-21.484)\n","INFO:root:Training: Train Iter(682500)\tEnv Step(682752)\tLoss(-27.020)\n","INFO:root:Training: Train Iter(682600)\tEnv Step(682816)\tLoss(-29.684)\n","INFO:root:Training: Train Iter(682700)\tEnv Step(682944)\tLoss(-26.557)\n","INFO:root:Training: Train Iter(682800)\tEnv Step(683008)\tLoss(-27.311)\n","INFO:root:Training: Train Iter(682900)\tEnv Step(683136)\tLoss(-26.605)\n","INFO:root:Training: Train Iter(683000)\tEnv Step(683200)\tLoss(-28.853)\n","INFO:root:Evaluation: Train Iter(683008) Env Step(683200) Episode Return(312.885) \n","INFO:root:Training: Train Iter(683100)\tEnv Step(683328)\tLoss(-27.153)\n","INFO:root:Training: Train Iter(683200)\tEnv Step(683456)\tLoss(-28.552)\n","INFO:root:Training: Train Iter(683300)\tEnv Step(683520)\tLoss(-20.481)\n","INFO:root:Training: Train Iter(683400)\tEnv Step(683648)\tLoss(-28.890)\n","INFO:root:Training: Train Iter(683500)\tEnv Step(683712)\tLoss(-29.718)\n","INFO:root:Training: Train Iter(683600)\tEnv Step(683840)\tLoss(-26.561)\n","INFO:root:Training: Train Iter(683700)\tEnv Step(683904)\tLoss(-24.955)\n","INFO:root:Training: Train Iter(683800)\tEnv Step(684032)\tLoss(-23.616)\n","INFO:root:Training: Train Iter(683900)\tEnv Step(684096)\tLoss(-28.795)\n","INFO:root:Training: Train Iter(684000)\tEnv Step(684224)\tLoss(-26.583)\n","INFO:root:Evaluation: Train Iter(684032) Env Step(684224) Episode Return(314.803) \n","INFO:root:Training: Train Iter(684100)\tEnv Step(684352)\tLoss(-27.654)\n","INFO:root:Training: Train Iter(684200)\tEnv Step(684416)\tLoss(-28.175)\n","INFO:root:Training: Train Iter(684300)\tEnv Step(684544)\tLoss(-26.681)\n","INFO:root:Training: Train Iter(684400)\tEnv Step(684608)\tLoss(-26.523)\n","INFO:root:Training: Train Iter(684500)\tEnv Step(684736)\tLoss(-24.698)\n","INFO:root:Training: Train Iter(684600)\tEnv Step(684800)\tLoss(-20.483)\n","INFO:root:Training: Train Iter(684700)\tEnv Step(684928)\tLoss(-19.092)\n","INFO:root:Training: Train Iter(684800)\tEnv Step(685056)\tLoss(-26.761)\n","INFO:root:Training: Train Iter(684900)\tEnv Step(685120)\tLoss(-28.702)\n","INFO:root:Training: Train Iter(685000)\tEnv Step(685248)\tLoss(-28.778)\n","INFO:root:Evaluation: Train Iter(685056) Env Step(685248) Episode Return(315.571) \n","INFO:root:Training: Train Iter(685100)\tEnv Step(685312)\tLoss(-26.419)\n","INFO:root:Training: Train Iter(685200)\tEnv Step(685440)\tLoss(-19.541)\n","INFO:root:Training: Train Iter(685300)\tEnv Step(685504)\tLoss(-15.322)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85696\n","train_sample_count: 2056704\n","avg_envstep_per_episode: 85696.0\n","avg_sample_per_episode: 2056704.0\n","avg_envstep_per_sec: 8519.734529577976\n","avg_train_sample_per_sec: 204473.6287098714\n","avg_episode_per_sec: 0.09941811204231207\n","reward_mean: 310.8382263183594\n","reward_std: 0.0\n","reward_max: 310.8382263183594\n","reward_min: 310.8382263183594\n","total_envstep_count: 685568\n","total_train_sample_count: 16453632\n","total_episode_count: 735\n","INFO:root:Training: Train Iter(685400)\tEnv Step(685632)\tLoss(-29.521)\n","INFO:root:Training: Train Iter(685500)\tEnv Step(685696)\tLoss(-30.944)\n","INFO:root:Training: Train Iter(685600)\tEnv Step(685824)\tLoss(-20.927)\n","INFO:root:Training: Train Iter(685700)\tEnv Step(685952)\tLoss(-21.189)\n","INFO:root:Training: Train Iter(685800)\tEnv Step(686016)\tLoss(-25.596)\n","INFO:root:Training: Train Iter(685900)\tEnv Step(686144)\tLoss(-21.177)\n","INFO:root:Training: Train Iter(686000)\tEnv Step(686208)\tLoss(-22.565)\n","INFO:root:Evaluation: Train Iter(686080) Env Step(686272) Episode Return(315.964) \n","INFO:root:Training: Train Iter(686100)\tEnv Step(686336)\tLoss(-26.628)\n","INFO:root:Training: Train Iter(686200)\tEnv Step(686400)\tLoss(-26.481)\n","INFO:root:Training: Train Iter(686300)\tEnv Step(686528)\tLoss(-19.045)\n","INFO:root:Training: Train Iter(686400)\tEnv Step(686656)\tLoss(-27.390)\n","INFO:root:Training: Train Iter(686500)\tEnv Step(686720)\tLoss(-28.275)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85853\n","train_sample_count: 2060472\n","avg_envstep_per_episode: 85853.0\n","avg_sample_per_episode: 2060472.0\n","avg_envstep_per_sec: 8864.625381916281\n","avg_train_sample_per_sec: 212751.00916599078\n","avg_episode_per_sec: 0.10325353082497155\n","reward_mean: 309.3075866699219\n","reward_std: 0.0\n","reward_max: 309.3075866699219\n","reward_min: 309.3075866699219\n","total_envstep_count: 686824\n","total_train_sample_count: 16483776\n","total_episode_count: 736\n","INFO:root:Training: Train Iter(686600)\tEnv Step(686848)\tLoss(-25.568)\n","INFO:root:Training: Train Iter(686700)\tEnv Step(686912)\tLoss(-24.133)\n","INFO:root:Training: Train Iter(686800)\tEnv Step(687040)\tLoss(-27.115)\n","INFO:root:Training: Train Iter(686900)\tEnv Step(687104)\tLoss(-28.090)\n","INFO:root:Training: Train Iter(687000)\tEnv Step(687232)\tLoss(-28.054)\n","INFO:root:Training: Train Iter(687100)\tEnv Step(687296)\tLoss(-23.596)\n","INFO:root:Evaluation: Train Iter(687104) Env Step(687296) Episode Return(311.483) \n","INFO:root:Training: Train Iter(687200)\tEnv Step(687424)\tLoss(-14.904)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85934\n","train_sample_count: 2062416\n","avg_envstep_per_episode: 85934.0\n","avg_sample_per_episode: 2062416.0\n","avg_envstep_per_sec: 8712.49860317956\n","avg_train_sample_per_sec: 209099.96647630946\n","avg_episode_per_sec: 0.10138593110037425\n","reward_mean: 309.7658386230469\n","reward_std: 0.0\n","reward_max: 309.7658386230469\n","reward_min: 309.7658386230469\n","total_envstep_count: 687472\n","total_train_sample_count: 16499328\n","total_episode_count: 737\n","INFO:root:Training: Train Iter(687300)\tEnv Step(687552)\tLoss(-27.858)\n","INFO:root:Training: Train Iter(687400)\tEnv Step(687616)\tLoss(-28.160)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 85954\n","train_sample_count: 2062896\n","avg_envstep_per_episode: 85954.0\n","avg_sample_per_episode: 2062896.0\n","avg_envstep_per_sec: 8712.3440314736\n","avg_train_sample_per_sec: 209096.25675536643\n","avg_episode_per_sec: 0.10136054205125533\n","reward_mean: 310.5702819824219\n","reward_std: 0.0\n","reward_max: 310.5702819824219\n","reward_min: 310.5702819824219\n","total_envstep_count: 687632\n","total_train_sample_count: 16503168\n","total_episode_count: 738\n","INFO:root:Training: Train Iter(687500)\tEnv Step(687744)\tLoss(-24.159)\n","INFO:root:Training: Train Iter(687600)\tEnv Step(687808)\tLoss(-27.775)\n","INFO:root:Training: Train Iter(687700)\tEnv Step(687936)\tLoss(-23.975)\n","INFO:root:Training: Train Iter(687800)\tEnv Step(688000)\tLoss(-26.477)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86011\n","train_sample_count: 2064264\n","avg_envstep_per_episode: 86011.0\n","avg_sample_per_episode: 2064264.0\n","avg_envstep_per_sec: 6123.267672270659\n","avg_train_sample_per_sec: 146958.4241344958\n","avg_episode_per_sec: 0.07119168097418538\n","reward_mean: 309.6685485839844\n","reward_std: 0.0\n","reward_max: 309.6685485839844\n","reward_min: 309.6685485839844\n","total_envstep_count: 688088\n","total_train_sample_count: 16514112\n","total_episode_count: 739\n","INFO:root:Training: Train Iter(687900)\tEnv Step(688128)\tLoss(-27.814)\n","INFO:root:Training: Train Iter(688000)\tEnv Step(688256)\tLoss(-29.847)\n","INFO:root:Training: Train Iter(688100)\tEnv Step(688320)\tLoss(-27.847)\n","INFO:root:Evaluation: Train Iter(688128) Env Step(688320) Episode Return(312.369) \n","INFO:root:Training: Train Iter(688200)\tEnv Step(688448)\tLoss(-26.494)\n","INFO:root:Training: Train Iter(688300)\tEnv Step(688512)\tLoss(-26.685)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86080\n","train_sample_count: 2065920\n","avg_envstep_per_episode: 86080.0\n","avg_sample_per_episode: 2065920.0\n","avg_envstep_per_sec: 8008.120593078074\n","avg_train_sample_per_sec: 192194.89423387378\n","avg_episode_per_sec: 0.0930311407188438\n","reward_mean: 309.570068359375\n","reward_std: 0.0\n","reward_max: 309.570068359375\n","reward_min: 309.570068359375\n","total_envstep_count: 688640\n","total_train_sample_count: 16527360\n","total_episode_count: 740\n","INFO:root:Training: Train Iter(688400)\tEnv Step(688640)\tLoss(-27.733)\n","INFO:root:Training: Train Iter(688500)\tEnv Step(688704)\tLoss(-21.194)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86097\n","train_sample_count: 2066328\n","avg_envstep_per_episode: 86097.0\n","avg_sample_per_episode: 2066328.0\n","avg_envstep_per_sec: 8783.919714331449\n","avg_train_sample_per_sec: 210814.07314395477\n","avg_episode_per_sec: 0.10202352828009628\n","reward_mean: 310.111572265625\n","reward_std: 0.0\n","reward_max: 310.111572265625\n","reward_min: 310.111572265625\n","total_envstep_count: 688776\n","total_train_sample_count: 16530624\n","total_episode_count: 741\n","INFO:root:Training: Train Iter(688600)\tEnv Step(688832)\tLoss(-27.866)\n","INFO:root:Training: Train Iter(688700)\tEnv Step(688896)\tLoss(-22.574)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86106\n","train_sample_count: 2066544\n","avg_envstep_per_episode: 86106.0\n","avg_sample_per_episode: 2066544.0\n","avg_envstep_per_sec: 8675.146211579968\n","avg_train_sample_per_sec: 208203.5090779192\n","avg_episode_per_sec: 0.10074961340185315\n","reward_mean: 310.55426025390625\n","reward_std: 0.0\n","reward_max: 310.55426025390625\n","reward_min: 310.55426025390625\n","total_envstep_count: 688904\n","total_train_sample_count: 16533696\n","total_episode_count: 742\n","INFO:root:Training: Train Iter(688800)\tEnv Step(689024)\tLoss(-27.331)\n","INFO:root:Training: Train Iter(688900)\tEnv Step(689152)\tLoss(-27.129)\n","INFO:root:Training: Train Iter(689000)\tEnv Step(689216)\tLoss(-14.940)\n","INFO:root:Training: Train Iter(689100)\tEnv Step(689344)\tLoss(-15.089)\n","INFO:root:Evaluation: Train Iter(689152) Env Step(689344) Episode Return(314.977) \n","INFO:root:Training: Train Iter(689200)\tEnv Step(689408)\tLoss(-26.384)\n","INFO:root:Training: Train Iter(689300)\tEnv Step(689536)\tLoss(-29.371)\n","INFO:root:Training: Train Iter(689400)\tEnv Step(689600)\tLoss(-27.877)\n","INFO:root:Training: Train Iter(689500)\tEnv Step(689728)\tLoss(-22.027)\n","INFO:root:Training: Train Iter(689600)\tEnv Step(689856)\tLoss(-27.403)\n","INFO:root:Training: Train Iter(689700)\tEnv Step(689920)\tLoss(-27.942)\n","INFO:root:Training: Train Iter(689800)\tEnv Step(690048)\tLoss(-26.704)\n","INFO:root:Training: Train Iter(689900)\tEnv Step(690112)\tLoss(-30.473)\n","INFO:root:Training: Train Iter(690000)\tEnv Step(690240)\tLoss(-27.356)\n","INFO:root:Training: Train Iter(690100)\tEnv Step(690304)\tLoss(-28.901)\n","INFO:root:Evaluation: Train Iter(690176) Env Step(690368) Episode Return(314.135) \n","INFO:root:Training: Train Iter(690200)\tEnv Step(690432)\tLoss(-26.360)\n","INFO:root:Training: Train Iter(690300)\tEnv Step(690496)\tLoss(-12.969)\n","INFO:root:Training: Train Iter(690400)\tEnv Step(690624)\tLoss(-24.935)\n","INFO:root:Training: Train Iter(690500)\tEnv Step(690752)\tLoss(-26.278)\n","INFO:root:Training: Train Iter(690600)\tEnv Step(690816)\tLoss(-18.946)\n","INFO:root:Training: Train Iter(690700)\tEnv Step(690944)\tLoss(-24.941)\n","INFO:root:Training: Train Iter(690800)\tEnv Step(691008)\tLoss(-25.261)\n","INFO:root:Training: Train Iter(690900)\tEnv Step(691136)\tLoss(-20.720)\n","INFO:root:Training: Train Iter(691000)\tEnv Step(691200)\tLoss(-27.375)\n","INFO:root:Training: Train Iter(691100)\tEnv Step(691328)\tLoss(-24.310)\n","INFO:root:Evaluation: Train Iter(691200) Env Step(691392) Episode Return(309.886) \n","INFO:root:Training: Train Iter(691200)\tEnv Step(691456)\tLoss(-28.923)\n","INFO:root:Training: Train Iter(691300)\tEnv Step(691520)\tLoss(-28.404)\n","INFO:root:Training: Train Iter(691400)\tEnv Step(691648)\tLoss(-18.682)\n","INFO:root:Training: Train Iter(691500)\tEnv Step(691712)\tLoss(-22.171)\n","INFO:root:Training: Train Iter(691600)\tEnv Step(691840)\tLoss(-12.555)\n","INFO:root:Training: Train Iter(691700)\tEnv Step(691904)\tLoss(-27.103)\n","INFO:root:Training: Train Iter(691800)\tEnv Step(692032)\tLoss(-28.744)\n","INFO:root:Training: Train Iter(691900)\tEnv Step(692096)\tLoss(-21.875)\n","INFO:root:Training: Train Iter(692000)\tEnv Step(692224)\tLoss(-29.214)\n","INFO:root:Training: Train Iter(692100)\tEnv Step(692352)\tLoss(-25.557)\n","INFO:root:Training: Train Iter(692200)\tEnv Step(692416)\tLoss(-29.664)\n","INFO:root:Evaluation: Train Iter(692224) Env Step(692416) Episode Return(314.945) \n","INFO:root:Training: Train Iter(692300)\tEnv Step(692544)\tLoss(-25.744)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86569\n","train_sample_count: 2077656\n","avg_envstep_per_episode: 86569.0\n","avg_sample_per_episode: 2077656.0\n","avg_envstep_per_sec: 8515.04416053765\n","avg_train_sample_per_sec: 204361.05985290362\n","avg_episode_per_sec: 0.09836135522574653\n","reward_mean: 309.3833923339844\n","reward_std: 0.0\n","reward_max: 309.3833923339844\n","reward_min: 309.3833923339844\n","total_envstep_count: 692552\n","total_train_sample_count: 16621248\n","total_episode_count: 743\n","INFO:root:Training: Train Iter(692400)\tEnv Step(692608)\tLoss(-18.205)\n","INFO:root:Training: Train Iter(692500)\tEnv Step(692736)\tLoss(-28.754)\n","INFO:root:Training: Train Iter(692600)\tEnv Step(692800)\tLoss(-24.786)\n","INFO:root:Training: Train Iter(692700)\tEnv Step(692928)\tLoss(-27.447)\n","INFO:root:Training: Train Iter(692800)\tEnv Step(693056)\tLoss(-26.078)\n","INFO:root:Training: Train Iter(692900)\tEnv Step(693120)\tLoss(-27.002)\n","INFO:root:Training: Train Iter(693000)\tEnv Step(693248)\tLoss(-27.394)\n","INFO:root:Training: Train Iter(693100)\tEnv Step(693312)\tLoss(-29.631)\n","INFO:root:Training: Train Iter(693200)\tEnv Step(693440)\tLoss(-28.311)\n","INFO:root:Evaluation: Train Iter(693248) Env Step(693440) Episode Return(313.903) \n","INFO:root:Training: Train Iter(693300)\tEnv Step(693504)\tLoss(-26.103)\n","INFO:root:Training: Train Iter(693400)\tEnv Step(693632)\tLoss(-25.944)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86705\n","train_sample_count: 2080920\n","avg_envstep_per_episode: 86705.0\n","avg_sample_per_episode: 2080920.0\n","avg_envstep_per_sec: 8865.522810555301\n","avg_train_sample_per_sec: 212772.54745332722\n","avg_episode_per_sec: 0.10224926833003058\n","reward_mean: 311.0951843261719\n","reward_std: 0.0\n","reward_max: 311.0951843261719\n","reward_min: 311.0951843261719\n","total_envstep_count: 693640\n","total_train_sample_count: 16647360\n","total_episode_count: 744\n","INFO:root:Training: Train Iter(693500)\tEnv Step(693696)\tLoss(-29.078)\n","INFO:root:Training: Train Iter(693600)\tEnv Step(693824)\tLoss(-28.382)\n","INFO:root:Training: Train Iter(693700)\tEnv Step(693952)\tLoss(-27.571)\n","INFO:root:Training: Train Iter(693800)\tEnv Step(694016)\tLoss(-20.939)\n","INFO:root:Training: Train Iter(693900)\tEnv Step(694144)\tLoss(-29.578)\n","INFO:root:Training: Train Iter(694000)\tEnv Step(694208)\tLoss(-30.596)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86780\n","train_sample_count: 2082720\n","avg_envstep_per_episode: 86780.0\n","avg_sample_per_episode: 2082720.0\n","avg_envstep_per_sec: 8710.528666424138\n","avg_train_sample_per_sec: 209052.6879941793\n","avg_episode_per_sec: 0.10037484059027584\n","reward_mean: 310.2036437988281\n","reward_std: 0.0\n","reward_max: 310.2036437988281\n","reward_min: 310.2036437988281\n","total_envstep_count: 694240\n","total_train_sample_count: 16661760\n","total_episode_count: 745\n","INFO:root:Training: Train Iter(694100)\tEnv Step(694336)\tLoss(-29.876)\n","INFO:root:Training: Train Iter(694200)\tEnv Step(694400)\tLoss(-27.026)\n","INFO:root:Evaluation: Train Iter(694272) Env Step(694464) Episode Return(310.768) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86813\n","train_sample_count: 2083512\n","avg_envstep_per_episode: 86813.0\n","avg_sample_per_episode: 2083512.0\n","avg_envstep_per_sec: 8709.134761472746\n","avg_train_sample_per_sec: 209019.2342753459\n","avg_episode_per_sec: 0.10032062895502684\n","reward_mean: 310.8208923339844\n","reward_std: 0.0\n","reward_max: 310.8208923339844\n","reward_min: 310.8208923339844\n","total_envstep_count: 694504\n","total_train_sample_count: 16668096\n","total_episode_count: 746\n","INFO:root:Training: Train Iter(694300)\tEnv Step(694528)\tLoss(-28.126)\n","INFO:root:Training: Train Iter(694400)\tEnv Step(694656)\tLoss(-28.634)\n","INFO:root:Training: Train Iter(694500)\tEnv Step(694720)\tLoss(-25.977)\n","INFO:root:Training: Train Iter(694600)\tEnv Step(694848)\tLoss(-29.629)\n","INFO:root:Training: Train Iter(694700)\tEnv Step(694912)\tLoss(-22.036)\n","INFO:root:Training: Train Iter(694800)\tEnv Step(695040)\tLoss(-29.004)\n","INFO:root:Training: Train Iter(694900)\tEnv Step(695104)\tLoss(-29.269)\n","INFO:root:Training: Train Iter(695000)\tEnv Step(695232)\tLoss(-19.916)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86906\n","train_sample_count: 2085744\n","avg_envstep_per_episode: 86906.0\n","avg_sample_per_episode: 2085744.0\n","avg_envstep_per_sec: 6122.014102681756\n","avg_train_sample_per_sec: 146928.33846436214\n","avg_episode_per_sec: 0.07044409019724479\n","reward_mean: 308.0395202636719\n","reward_std: 0.0\n","reward_max: 308.0395202636719\n","reward_min: 308.0395202636719\n","total_envstep_count: 695248\n","total_train_sample_count: 16685952\n","total_episode_count: 747\n","INFO:root:Training: Train Iter(695100)\tEnv Step(695296)\tLoss(-28.071)\n","INFO:root:Training: Train Iter(695200)\tEnv Step(695424)\tLoss(-29.061)\n","INFO:root:Evaluation: Train Iter(695296) Env Step(695488) Episode Return(314.568) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86937\n","train_sample_count: 2086488\n","avg_envstep_per_episode: 86937.0\n","avg_sample_per_episode: 2086488.0\n","avg_envstep_per_sec: 8007.532114994747\n","avg_train_sample_per_sec: 192180.7707598739\n","avg_episode_per_sec: 0.09210729741070829\n","reward_mean: 310.3415222167969\n","reward_std: 0.0\n","reward_max: 310.3415222167969\n","reward_min: 310.3415222167969\n","total_envstep_count: 695496\n","total_train_sample_count: 16691904\n","total_episode_count: 748\n","INFO:root:Training: Train Iter(695300)\tEnv Step(695552)\tLoss(-30.006)\n","INFO:root:Training: Train Iter(695400)\tEnv Step(695616)\tLoss(-24.243)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86966\n","train_sample_count: 2087184\n","avg_envstep_per_episode: 86966.0\n","avg_sample_per_episode: 2087184.0\n","avg_envstep_per_sec: 8672.347347607347\n","avg_train_sample_per_sec: 208136.33634257634\n","avg_episode_per_sec: 0.09972112489487095\n","reward_mean: 309.355712890625\n","reward_std: 0.0\n","reward_max: 309.355712890625\n","reward_min: 309.355712890625\n","total_envstep_count: 695728\n","total_train_sample_count: 16697472\n","total_episode_count: 749\n","INFO:root:Training: Train Iter(695500)\tEnv Step(695744)\tLoss(-25.980)\n","INFO:root:Training: Train Iter(695600)\tEnv Step(695808)\tLoss(-18.758)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 86980\n","train_sample_count: 2087520\n","avg_envstep_per_episode: 86980.0\n","avg_sample_per_episode: 2087520.0\n","avg_envstep_per_sec: 8782.641238838401\n","avg_train_sample_per_sec: 210783.38973212164\n","avg_episode_per_sec: 0.10097311150653485\n","reward_mean: 308.5531005859375\n","reward_std: 0.0\n","reward_max: 308.5531005859375\n","reward_min: 308.5531005859375\n","total_envstep_count: 695840\n","total_train_sample_count: 16700160\n","total_episode_count: 750\n","INFO:root:Training: Train Iter(695700)\tEnv Step(695936)\tLoss(-29.539)\n","INFO:root:Training: Train Iter(695800)\tEnv Step(696000)\tLoss(-24.056)\n","INFO:root:Training: Train Iter(695900)\tEnv Step(696128)\tLoss(-19.543)\n","INFO:root:Training: Train Iter(696000)\tEnv Step(696256)\tLoss(-18.751)\n","INFO:root:Training: Train Iter(696100)\tEnv Step(696320)\tLoss(-28.535)\n","INFO:root:Training: Train Iter(696200)\tEnv Step(696448)\tLoss(-22.315)\n","INFO:root:Training: Train Iter(696300)\tEnv Step(696512)\tLoss(-13.778)\n","INFO:root:Evaluation: Train Iter(696320) Env Step(696512) Episode Return(315.067) \n","INFO:root:Training: Train Iter(696400)\tEnv Step(696640)\tLoss(-26.144)\n","INFO:root:Training: Train Iter(696500)\tEnv Step(696704)\tLoss(-24.524)\n","INFO:root:Training: Train Iter(696600)\tEnv Step(696832)\tLoss(-26.011)\n","INFO:root:Training: Train Iter(696700)\tEnv Step(696896)\tLoss(-19.108)\n","INFO:root:Training: Train Iter(696800)\tEnv Step(697024)\tLoss(-28.067)\n","INFO:root:Training: Train Iter(696900)\tEnv Step(697152)\tLoss(-26.746)\n","INFO:root:Training: Train Iter(697000)\tEnv Step(697216)\tLoss(-26.127)\n","INFO:root:Training: Train Iter(697100)\tEnv Step(697344)\tLoss(-27.484)\n","INFO:root:Training: Train Iter(697200)\tEnv Step(697408)\tLoss(-30.194)\n","INFO:root:Training: Train Iter(697300)\tEnv Step(697536)\tLoss(-26.285)\n","INFO:root:Evaluation: Train Iter(697344) Env Step(697536) Episode Return(305.488) \n","INFO:root:Training: Train Iter(697400)\tEnv Step(697600)\tLoss(-26.191)\n","INFO:root:Training: Train Iter(697500)\tEnv Step(697728)\tLoss(-26.432)\n","INFO:root:Training: Train Iter(697600)\tEnv Step(697856)\tLoss(-12.072)\n","INFO:root:Training: Train Iter(697700)\tEnv Step(697920)\tLoss(-26.539)\n","INFO:root:Training: Train Iter(697800)\tEnv Step(698048)\tLoss(-23.697)\n","INFO:root:Training: Train Iter(697900)\tEnv Step(698112)\tLoss(-28.785)\n","INFO:root:Training: Train Iter(698000)\tEnv Step(698240)\tLoss(-26.628)\n","INFO:root:Training: Train Iter(698100)\tEnv Step(698304)\tLoss(-25.890)\n","INFO:root:Training: Train Iter(698200)\tEnv Step(698432)\tLoss(-28.356)\n","INFO:root:Training: Train Iter(698300)\tEnv Step(698496)\tLoss(-24.817)\n","INFO:root:Evaluation: Train Iter(698368) Env Step(698560) Episode Return(311.588) \n","INFO:root:Training: Train Iter(698400)\tEnv Step(698624)\tLoss(-29.456)\n","INFO:root:Training: Train Iter(698500)\tEnv Step(698752)\tLoss(-20.724)\n","INFO:root:Training: Train Iter(698600)\tEnv Step(698816)\tLoss(-26.251)\n","INFO:root:Training: Train Iter(698700)\tEnv Step(698944)\tLoss(-23.431)\n","INFO:root:Training: Train Iter(698800)\tEnv Step(699008)\tLoss(-27.918)\n","INFO:root:Training: Train Iter(698900)\tEnv Step(699136)\tLoss(-26.469)\n","INFO:root:Training: Train Iter(699000)\tEnv Step(699200)\tLoss(-29.014)\n","INFO:root:Training: Train Iter(699100)\tEnv Step(699328)\tLoss(-25.363)\n","INFO:root:Training: Train Iter(699200)\tEnv Step(699456)\tLoss(-20.162)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87438\n","train_sample_count: 2098512\n","avg_envstep_per_episode: 87438.0\n","avg_sample_per_episode: 2098512.0\n","avg_envstep_per_sec: 8518.830871261503\n","avg_train_sample_per_sec: 204451.94091027608\n","avg_episode_per_sec: 0.09742710116038225\n","reward_mean: 309.4749755859375\n","reward_std: 0.0\n","reward_max: 309.4749755859375\n","reward_min: 309.4749755859375\n","total_envstep_count: 699504\n","total_train_sample_count: 16788096\n","total_episode_count: 751\n","INFO:root:Training: Train Iter(699300)\tEnv Step(699520)\tLoss(-27.974)\n","INFO:root:Evaluation: Train Iter(699392) Env Step(699584) Episode Return(316.609) \n","INFO:root:Training: Train Iter(699400)\tEnv Step(699648)\tLoss(-21.676)\n","INFO:root:Training: Train Iter(699500)\tEnv Step(699712)\tLoss(-19.427)\n","INFO:root:Training: Train Iter(699600)\tEnv Step(699840)\tLoss(-26.208)\n","INFO:root:Training: Train Iter(699700)\tEnv Step(699904)\tLoss(-27.951)\n","INFO:root:Training: Train Iter(699800)\tEnv Step(700032)\tLoss(-23.723)\n","INFO:root:Training: Train Iter(699900)\tEnv Step(700096)\tLoss(-29.601)\n","INFO:root:Training: Train Iter(700000)\tEnv Step(700224)\tLoss(-28.713)\n","INFO:root:Training: Train Iter(700100)\tEnv Step(700352)\tLoss(-13.010)\n","INFO:root:Training: Train Iter(700200)\tEnv Step(700416)\tLoss(-20.386)\n","INFO:root:Training: Train Iter(700300)\tEnv Step(700544)\tLoss(-26.530)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87576\n","train_sample_count: 2101824\n","avg_envstep_per_episode: 87576.0\n","avg_sample_per_episode: 2101824.0\n","avg_envstep_per_sec: 8866.684001510852\n","avg_train_sample_per_sec: 212800.41603626046\n","avg_episode_per_sec: 0.10124559241699613\n","reward_mean: 310.89129638671875\n","reward_std: 0.0\n","reward_max: 310.89129638671875\n","reward_min: 310.89129638671875\n","total_envstep_count: 700608\n","total_train_sample_count: 16814592\n","total_episode_count: 752\n","INFO:root:Training: Train Iter(700400)\tEnv Step(700608)\tLoss(-28.175)\n","INFO:root:Evaluation: Train Iter(700416) Env Step(700608) Episode Return(313.948) \n","INFO:root:Training: Train Iter(700500)\tEnv Step(700736)\tLoss(-28.638)\n","INFO:root:Training: Train Iter(700600)\tEnv Step(700800)\tLoss(-28.480)\n","INFO:root:Training: Train Iter(700700)\tEnv Step(700928)\tLoss(-25.437)\n","INFO:root:Training: Train Iter(700800)\tEnv Step(701056)\tLoss(-27.689)\n","INFO:root:Training: Train Iter(700900)\tEnv Step(701120)\tLoss(-22.099)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87656\n","train_sample_count: 2103744\n","avg_envstep_per_episode: 87656.0\n","avg_sample_per_episode: 2103744.0\n","avg_envstep_per_sec: 8713.156700105186\n","avg_train_sample_per_sec: 209115.76080252446\n","avg_episode_per_sec: 0.09940171465849669\n","reward_mean: 309.6548767089844\n","reward_std: 0.0\n","reward_max: 309.6548767089844\n","reward_min: 309.6548767089844\n","total_envstep_count: 701248\n","total_train_sample_count: 16829952\n","total_episode_count: 753\n","INFO:root:Training: Train Iter(701000)\tEnv Step(701248)\tLoss(-26.258)\n","INFO:root:Training: Train Iter(701100)\tEnv Step(701312)\tLoss(-28.862)\n","INFO:root:Training: Train Iter(701200)\tEnv Step(701440)\tLoss(-26.181)\n","INFO:root:Training: Train Iter(701300)\tEnv Step(701504)\tLoss(-19.070)\n","INFO:root:Training: Train Iter(701400)\tEnv Step(701632)\tLoss(-27.012)\n","INFO:root:Evaluation: Train Iter(701440) Env Step(701632) Episode Return(315.511) \n","INFO:root:Training: Train Iter(701500)\tEnv Step(701696)\tLoss(-29.297)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87717\n","train_sample_count: 2105208\n","avg_envstep_per_episode: 87717.0\n","avg_sample_per_episode: 2105208.0\n","avg_envstep_per_sec: 8707.021746375982\n","avg_train_sample_per_sec: 208968.52191302355\n","avg_episode_per_sec: 0.09926264859007924\n","reward_mean: 307.945556640625\n","reward_std: 0.0\n","reward_max: 307.945556640625\n","reward_min: 307.945556640625\n","total_envstep_count: 701736\n","total_train_sample_count: 16841664\n","total_episode_count: 754\n","INFO:root:Training: Train Iter(701600)\tEnv Step(701824)\tLoss(-27.326)\n","INFO:root:Training: Train Iter(701700)\tEnv Step(701952)\tLoss(-27.465)\n","INFO:root:Training: Train Iter(701800)\tEnv Step(702016)\tLoss(-28.405)\n","INFO:root:Training: Train Iter(701900)\tEnv Step(702144)\tLoss(-29.556)\n","INFO:root:Training: Train Iter(702000)\tEnv Step(702208)\tLoss(-26.212)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87780\n","train_sample_count: 2106720\n","avg_envstep_per_episode: 87780.0\n","avg_sample_per_episode: 2106720.0\n","avg_envstep_per_sec: 6121.44637350593\n","avg_train_sample_per_sec: 146914.71296414232\n","avg_episode_per_sec: 0.0697362311859869\n","reward_mean: 310.1290283203125\n","reward_std: 0.0\n","reward_max: 310.1290283203125\n","reward_min: 310.1290283203125\n","total_envstep_count: 702240\n","total_train_sample_count: 16853760\n","total_episode_count: 755\n","INFO:root:Training: Train Iter(702100)\tEnv Step(702336)\tLoss(-23.599)\n","INFO:root:Training: Train Iter(702200)\tEnv Step(702400)\tLoss(-30.531)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87807\n","train_sample_count: 2107368\n","avg_envstep_per_episode: 87807.0\n","avg_sample_per_episode: 2107368.0\n","avg_envstep_per_sec: 8009.651949408172\n","avg_train_sample_per_sec: 192231.64678579613\n","avg_episode_per_sec: 0.0912188316353841\n","reward_mean: 310.36724853515625\n","reward_std: 0.0\n","reward_max: 310.36724853515625\n","reward_min: 310.36724853515625\n","total_envstep_count: 702456\n","total_train_sample_count: 16858944\n","total_episode_count: 756\n","INFO:root:Training: Train Iter(702300)\tEnv Step(702528)\tLoss(-27.943)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87808\n","train_sample_count: 2107392\n","avg_envstep_per_episode: 87808.0\n","avg_sample_per_episode: 2107392.0\n","avg_envstep_per_sec: 8674.09185357166\n","avg_train_sample_per_sec: 208178.2044857198\n","avg_episode_per_sec: 0.0987847559854644\n","reward_mean: 311.46136474609375\n","reward_std: 0.0\n","reward_max: 311.46136474609375\n","reward_min: 311.46136474609375\n","total_envstep_count: 702536\n","total_train_sample_count: 16860864\n","total_episode_count: 757\n","INFO:root:Training: Train Iter(702400)\tEnv Step(702656)\tLoss(-28.203)\n","INFO:root:Evaluation: Train Iter(702464) Env Step(702656) Episode Return(311.023) \n","INFO:root:Training: Train Iter(702500)\tEnv Step(702720)\tLoss(-29.756)\n","INFO:root:Training: Train Iter(702600)\tEnv Step(702848)\tLoss(-25.927)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 87863\n","train_sample_count: 2108712\n","avg_envstep_per_episode: 87863.0\n","avg_sample_per_episode: 2108712.0\n","avg_envstep_per_sec: 8783.733989055134\n","avg_train_sample_per_sec: 210809.6157373232\n","avg_episode_per_sec: 0.09997079531833802\n","reward_mean: 309.2225036621094\n","reward_std: 0.0\n","reward_max: 309.2225036621094\n","reward_min: 309.2225036621094\n","total_envstep_count: 702904\n","total_train_sample_count: 16869696\n","total_episode_count: 758\n","INFO:root:Training: Train Iter(702700)\tEnv Step(702912)\tLoss(-24.482)\n","INFO:root:Training: Train Iter(702800)\tEnv Step(703040)\tLoss(-26.997)\n","INFO:root:Training: Train Iter(702900)\tEnv Step(703104)\tLoss(-27.412)\n","INFO:root:Training: Train Iter(703000)\tEnv Step(703232)\tLoss(-25.754)\n","INFO:root:Training: Train Iter(703100)\tEnv Step(703296)\tLoss(-27.403)\n","INFO:root:Training: Train Iter(703200)\tEnv Step(703424)\tLoss(-29.227)\n","INFO:root:Training: Train Iter(703300)\tEnv Step(703552)\tLoss(-27.159)\n","INFO:root:Training: Train Iter(703400)\tEnv Step(703616)\tLoss(-26.966)\n","INFO:root:Evaluation: Train Iter(703488) Env Step(703680) Episode Return(308.412) \n","INFO:root:Training: Train Iter(703500)\tEnv Step(703744)\tLoss(-27.026)\n","INFO:root:Training: Train Iter(703600)\tEnv Step(703808)\tLoss(-29.052)\n","INFO:root:Training: Train Iter(703700)\tEnv Step(703936)\tLoss(-28.227)\n","INFO:root:Training: Train Iter(703800)\tEnv Step(704000)\tLoss(-27.804)\n","INFO:root:Training: Train Iter(703900)\tEnv Step(704128)\tLoss(-29.606)\n","INFO:root:Training: Train Iter(704000)\tEnv Step(704256)\tLoss(-28.439)\n","INFO:root:Training: Train Iter(704100)\tEnv Step(704320)\tLoss(-29.577)\n","INFO:root:Training: Train Iter(704200)\tEnv Step(704448)\tLoss(-22.698)\n","INFO:root:Training: Train Iter(704300)\tEnv Step(704512)\tLoss(-28.595)\n","INFO:root:Training: Train Iter(704400)\tEnv Step(704640)\tLoss(-29.511)\n","INFO:root:Training: Train Iter(704500)\tEnv Step(704704)\tLoss(-26.862)\n","INFO:root:Evaluation: Train Iter(704512) Env Step(704704) Episode Return(311.493) \n","INFO:root:Training: Train Iter(704600)\tEnv Step(704832)\tLoss(-27.729)\n","INFO:root:Training: Train Iter(704700)\tEnv Step(704896)\tLoss(-27.681)\n","INFO:root:Training: Train Iter(704800)\tEnv Step(705024)\tLoss(-30.246)\n","INFO:root:Training: Train Iter(704900)\tEnv Step(705152)\tLoss(-25.604)\n","INFO:root:Training: Train Iter(705000)\tEnv Step(705216)\tLoss(-20.766)\n","INFO:root:Training: Train Iter(705100)\tEnv Step(705344)\tLoss(-21.631)\n","INFO:root:Training: Train Iter(705200)\tEnv Step(705408)\tLoss(-28.522)\n","INFO:root:Training: Train Iter(705300)\tEnv Step(705536)\tLoss(-26.270)\n","INFO:root:Training: Train Iter(705400)\tEnv Step(705600)\tLoss(-27.051)\n","INFO:root:Training: Train Iter(705500)\tEnv Step(705728)\tLoss(-29.549)\n","INFO:root:Evaluation: Train Iter(705536) Env Step(705728) Episode Return(309.751) \n","INFO:root:Training: Train Iter(705600)\tEnv Step(705856)\tLoss(-29.742)\n","INFO:root:Training: Train Iter(705700)\tEnv Step(705920)\tLoss(-27.222)\n","INFO:root:Training: Train Iter(705800)\tEnv Step(706048)\tLoss(-22.617)\n","INFO:root:Training: Train Iter(705900)\tEnv Step(706112)\tLoss(-28.081)\n","INFO:root:Training: Train Iter(706000)\tEnv Step(706240)\tLoss(-30.532)\n","INFO:root:Training: Train Iter(706100)\tEnv Step(706304)\tLoss(-27.494)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88296\n","train_sample_count: 2119104\n","avg_envstep_per_episode: 88296.0\n","avg_sample_per_episode: 2119104.0\n","avg_envstep_per_sec: 8519.090509245736\n","avg_train_sample_per_sec: 204458.17222189766\n","avg_episode_per_sec: 0.09648331191951771\n","reward_mean: 311.0929870605469\n","reward_std: 0.0\n","reward_max: 311.0929870605469\n","reward_min: 311.0929870605469\n","total_envstep_count: 706368\n","total_train_sample_count: 16952832\n","total_episode_count: 759\n","INFO:root:Training: Train Iter(706200)\tEnv Step(706432)\tLoss(-29.611)\n","INFO:root:Training: Train Iter(706300)\tEnv Step(706496)\tLoss(-21.612)\n","INFO:root:Training: Train Iter(706400)\tEnv Step(706624)\tLoss(-27.944)\n","INFO:root:Training: Train Iter(706500)\tEnv Step(706752)\tLoss(-14.270)\n","INFO:root:Evaluation: Train Iter(706560) Env Step(706752) Episode Return(311.232) \n","INFO:root:Training: Train Iter(706600)\tEnv Step(706816)\tLoss(-27.305)\n","INFO:root:Training: Train Iter(706700)\tEnv Step(706944)\tLoss(-28.846)\n","INFO:root:Training: Train Iter(706800)\tEnv Step(707008)\tLoss(-27.087)\n","INFO:root:Training: Train Iter(706900)\tEnv Step(707136)\tLoss(-29.115)\n","INFO:root:Training: Train Iter(707000)\tEnv Step(707200)\tLoss(-29.410)\n","INFO:root:Training: Train Iter(707100)\tEnv Step(707328)\tLoss(-29.621)\n","INFO:root:Training: Train Iter(707200)\tEnv Step(707456)\tLoss(-26.926)\n","INFO:root:Training: Train Iter(707300)\tEnv Step(707520)\tLoss(-27.060)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88447\n","train_sample_count: 2122728\n","avg_envstep_per_episode: 88447.0\n","avg_sample_per_episode: 2122728.0\n","avg_envstep_per_sec: 8865.464656749527\n","avg_train_sample_per_sec: 212771.15176198864\n","avg_episode_per_sec: 0.10023476948624065\n","reward_mean: 308.95391845703125\n","reward_std: 0.0\n","reward_max: 308.95391845703125\n","reward_min: 308.95391845703125\n","total_envstep_count: 707576\n","total_train_sample_count: 16981824\n","total_episode_count: 760\n","INFO:root:Training: Train Iter(707400)\tEnv Step(707648)\tLoss(-26.752)\n","INFO:root:Training: Train Iter(707500)\tEnv Step(707712)\tLoss(-27.543)\n","INFO:root:Evaluation: Train Iter(707584) Env Step(707776) Episode Return(316.982) \n","INFO:root:Training: Train Iter(707600)\tEnv Step(707840)\tLoss(-28.563)\n","INFO:root:Training: Train Iter(707700)\tEnv Step(707904)\tLoss(-28.721)\n","INFO:root:Training: Train Iter(707800)\tEnv Step(708032)\tLoss(-25.946)\n","INFO:root:Training: Train Iter(707900)\tEnv Step(708096)\tLoss(-27.600)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88528\n","train_sample_count: 2124672\n","avg_envstep_per_episode: 88528.0\n","avg_sample_per_episode: 2124672.0\n","avg_envstep_per_sec: 8716.409310661817\n","avg_train_sample_per_sec: 209193.8234558836\n","avg_episode_per_sec: 0.09845934970474671\n","reward_mean: 308.48785400390625\n","reward_std: 0.0\n","reward_max: 308.48785400390625\n","reward_min: 308.48785400390625\n","total_envstep_count: 708224\n","total_train_sample_count: 16997376\n","total_episode_count: 761\n","INFO:root:Training: Train Iter(708000)\tEnv Step(708224)\tLoss(-14.311)\n","INFO:root:Training: Train Iter(708100)\tEnv Step(708352)\tLoss(-26.738)\n","INFO:root:Training: Train Iter(708200)\tEnv Step(708416)\tLoss(-27.543)\n","INFO:root:Training: Train Iter(708300)\tEnv Step(708544)\tLoss(-30.235)\n","INFO:root:Training: Train Iter(708400)\tEnv Step(708608)\tLoss(-28.853)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88580\n","train_sample_count: 2125920\n","avg_envstep_per_episode: 88580.0\n","avg_sample_per_episode: 2125920.0\n","avg_envstep_per_sec: 8707.169527576312\n","avg_train_sample_per_sec: 208972.0686618315\n","avg_episode_per_sec: 0.09829724009456212\n","reward_mean: 310.2469177246094\n","reward_std: 0.0\n","reward_max: 310.2469177246094\n","reward_min: 310.2469177246094\n","total_envstep_count: 708640\n","total_train_sample_count: 17007360\n","total_episode_count: 762\n","INFO:root:Training: Train Iter(708500)\tEnv Step(708736)\tLoss(-19.912)\n","INFO:root:Training: Train Iter(708600)\tEnv Step(708800)\tLoss(-29.079)\n","INFO:root:Evaluation: Train Iter(708608) Env Step(708800) Episode Return(313.562) \n","INFO:root:Training: Train Iter(708700)\tEnv Step(708928)\tLoss(-22.490)\n","INFO:root:Training: Train Iter(708800)\tEnv Step(709056)\tLoss(-26.891)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88639\n","train_sample_count: 2127336\n","avg_envstep_per_episode: 88639.0\n","avg_sample_per_episode: 2127336.0\n","avg_envstep_per_sec: 6121.996736903992\n","avg_train_sample_per_sec: 146927.92168569582\n","avg_episode_per_sec: 0.0690666268448876\n","reward_mean: 310.1144714355469\n","reward_std: 0.0\n","reward_max: 310.1144714355469\n","reward_min: 310.1144714355469\n","total_envstep_count: 709112\n","total_train_sample_count: 17018688\n","total_episode_count: 763\n","INFO:root:Training: Train Iter(708900)\tEnv Step(709120)\tLoss(-29.022)\n","INFO:root:Training: Train Iter(709000)\tEnv Step(709248)\tLoss(-30.909)\n","INFO:root:Training: Train Iter(709100)\tEnv Step(709312)\tLoss(-11.434)\n","INFO:root:Training: Train Iter(709200)\tEnv Step(709440)\tLoss(-24.902)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88683\n","train_sample_count: 2128392\n","avg_envstep_per_episode: 88683.0\n","avg_sample_per_episode: 2128392.0\n","avg_envstep_per_sec: 8011.455858714229\n","avg_train_sample_per_sec: 192274.9406091415\n","avg_episode_per_sec: 0.0903381240904596\n","reward_mean: 310.0602722167969\n","reward_std: 0.0\n","reward_max: 310.0602722167969\n","reward_min: 310.0602722167969\n","total_envstep_count: 709464\n","total_train_sample_count: 17027136\n","total_episode_count: 764\n","INFO:root:Training: Train Iter(709300)\tEnv Step(709504)\tLoss(-27.241)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88686\n","train_sample_count: 2128464\n","avg_envstep_per_episode: 88686.0\n","avg_sample_per_episode: 2128464.0\n","avg_envstep_per_sec: 8676.185792143813\n","avg_train_sample_per_sec: 208228.45901145152\n","avg_episode_per_sec: 0.09783038802227874\n","reward_mean: 308.44635009765625\n","reward_std: 0.0\n","reward_max: 308.44635009765625\n","reward_min: 308.44635009765625\n","total_envstep_count: 709576\n","total_train_sample_count: 17029824\n","total_episode_count: 765\n","INFO:root:Training: Train Iter(709400)\tEnv Step(709632)\tLoss(-30.101)\n","INFO:root:Training: Train Iter(709500)\tEnv Step(709696)\tLoss(-27.391)\n","INFO:root:Training: Train Iter(709600)\tEnv Step(709824)\tLoss(-27.692)\n","INFO:root:Evaluation: Train Iter(709632) Env Step(709824) Episode Return(313.889) \n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 88737\n","train_sample_count: 2129688\n","avg_envstep_per_episode: 88737.0\n","avg_sample_per_episode: 2129688.0\n","avg_envstep_per_sec: 8786.263935976292\n","avg_train_sample_per_sec: 210870.33446343103\n","avg_episode_per_sec: 0.09901466058100108\n","reward_mean: 309.9836120605469\n","reward_std: 0.0\n","reward_max: 309.9836120605469\n","reward_min: 309.9836120605469\n","total_envstep_count: 709896\n","total_train_sample_count: 17037504\n","total_episode_count: 766\n","INFO:root:Training: Train Iter(709700)\tEnv Step(709952)\tLoss(-26.395)\n","INFO:root:Training: Train Iter(709800)\tEnv Step(710016)\tLoss(-22.849)\n","INFO:root:Training: Train Iter(709900)\tEnv Step(710144)\tLoss(-26.283)\n","INFO:root:Training: Train Iter(710000)\tEnv Step(710208)\tLoss(-17.165)\n","INFO:root:Training: Train Iter(710100)\tEnv Step(710336)\tLoss(-27.169)\n","INFO:root:Training: Train Iter(710200)\tEnv Step(710400)\tLoss(-20.886)\n","INFO:root:Training: Train Iter(710300)\tEnv Step(710528)\tLoss(-28.780)\n","INFO:root:Training: Train Iter(710400)\tEnv Step(710656)\tLoss(-21.440)\n","INFO:root:Training: Train Iter(710500)\tEnv Step(710720)\tLoss(-30.916)\n","INFO:root:Training: Train Iter(710600)\tEnv Step(710848)\tLoss(-27.664)\n","INFO:root:Evaluation: Train Iter(710656) Env Step(710848) Episode Return(-126.052) \n","INFO:root:Training: Train Iter(710700)\tEnv Step(710912)\tLoss(-25.576)\n","INFO:root:Training: Train Iter(710800)\tEnv Step(711040)\tLoss(-27.477)\n","INFO:root:Training: Train Iter(710900)\tEnv Step(711104)\tLoss(-7.505)\n","INFO:root:Training: Train Iter(711000)\tEnv Step(711232)\tLoss(-27.978)\n","INFO:root:Training: Train Iter(711100)\tEnv Step(711296)\tLoss(-28.060)\n","INFO:root:Training: Train Iter(711200)\tEnv Step(711424)\tLoss(-24.849)\n","INFO:root:Training: Train Iter(711300)\tEnv Step(711552)\tLoss(-29.210)\n","INFO:root:Training: Train Iter(711400)\tEnv Step(711616)\tLoss(-24.589)\n","INFO:root:Training: Train Iter(711500)\tEnv Step(711744)\tLoss(-28.519)\n","INFO:root:Training: Train Iter(711600)\tEnv Step(711808)\tLoss(-29.013)\n","INFO:root:Evaluation: Train Iter(711680) Env Step(711872) Episode Return(-122.751) \n","INFO:root:Training: Train Iter(711700)\tEnv Step(711936)\tLoss(-25.650)\n","INFO:root:Training: Train Iter(711800)\tEnv Step(712000)\tLoss(-29.519)\n","INFO:root:Training: Train Iter(711900)\tEnv Step(712128)\tLoss(-14.474)\n","INFO:root:Training: Train Iter(712000)\tEnv Step(712256)\tLoss(-21.419)\n","INFO:root:Training: Train Iter(712100)\tEnv Step(712320)\tLoss(-30.301)\n","INFO:root:Training: Train Iter(712200)\tEnv Step(712448)\tLoss(-14.633)\n","INFO:root:Training: Train Iter(712300)\tEnv Step(712512)\tLoss(-21.240)\n","INFO:root:Training: Train Iter(712400)\tEnv Step(712640)\tLoss(-28.735)\n","INFO:root:Training: Train Iter(712500)\tEnv Step(712704)\tLoss(-14.199)\n","INFO:root:Training: Train Iter(712600)\tEnv Step(712832)\tLoss(-28.147)\n","INFO:root:Training: Train Iter(712700)\tEnv Step(712896)\tLoss(-20.490)\n","INFO:root:Evaluation: Train Iter(712704) Env Step(712896) Episode Return(-120.213) \n","INFO:root:Training: Train Iter(712800)\tEnv Step(713024)\tLoss(-29.018)\n","INFO:root:Training: Train Iter(712900)\tEnv Step(713152)\tLoss(-28.728)\n","INFO:root:Training: Train Iter(713000)\tEnv Step(713216)\tLoss(-20.831)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89166\n","train_sample_count: 2139984\n","avg_envstep_per_episode: 89166.0\n","avg_sample_per_episode: 2139984.0\n","avg_envstep_per_sec: 8519.117100136615\n","avg_train_sample_per_sec: 204458.81040327874\n","avg_episode_per_sec: 0.0955422145227622\n","reward_mean: 311.1147155761719\n","reward_std: 0.0\n","reward_max: 311.1147155761719\n","reward_min: 311.1147155761719\n","total_envstep_count: 713328\n","total_train_sample_count: 17119872\n","total_episode_count: 767\n","INFO:root:Training: Train Iter(713100)\tEnv Step(713344)\tLoss(-28.407)\n","INFO:root:Training: Train Iter(713200)\tEnv Step(713408)\tLoss(-28.940)\n","INFO:root:Training: Train Iter(713300)\tEnv Step(713536)\tLoss(-29.525)\n","INFO:root:Training: Train Iter(713400)\tEnv Step(713600)\tLoss(-29.044)\n","INFO:root:Training: Train Iter(713500)\tEnv Step(713728)\tLoss(-27.231)\n","INFO:root:Training: Train Iter(713600)\tEnv Step(713856)\tLoss(-27.081)\n","INFO:root:Training: Train Iter(713700)\tEnv Step(713920)\tLoss(-19.215)\n","INFO:root:Evaluation: Train Iter(713728) Env Step(713920) Episode Return(-106.899) \n","INFO:root:Training: Train Iter(713800)\tEnv Step(714048)\tLoss(-28.056)\n","INFO:root:Training: Train Iter(713900)\tEnv Step(714112)\tLoss(-22.178)\n","INFO:root:Training: Train Iter(714000)\tEnv Step(714240)\tLoss(-26.842)\n","INFO:root:Training: Train Iter(714100)\tEnv Step(714304)\tLoss(-30.041)\n","INFO:root:Training: Train Iter(714200)\tEnv Step(714432)\tLoss(-26.670)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89312\n","train_sample_count: 2143488\n","avg_envstep_per_episode: 89312.0\n","avg_sample_per_episode: 2143488.0\n","avg_envstep_per_sec: 8860.224305674996\n","avg_train_sample_per_sec: 212645.38333619988\n","avg_episode_per_sec: 0.09920530618142014\n","reward_mean: 310.3334655761719\n","reward_std: 0.0\n","reward_max: 310.3334655761719\n","reward_min: 310.3334655761719\n","total_envstep_count: 714496\n","total_train_sample_count: 17147904\n","total_episode_count: 768\n","INFO:root:Training: Train Iter(714300)\tEnv Step(714496)\tLoss(-25.268)\n","INFO:root:Training: Train Iter(714400)\tEnv Step(714624)\tLoss(-25.290)\n","INFO:root:Training: Train Iter(714500)\tEnv Step(714752)\tLoss(-27.894)\n","INFO:root:Training: Train Iter(714600)\tEnv Step(714816)\tLoss(-27.872)\n","INFO:root:Training: Train Iter(714700)\tEnv Step(714944)\tLoss(-20.751)\n","INFO:root:Evaluation: Train Iter(714752) Env Step(714944) Episode Return(-88.016) \n","INFO:root:Training: Train Iter(714800)\tEnv Step(715008)\tLoss(-18.878)\n","INFO:root:Training: Train Iter(714900)\tEnv Step(715136)\tLoss(-19.754)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89397\n","train_sample_count: 2145528\n","avg_envstep_per_episode: 89397.0\n","avg_sample_per_episode: 2145528.0\n","avg_envstep_per_sec: 8713.44751916414\n","avg_train_sample_per_sec: 209122.74045993932\n","avg_episode_per_sec: 0.09746912669512554\n","reward_mean: 308.9013977050781\n","reward_std: 0.0\n","reward_max: 308.9013977050781\n","reward_min: 308.9013977050781\n","total_envstep_count: 715176\n","total_train_sample_count: 17164224\n","total_episode_count: 769\n","INFO:root:Training: Train Iter(715000)\tEnv Step(715200)\tLoss(-27.801)\n","INFO:root:Training: Train Iter(715100)\tEnv Step(715328)\tLoss(-29.809)\n","INFO:root:Training: Train Iter(715200)\tEnv Step(715456)\tLoss(-28.313)\n","INFO:root:Training: Train Iter(715300)\tEnv Step(715520)\tLoss(-27.531)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89448\n","train_sample_count: 2146752\n","avg_envstep_per_episode: 89448.0\n","avg_sample_per_episode: 2146752.0\n","avg_envstep_per_sec: 8704.431064716719\n","avg_train_sample_per_sec: 208906.34555320124\n","avg_episode_per_sec: 0.097312752266308\n","reward_mean: 310.4617004394531\n","reward_std: 0.0\n","reward_max: 310.4617004394531\n","reward_min: 310.4617004394531\n","total_envstep_count: 715584\n","total_train_sample_count: 17174016\n","total_episode_count: 770\n","INFO:root:Training: Train Iter(715400)\tEnv Step(715648)\tLoss(-29.472)\n","INFO:root:Training: Train Iter(715500)\tEnv Step(715712)\tLoss(-26.763)\n","INFO:root:Training: Train Iter(715600)\tEnv Step(715840)\tLoss(-20.448)\n","INFO:root:Training: Train Iter(715700)\tEnv Step(715904)\tLoss(-26.997)\n","INFO:root:Evaluation: Train Iter(715776) Env Step(715968) Episode Return(-92.599) \n","INFO:root:Training: Train Iter(715800)\tEnv Step(716032)\tLoss(-28.021)\n","INFO:root:Training: Train Iter(715900)\tEnv Step(716096)\tLoss(-19.734)\n","INFO:root:Training: Train Iter(716000)\tEnv Step(716224)\tLoss(-27.385)\n","INFO:root:Training: Train Iter(716100)\tEnv Step(716352)\tLoss(-22.265)\n","INFO:root:Training: Train Iter(716200)\tEnv Step(716416)\tLoss(-22.398)\n","INFO:root:Training: Train Iter(716300)\tEnv Step(716544)\tLoss(-22.717)\n","INFO:root:Training: Train Iter(716400)\tEnv Step(716608)\tLoss(-11.542)\n","INFO:root:Training: Train Iter(716500)\tEnv Step(716736)\tLoss(-19.900)\n","INFO:root:Training: Train Iter(716600)\tEnv Step(716800)\tLoss(-20.979)\n","INFO:root:Training: Train Iter(716700)\tEnv Step(716928)\tLoss(-10.003)\n","INFO:root:Evaluation: Train Iter(716800) Env Step(716992) Episode Return(-95.989) \n","INFO:root:Training: Train Iter(716800)\tEnv Step(717056)\tLoss(-15.936)\n","INFO:root:Training: Train Iter(716900)\tEnv Step(717120)\tLoss(-6.477)\n","INFO:root:Training: Train Iter(717000)\tEnv Step(717248)\tLoss(-14.797)\n","INFO:root:Training: Train Iter(717100)\tEnv Step(717312)\tLoss(-14.352)\n","INFO:root:Training: Train Iter(717200)\tEnv Step(717440)\tLoss(-15.812)\n","INFO:root:Training: Train Iter(717300)\tEnv Step(717504)\tLoss(-14.070)\n","INFO:root:Training: Train Iter(717400)\tEnv Step(717632)\tLoss(-6.818)\n","INFO:root:Training: Train Iter(717500)\tEnv Step(717696)\tLoss(-17.834)\n","INFO:root:Training: Train Iter(717600)\tEnv Step(717824)\tLoss(-18.694)\n","INFO:root:Training: Train Iter(717700)\tEnv Step(717952)\tLoss(-18.789)\n","INFO:root:Training: Train Iter(717800)\tEnv Step(718016)\tLoss(-19.566)\n","INFO:root:Evaluation: Train Iter(717824) Env Step(718016) Episode Return(-97.677) \n","INFO:root:Training: Train Iter(717900)\tEnv Step(718144)\tLoss(-20.779)\n","INFO:root:Training: Train Iter(718000)\tEnv Step(718208)\tLoss(-26.583)\n","INFO:root:Training: Train Iter(718100)\tEnv Step(718336)\tLoss(-25.492)\n","INFO:root:Training: Train Iter(718200)\tEnv Step(718400)\tLoss(-27.712)\n","INFO:root:Training: Train Iter(718300)\tEnv Step(718528)\tLoss(-12.316)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89831\n","train_sample_count: 2155944\n","avg_envstep_per_episode: 89831.0\n","avg_sample_per_episode: 2155944.0\n","avg_envstep_per_sec: 8013.2356536301895\n","avg_train_sample_per_sec: 192317.65568712456\n","avg_episode_per_sec: 0.08920345597433169\n","reward_mean: 124.9381332397461\n","reward_std: 0.0\n","reward_max: 124.9381332397461\n","reward_min: 124.9381332397461\n","total_envstep_count: 718648\n","total_train_sample_count: 17247552\n","total_episode_count: 771\n","INFO:root:Training: Train Iter(718400)\tEnv Step(718656)\tLoss(-29.919)\n","INFO:root:Training: Train Iter(718500)\tEnv Step(718720)\tLoss(-32.278)\n","INFO:root:Training: Train Iter(718600)\tEnv Step(718848)\tLoss(-32.560)\n","INFO:root:Training: Train Iter(718700)\tEnv Step(718912)\tLoss(-35.314)\n","INFO:root:Training: Train Iter(718800)\tEnv Step(719040)\tLoss(-36.321)\n","INFO:root:Evaluation: Train Iter(718848) Env Step(719040) Episode Return(-116.364) \n","INFO:root:Training: Train Iter(718900)\tEnv Step(719104)\tLoss(-34.111)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89890\n","train_sample_count: 2157360\n","avg_envstep_per_episode: 89890.0\n","avg_sample_per_episode: 2157360.0\n","avg_envstep_per_sec: 8012.697385642826\n","avg_train_sample_per_sec: 192304.7372554278\n","avg_episode_per_sec: 0.08913891851866532\n","reward_mean: -114.36591339111328\n","reward_std: 0.0\n","reward_max: -114.36591339111328\n","reward_min: -114.36591339111328\n","total_envstep_count: 719120\n","total_train_sample_count: 17258880\n","total_episode_count: 772\n","INFO:root:Training: Train Iter(719000)\tEnv Step(719232)\tLoss(-36.005)\n","INFO:root:Training: Train Iter(719100)\tEnv Step(719296)\tLoss(-25.089)\n","INFO:root:Training: Train Iter(719200)\tEnv Step(719424)\tLoss(-16.817)\n","INFO:root:Training: Train Iter(719300)\tEnv Step(719552)\tLoss(-37.372)\n","INFO:root:Training: Train Iter(719400)\tEnv Step(719616)\tLoss(-36.990)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89954\n","train_sample_count: 2158896\n","avg_envstep_per_episode: 89954.0\n","avg_sample_per_episode: 2158896.0\n","avg_envstep_per_sec: 8786.60607196008\n","avg_train_sample_per_sec: 210878.5457270419\n","avg_episode_per_sec: 0.09767888111657157\n","reward_mean: 156.31002807617188\n","reward_std: 0.0\n","reward_max: 156.31002807617188\n","reward_min: 156.31002807617188\n","total_envstep_count: 719632\n","total_train_sample_count: 17271168\n","total_episode_count: 773\n","INFO:root:Training: Train Iter(719500)\tEnv Step(719744)\tLoss(-38.873)\n","INFO:root:Training: Train Iter(719600)\tEnv Step(719808)\tLoss(-36.967)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89979\n","train_sample_count: 2159496\n","avg_envstep_per_episode: 89979.0\n","avg_sample_per_episode: 2159496.0\n","avg_envstep_per_sec: 6122.664214942483\n","avg_train_sample_per_sec: 146943.9411586196\n","avg_episode_per_sec: 0.06804547966683874\n","reward_mean: 161.7405242919922\n","reward_std: 0.0\n","reward_max: 161.7405242919922\n","reward_min: 161.7405242919922\n","total_envstep_count: 719832\n","total_train_sample_count: 17275968\n","total_episode_count: 774\n","INFO:root:Training: Train Iter(719700)\tEnv Step(719936)\tLoss(-37.383)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 89995\n","train_sample_count: 2159880\n","avg_envstep_per_episode: 89995.0\n","avg_sample_per_episode: 2159880.0\n","avg_envstep_per_sec: 8786.606700835231\n","avg_train_sample_per_sec: 210878.56082004553\n","avg_episode_per_sec: 0.09763438747525119\n","reward_mean: -108.37432861328125\n","reward_std: 0.0\n","reward_max: -108.37432861328125\n","reward_min: -108.37432861328125\n","total_envstep_count: 719960\n","total_train_sample_count: 17279040\n","total_episode_count: 775\n","INFO:root:Training: Train Iter(719800)\tEnv Step(720000)\tLoss(-13.368)\n","INFO:root:Evaluation: Train Iter(719872) Env Step(720064) Episode Return(-105.951) \n","INFO:root:Training: Train Iter(719900)\tEnv Step(720128)\tLoss(-40.337)\n","INFO:root:Training: Train Iter(720000)\tEnv Step(720256)\tLoss(-40.298)\n","INFO:root:Training: Train Iter(720100)\tEnv Step(720320)\tLoss(-38.917)\n","INFO:root:Training: Train Iter(720200)\tEnv Step(720448)\tLoss(-38.654)\n","INFO:root:Training: Train Iter(720300)\tEnv Step(720512)\tLoss(-36.254)\n","INFO:root:Training: Train Iter(720400)\tEnv Step(720640)\tLoss(-36.978)\n","INFO:root:Training: Train Iter(720500)\tEnv Step(720704)\tLoss(-39.503)\n","INFO:root:collect end:\n","episode_count: 1\n","envstep_count: 90101\n","train_sample_count: 2162424\n","avg_envstep_per_episode: 90101.0\n","avg_sample_per_episode: 2162424.0\n","avg_envstep_per_sec: 8671.293494245805\n","avg_train_sample_per_sec: 208111.0438618993\n","avg_episode_per_sec: 0.09623970315807598\n","reward_mean: 269.3097229003906\n","reward_std: 0.0\n","reward_max: 269.3097229003906\n","reward_min: 269.3097229003906\n","total_envstep_count: 720808\n","total_train_sample_count: 17299392\n","total_episode_count: 776\n","INFO:root:Training: Train Iter(720600)\tEnv Step(720832)\tLoss(-38.588)\n","INFO:root:Training: Train Iter(720700)\tEnv Step(720896)\tLoss(-37.637)\n","INFO:root:Training: Train Iter(720800)\tEnv Step(721024)\tLoss(-36.600)\n","INFO:root:Evaluation: Train Iter(720896) Env Step(721088) Episode Return(272.935) \n","INFO:root:Training: Train Iter(720900)\tEnv Step(721152)\tLoss(-28.367)\n","INFO:root:Training: Train Iter(721000)\tEnv Step(721216)\tLoss(-32.385)\n","INFO:root:Training: Train Iter(721100)\tEnv Step(721344)\tLoss(-33.232)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-00f6257b2c46>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCkptSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, max_step)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_middleware\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Sync should be called before backward, otherwise it is possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# that some generators have not been pushed to backward_stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mruntime_handler\u001b[0;34m(task, async_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mruntime_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fn, ctx)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mruntime_handler\u001b[0;34m(task, async_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mruntime_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fn, ctx)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/middleware/learner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_output_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_per_collect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mruntime_handler\u001b[0;34m(task, async_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mruntime_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fn, ctx)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/middleware/functional/data_processor.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuffered_data\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# B, unroll_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mbuffered_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbuffered_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# like sqil, r2d3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/data/buffer/buffer.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(buffer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_middleware\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/data/buffer/buffer.py\u001b[0m in \u001b[0;36mwrap_handler\u001b[0;34m(middleware, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrap_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiddleware\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmiddleware\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/data/buffer/deque_buffer.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, size, indices, replace, sample_range, ignore_insufficient, groupby, unroll_len)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0msampled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mvalue_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    501\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0mselected_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# /content/bipedalwalker_sac_config0/total_config.py 里面stop_value是不是得改一下？300貌似是到顶了"],"metadata":{"id":"5cEXz1AShvw1"},"execution_count":null,"outputs":[]}]}