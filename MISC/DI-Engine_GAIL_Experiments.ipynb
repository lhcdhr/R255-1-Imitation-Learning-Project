{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+G7wxQPmW/dmImnycoUbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bde63b395e214fd3be471faea7427b33":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_462e36fb06bd4544ad82a5b201d64142","IPY_MODEL_c884797cbd0840fabc6b6e7409f25c44"],"layout":"IPY_MODEL_281ba6c596894037aee7301b6ceb693b"}},"462e36fb06bd4544ad82a5b201d64142":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7208ca175e0b448dad3b7f8ca68f62f3","placeholder":"​","style":"IPY_MODEL_040ea7f7b5204a5fba87f1c6b2f9999e","value":"0.015 MB of 0.015 MB uploaded\r"}},"c884797cbd0840fabc6b6e7409f25c44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_139cef0b0cd74470bbdb762a940cb18d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df3c108ceb824d82b88e0bad160e193c","value":1}},"281ba6c596894037aee7301b6ceb693b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7208ca175e0b448dad3b7f8ca68f62f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"040ea7f7b5204a5fba87f1c6b2f9999e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"139cef0b0cd74470bbdb762a940cb18d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df3c108ceb824d82b88e0bad160e193c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install DI-engine"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ZWZBQuta-rCu","executionInfo":{"status":"ok","timestamp":1709145062711,"user_tz":0,"elapsed":49150,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"cd4cf5ca-d0ec-49d5-e887-dc3d8e299d4b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting DI-engine\n","  Downloading DI_engine-0.5.1-py3-none-any.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools<=66.1.1 (from DI-engine)\n","  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yapf==0.29.0 (from DI-engine)\n","  Downloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gym==0.25.1 (from DI-engine)\n","  Downloading gym-0.25.1.tar.gz (732 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.2/732.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting gymnasium (from DI-engine)\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.1.0+cu121)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.25.2)\n","Collecting DI-treetensor>=0.4.0 (from DI-engine)\n","  Downloading DI_treetensor-0.4.1-py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting DI-toolkit>=0.1.0 (from DI-engine)\n","  Downloading DI_toolkit-0.2.1-py3-none-any.whl (29 kB)\n","Collecting trueskill (from DI-engine)\n","  Downloading trueskill-0.4.5.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tensorboardX>=2.2 (from DI-engine)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting wandb (from DI-engine)\n","  Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from DI-engine) (3.7.1)\n","Collecting easydict==1.9 (from DI-engine)\n","  Downloading easydict-1.9.tar.gz (6.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from DI-engine) (6.0.1)\n","Collecting enum-tools (from DI-engine)\n","  Downloading enum_tools-0.11.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.2.1)\n","Collecting hickle (from DI-engine)\n","  Downloading hickle-5.0.2-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from DI-engine) (0.9.0)\n","Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (8.1.7)\n","Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from DI-engine) (2.31.0)\n","Collecting flask~=1.1.2 (from DI-engine)\n","  Downloading Flask-1.1.4-py2.py3-none-any.whl (94 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses~=0.12.1 (from DI-engine)\n","  Downloading responses-0.12.1-py2.py3-none-any.whl (16 kB)\n","Collecting URLObject>=2.4.0 (from DI-engine)\n","  Downloading URLObject-2.4.3.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting MarkupSafe==2.0.1 (from DI-engine)\n","  Downloading MarkupSafe-2.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n","Collecting pynng (from DI-engine)\n","  Downloading pynng-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.4/936.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from DI-engine) (1.3.0)\n","Collecting redis (from DI-engine)\n","  Downloading redis-5.0.2-py3-none-any.whl (251 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mpire>=2.3.5 (from DI-engine)\n","  Downloading mpire-2.10.0-py3-none-any.whl (272 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.1/272.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.25.1->DI-engine) (0.0.8)\n","Collecting hbutils>=0.9.1 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading hbutils-0.9.3-py3-none-any.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich>=12.2.0 in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (13.7.0)\n","Collecting yattag>=1.14.0 (from DI-toolkit>=0.1.0->DI-engine)\n","  Downloading yattag-1.15.2.tar.gz (28 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.5.3)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (2.15.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (4.66.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.11.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (1.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from DI-toolkit>=0.1.0->DI-engine) (0.13.1)\n","Collecting treevalue>=1.4.11 (from DI-treetensor>=0.4.0->DI-engine)\n","  Downloading treevalue-1.4.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Werkzeug<2.0,>=0.15 (from flask~=1.1.2->DI-engine)\n","  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Jinja2<3.0,>=2.10.1 (from flask~=1.1.2->DI-engine)\n","  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itsdangerous<2.0,>=0.24 (from flask~=1.1.2->DI-engine)\n","  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting click>=7.0.0 (from DI-engine)\n","  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from mpire>=2.3.5->DI-engine) (2.16.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->DI-engine) (2024.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from responses~=0.12.1->DI-engine) (1.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (23.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->DI-engine) (3.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (3.2.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->DI-engine) (2.1.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium->DI-engine)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from hickle->DI-engine) (3.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->DI-engine) (2.8.2)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from pynng->DI-engine) (1.16.0)\n","Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis->DI-engine) (4.0.3)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->DI-engine)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb->DI-engine)\n","  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->DI-engine)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting setproctitle (from wandb->DI-engine)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->DI-engine) (1.4.4)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytimeparse>=1.1.8 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n","Collecting bitmath>=1.3.3.1 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading bitmath-1.3.3.1.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting chardet<5,>=3.0.4 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deprecation>=2.0.0 (from hbutils>=0.9.1->DI-toolkit>=0.1.0->DI-engine)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (3.0.0)\n","Requirement already satisfied: graphviz>=0.17 in /usr/local/lib/python3.10/dist-packages (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine) (0.20.1)\n","Collecting dill>=0.3.4 (from treevalue>=1.4.11->DI-treetensor>=0.4.0->DI-engine)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->pynng->DI-engine) (2.21)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->DI-toolkit>=0.1.0->DI-engine) (2023.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->DI-toolkit>=0.1.0->DI-engine) (3.3.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->DI-engine) (1.3.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.60.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.7.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->DI-engine)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.2.0->DI-toolkit>=0.1.0->DI-engine) (0.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->DI-toolkit>=0.1.0->DI-engine) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->DI-toolkit>=0.1.0->DI-engine) (3.2.2)\n","Building wheels for collected packages: easydict, gym, URLObject, trueskill, yattag, bitmath\n","  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6344 sha256=d115beee94e523a9dd4830518536c6b01e5cfaa59d51f2fedae64e2192c8bdb8\n","  Stored in directory: /root/.cache/pip/wheels/fd/d2/35/4c11d19a72280492846f4c4df975311a2bac475e8021f86c1d\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.25.1-py3-none-any.whl size=849015 sha256=cabffeae017ed7c8d7633c3f3e134a1f585c0f08cec1b072dfaa5dadd1713aff\n","  Stored in directory: /root/.cache/pip/wheels/2e/3b/df/78994c45c86a980cd5d8404c6d38cd28b871d5120e45c32ce4\n","  Building wheel for URLObject (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for URLObject: filename=URLObject-2.4.3-py3-none-any.whl size=14511 sha256=f05df21ee9da4e28e48b4c2df5e12fd083bd47d01e26048557b58e0924f5ab65\n","  Stored in directory: /root/.cache/pip/wheels/0d/a2/8a/05c4a3cbe66487af088bc8967fad6de3cc30f4680a5e2e27b8\n","  Building wheel for trueskill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for trueskill: filename=trueskill-0.4.5-py3-none-any.whl size=18049 sha256=2dfd038006de64674d8fdd9c11f8342b53919fb0fff5a73993a64ec523d49091\n","  Stored in directory: /root/.cache/pip/wheels/b9/4f/29/c79f0a2956775524c7a23638ac2b6fbb516c680f8e5eed9b53\n","  Building wheel for yattag (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yattag: filename=yattag-1.15.2-py3-none-any.whl size=15668 sha256=e8a7d652e3476df1c41411e7ba2234dc71ef1cb347c80b996ebbd85938b00971\n","  Stored in directory: /root/.cache/pip/wheels/3f/6e/e5/d526243c27041915f63eacc0804babeb86b6973b0bc1991f06\n","  Building wheel for bitmath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bitmath: filename=bitmath-1.3.3.1-py3-none-any.whl size=23976 sha256=4312882bb94fa35f3a8ed6bda0874cc8005aa70305e216df8d61b19ffde4c6bd\n","  Stored in directory: /root/.cache/pip/wheels/2d/32/d2/936069b5a9583c55bc7c7dce6c746a543ce61d7dfbb4013e13\n","Successfully built easydict gym URLObject trueskill yattag bitmath\n","Installing collected packages: yattag, yapf, URLObject, pytimeparse, farama-notifications, easydict, bitmath, Werkzeug, trueskill, tensorboardX, smmap, setuptools, setproctitle, sentry-sdk, redis, mpire, MarkupSafe, itsdangerous, gymnasium, gym, enum-tools, docker-pycreds, dill, deprecation, click, chardet, responses, pynng, Jinja2, hickle, hbutils, gitdb, treevalue, GitPython, flask, wandb, DI-treetensor, DI-toolkit, DI-engine\n","  Attempting uninstall: easydict\n","    Found existing installation: easydict 1.12\n","    Uninstalling easydict-1.12:\n","      Successfully uninstalled easydict-1.12\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.0.1\n","    Uninstalling Werkzeug-3.0.1:\n","      Successfully uninstalled Werkzeug-3.0.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: itsdangerous\n","    Found existing installation: itsdangerous 2.1.2\n","    Uninstalling itsdangerous-2.1.2:\n","      Successfully uninstalled itsdangerous-2.1.2\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","  Attempting uninstall: click\n","    Found existing installation: click 8.1.7\n","    Uninstalling click-8.1.7:\n","      Successfully uninstalled click-8.1.7\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 3.1.3\n","    Uninstalling Jinja2-3.1.3:\n","      Successfully uninstalled Jinja2-3.1.3\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 2.2.5\n","    Uninstalling Flask-2.2.5:\n","      Successfully uninstalled Flask-2.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","branca 0.7.1 requires jinja2>=3, but you have jinja2 2.11.3 which is incompatible.\n","dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n","fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n","nbconvert 6.5.4 requires jinja2>=3.0, but you have jinja2 2.11.3 which is incompatible.\n","pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed DI-engine-0.5.1 DI-toolkit-0.2.1 DI-treetensor-0.4.1 GitPython-3.1.42 Jinja2-2.11.3 MarkupSafe-2.0.1 URLObject-2.4.3 Werkzeug-1.0.1 bitmath-1.3.3.1 chardet-4.0.0 click-7.1.2 deprecation-2.1.0 dill-0.3.8 docker-pycreds-0.4.0 easydict-1.9 enum-tools-0.11.0 farama-notifications-0.0.4 flask-1.1.4 gitdb-4.0.11 gym-0.25.1 gymnasium-0.29.1 hbutils-0.9.3 hickle-5.0.2 itsdangerous-1.1.0 mpire-2.10.0 pynng-0.8.0 pytimeparse-1.1.8 redis-5.0.2 responses-0.12.1 sentry-sdk-1.40.6 setproctitle-1.3.3 setuptools-66.1.1 smmap-5.0.1 tensorboardX-2.6.2.2 treevalue-1.4.12 trueskill-0.4.5 wandb-0.16.3 yapf-0.29.0 yattag-1.15.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","pkg_resources","setuptools"]},"id":"3e43eb87768343bbad3a2f44effee9e5"}},"metadata":{}}]},{"cell_type":"code","source":["!apt-get install swig3.0\n","!ln -s /usr/bin/swig3.0 /usr/bin/swig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9usYbhe_l6c","executionInfo":{"status":"ok","timestamp":1709145261611,"user_tz":0,"elapsed":2265,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"e3324444-cc2c-4c79-be45-d86a3b11eff1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","swig3.0 is already the newest version (3.0.12-2.2ubuntu1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"viA2hufc-hfo","executionInfo":{"status":"ok","timestamp":1709145338540,"user_tz":0,"elapsed":74012,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"66b26b95-4ffd-402f-b294-05674ed46f93"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n","Collecting box2d-py==2.3.5 (from gym[box2d])\n","  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pygame==2.1.0 (from gym[box2d])\n","  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2309694 sha256=6558261a35a96998135d212fc7502b73722c42b25e160805885fd5273536aa43\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py, pygame\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"]}],"source":["!pip install gym\n","!pip install gym[box2d]"]},{"cell_type":"code","source":[],"metadata":{"id":"nQVK5F1VFLd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cartpole (for reference)"],"metadata":{"id":"R0qDhVrdsOr0"}},{"cell_type":"code","source":["from dizoo.classic_control.cartpole.config.cartpole_dqn_config import main_config, create_config\n","from ding.config import compile_config\n","import gym\n","cfg = compile_config(main_config, create_cfg=create_config, auto=True)"],"metadata":{"id":"UdXDcJENFLIG","executionInfo":{"status":"ok","timestamp":1709146737368,"user_tz":0,"elapsed":3785,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["from ding.envs import DingEnvWrapper, BaseEnvManagerV2\n","\n","collector_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"CartPole-v0\")) for _ in range(cfg.env.collector_env_num)],\n","    cfg=cfg.env.manager\n",")\n","evaluator_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"CartPole-v0\")) for _ in range(cfg.env.evaluator_env_num)],\n","    cfg=cfg.env.manager\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKRkfGkVFK_M","executionInfo":{"status":"ok","timestamp":1709146738898,"user_tz":0,"elapsed":265,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"169a9b2b-7659-4e85-c89e-1420a0dcdf62"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n"]}]},{"cell_type":"code","source":["from ding.model import DQN\n","from ding.policy import DQNPolicy\n","from ding.data import DequeBuffer\n","\n","model = DQN(**cfg.policy.model)\n","buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\n","policy = DQNPolicy(cfg.policy, model=model)"],"metadata":{"id":"cCXTxpUBFML_","executionInfo":{"status":"ok","timestamp":1709146746829,"user_tz":0,"elapsed":854,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from ding.framework import task\n","from ding.framework.context import OnlineRLContext\n","from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, eps_greedy_handler, CkptSaver\n","\n","import logging\n","logging.getLogger().setLevel(logging.INFO)\n","\n","with task.start(async_mode=False, ctx=OnlineRLContext()):\n","    # Evaluating, we place it on the first place to get the score of the random model as a benchmark value\n","    task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\n","    task.use(eps_greedy_handler(cfg))  # Decay probability of explore-exploit\n","    task.use(StepCollector(cfg, policy.collect_mode, collector_env))  # Collect environmental data\n","    task.use(data_pusher(cfg, buffer_))  # Push data to buffer\n","    task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))  # Train the model\n","    task.use(CkptSaver(policy, cfg.exp_name, train_freq=100))  # Save the model\n","    # In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\n","    task.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jxhxx85SFP3x","executionInfo":{"status":"ok","timestamp":1709146757974,"user_tz":0,"elapsed":7761,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"7f0dfe22-3999-4d65-9550-6c8523560152"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Env Space Information:\n","INFO:root:\tObservation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n","INFO:root:\tAction Space: Discrete(2)\n","INFO:root:\tReward Space: Box(-inf, inf, (1,), float32)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","INFO:root:Evaluation: Train Iter(0) Env Step(0) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(8) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(16) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(24) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(32) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(40) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(48) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(0) Env Step(56) Episode Return(9.400) \n","INFO:root:Training: Train Iter(0)\tEnv Step(64)\tLoss(1.208)\n","INFO:root:Evaluation: Train Iter(40) Env Step(120) Episode Return(10.800) \n","INFO:root:Evaluation: Train Iter(80) Env Step(184) Episode Return(21.000) \n","INFO:root:collect end:\n","episode_count: 7\n","envstep_count: 141\n","train_sample_count: 564\n","avg_envstep_per_episode: 20.142857142857142\n","avg_sample_per_episode: 80.57142857142857\n","avg_envstep_per_sec: 10840.825879538612\n","avg_train_sample_per_sec: 43363.30351815445\n","avg_episode_per_sec: 538.1970294806403\n","reward_mean: 18.857142857142858\n","reward_std: 6.6424731071786685\n","reward_max: 27.0\n","reward_min: 9.0\n","total_envstep_count: 224\n","total_train_sample_count: 896\n","total_episode_count: 7\n","INFO:root:Training: Train Iter(100)\tEnv Step(224)\tLoss(0.966)\n","INFO:root:Evaluation: Train Iter(120) Env Step(248) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(160) Env Step(312) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(200) Env Step(376) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 10\n","envstep_count: 382\n","train_sample_count: 1528\n","avg_envstep_per_episode: 38.2\n","avg_sample_per_episode: 152.8\n","avg_envstep_per_sec: 9423.01171021823\n","avg_train_sample_per_sec: 37692.04684087292\n","avg_episode_per_sec: 246.6756992203725\n","reward_mean: 18.7\n","reward_std: 7.000714249274855\n","reward_max: 32.0\n","reward_min: 12.0\n","total_envstep_count: 384\n","total_train_sample_count: 1536\n","total_episode_count: 17\n","INFO:root:Training: Train Iter(200)\tEnv Step(384)\tLoss(0.894)\n","INFO:root:Evaluation: Train Iter(240) Env Step(440) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(280) Env Step(504) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 9\n","envstep_count: 540\n","train_sample_count: 2160\n","avg_envstep_per_episode: 60.0\n","avg_sample_per_episode: 240.0\n","avg_envstep_per_sec: 10627.303630237217\n","avg_train_sample_per_sec: 42509.214520948866\n","avg_episode_per_sec: 177.12172717062026\n","reward_mean: 17.88888888888889\n","reward_std: 5.743487992586705\n","reward_max: 30.0\n","reward_min: 11.0\n","total_envstep_count: 544\n","total_train_sample_count: 2176\n","total_episode_count: 26\n","INFO:root:Training: Train Iter(300)\tEnv Step(544)\tLoss(0.860)\n","INFO:root:Evaluation: Train Iter(320) Env Step(568) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(360) Env Step(632) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(400) Env Step(696) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 3\n","envstep_count: 236\n","train_sample_count: 944\n","avg_envstep_per_episode: 78.66666666666667\n","avg_sample_per_episode: 314.6666666666667\n","avg_envstep_per_sec: 12327.949373315752\n","avg_train_sample_per_sec: 49311.79749326301\n","avg_episode_per_sec: 156.71122084723413\n","reward_mean: 17.0\n","reward_std: 6.377042156569663\n","reward_max: 26.0\n","reward_min: 12.0\n","total_envstep_count: 704\n","total_train_sample_count: 2816\n","total_episode_count: 29\n","INFO:root:Training: Train Iter(400)\tEnv Step(704)\tLoss(0.969)\n","INFO:root:Evaluation: Train Iter(440) Env Step(760) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(480) Env Step(824) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 11\n","envstep_count: 1076\n","train_sample_count: 4304\n","avg_envstep_per_episode: 97.81818181818181\n","avg_sample_per_episode: 391.27272727272725\n","avg_envstep_per_sec: 10186.108197763797\n","avg_train_sample_per_sec: 40744.43279105519\n","avg_episode_per_sec: 104.13307637119124\n","reward_mean: 23.363636363636363\n","reward_std: 11.395954044968825\n","reward_max: 45.0\n","reward_min: 9.0\n","total_envstep_count: 864\n","total_train_sample_count: 3456\n","total_episode_count: 40\n","INFO:root:Training: Train Iter(500)\tEnv Step(864)\tLoss(0.849)\n","INFO:root:Evaluation: Train Iter(520) Env Step(888) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(560) Env Step(952) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(600) Env Step(1016) Episode Return(13.800) \n","INFO:root:collect end:\n","episode_count: 6\n","envstep_count: 695\n","train_sample_count: 2780\n","avg_envstep_per_episode: 115.83333333333333\n","avg_sample_per_episode: 463.3333333333333\n","avg_envstep_per_sec: 11996.2768034799\n","avg_train_sample_per_sec: 47985.1072139196\n","avg_episode_per_sec: 103.56497959838762\n","reward_mean: 20.166666666666668\n","reward_std: 8.334999833366659\n","reward_max: 35.0\n","reward_min: 11.0\n","total_envstep_count: 1024\n","total_train_sample_count: 4096\n","total_episode_count: 46\n","INFO:root:Training: Train Iter(600)\tEnv Step(1024)\tLoss(0.695)\n","INFO:root:Evaluation: Train Iter(640) Env Step(1080) Episode Return(9.400) \n","INFO:root:Evaluation: Train Iter(680) Env Step(1144) Episode Return(9.400) \n","INFO:root:collect end:\n","episode_count: 6\n","envstep_count: 834\n","train_sample_count: 3336\n","avg_envstep_per_episode: 139.0\n","avg_sample_per_episode: 556.0\n","avg_envstep_per_sec: 10264.049154064189\n","avg_train_sample_per_sec: 41056.196616256755\n","avg_episode_per_sec: 73.84208024506611\n","reward_mean: 22.0\n","reward_std: 10.503967504392486\n","reward_max: 41.0\n","reward_min: 10.0\n","total_envstep_count: 1184\n","total_train_sample_count: 4736\n","total_episode_count: 52\n","INFO:root:Training: Train Iter(700)\tEnv Step(1184)\tLoss(0.613)\n","INFO:root:Evaluation: Train Iter(720) Env Step(1208) Episode Return(43.000) \n","INFO:root:Evaluation: Train Iter(760) Env Step(1272) Episode Return(55.400) \n","INFO:root:Evaluation: Train Iter(800) Env Step(1336) Episode Return(112.200) \n","INFO:root:collect end:\n","episode_count: 8\n","envstep_count: 1267\n","train_sample_count: 5068\n","avg_envstep_per_episode: 158.375\n","avg_sample_per_episode: 633.5\n","avg_envstep_per_sec: 11940.907952962669\n","avg_train_sample_per_sec: 47763.631811850675\n","avg_episode_per_sec: 75.39641959250304\n","reward_mean: 27.375\n","reward_std: 12.951230636507097\n","reward_max: 54.0\n","reward_min: 10.0\n","total_envstep_count: 1344\n","total_train_sample_count: 5376\n","total_episode_count: 60\n","INFO:root:Training: Train Iter(800)\tEnv Step(1344)\tLoss(0.755)\n","INFO:root:Evaluation: Train Iter(840) Env Step(1400) Episode Return(180.800) \n","INFO:root:Evaluation: Train Iter(880) Env Step(1464) Episode Return(200.000) \n"]}]},{"cell_type":"markdown","source":["# serial pipeline and GAIL prep"],"metadata":{"id":"GBnogsDcsVAM"}},{"cell_type":"code","source":["# gail serial pipeline\n","from typing import Optional, Tuple\n","import os\n","import torch\n","from ditk import logging\n","from functools import partial\n","from tensorboardX import SummaryWriter\n","from copy import deepcopy\n","import numpy as np\n","\n","from ding.envs import get_vec_env_setting, create_env_manager\n","from ding.worker import BaseLearner, InteractionSerialEvaluator, BaseSerialCommander, create_buffer, \\\n","    create_serial_collector\n","from ding.config import read_config, compile_config\n","from ding.policy import create_policy\n","from ding.reward_model import create_reward_model\n","from ding.utils import set_pkg_seed\n","from ding.entry import collect_demo_data\n","from ding.utils import save_file\n","#from .utils import random_collect\n","\n","\n","def save_reward_model(path, reward_model, weights_name='best'):\n","    path = os.path.join(path, 'reward_model', 'ckpt')\n","    if not os.path.exists(path):\n","        try:\n","            os.makedirs(path)\n","        except FileExistsError:\n","            pass\n","    path = os.path.join(path, 'ckpt_{}.pth.tar'.format(weights_name))\n","    state_dict = reward_model.state_dict()\n","    save_file(path, state_dict)\n","    print('Saved reward model ckpt in {}'.format(path))\n","\n","\n","def serial_pipeline_gail(\n","        input_cfg: Tuple[dict, dict],\n","        expert_cfg: Tuple[dict, dict],\n","        seed: int = 0,\n","        model: Optional[torch.nn.Module] = None,\n","        max_train_iter: Optional[int] = int(1e10),\n","        max_env_step: Optional[int] = int(1e10),\n","        collect_data: bool = True,\n",") -> 'Policy':  # noqa\n","    \"\"\"\n","    Overview:\n","        Serial pipeline entry for GAIL reward model.\n","    Arguments:\n","        - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type. \\\n","            ``str`` type means config file path. \\\n","            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n","        - expert_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Expert config in dict type. \\\n","            ``str`` type means config file path. \\\n","            ``Tuple[dict, dict]`` type means [user_config, create_cfg].\n","        - seed (:obj:`int`): Random seed.\n","        - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.\n","        - max_train_iter (:obj:`Optional[int]`): Maximum policy update iterations in training.\n","        - max_env_step (:obj:`Optional[int]`): Maximum collected environment interaction steps.\n","        - collect_data (:obj:`bool`): Collect expert data.\n","    Returns:\n","        - policy (:obj:`Policy`): Converged policy.\n","    \"\"\"\n","    if isinstance(input_cfg, str):\n","        cfg, create_cfg = read_config(input_cfg)\n","    else:\n","        cfg, create_cfg = deepcopy(input_cfg)\n","    if isinstance(expert_cfg, str):\n","        expert_cfg, expert_create_cfg = read_config(expert_cfg)\n","    else:\n","        expert_cfg, expert_create_cfg = expert_cfg\n","    create_cfg.policy.type = create_cfg.policy.type + '_command'\n","    cfg = compile_config(cfg, seed=seed, auto=True, create_cfg=create_cfg, save_cfg=True)\n","    if 'data_path' not in cfg.reward_model:\n","        cfg.reward_model.data_path = cfg.exp_name\n","    # Load expert data\n","    if collect_data:\n","        if expert_cfg.policy.get('other', None) is not None and expert_cfg.policy.other.get('eps', None) is not None:\n","            expert_cfg.policy.other.eps.collect = -1\n","        if expert_cfg.policy.get('load_path', None) is None:\n","            expert_cfg.policy.load_path = cfg.reward_model.expert_model_path\n","        collect_demo_data(\n","            (expert_cfg, expert_create_cfg),\n","            seed,\n","            state_dict_path=expert_cfg.policy.load_path,\n","            expert_data_path=cfg.reward_model.data_path + '/expert_data.pkl',\n","            collect_count=cfg.reward_model.collect_count\n","        )\n","    # Create main components: env, policy\n","    env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg.env)\n","    collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])\n","    evaluator_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in evaluator_env_cfg])\n","    collector_env.seed(cfg.seed)\n","    evaluator_env.seed(cfg.seed, dynamic_seed=False)\n","    set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)\n","    policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval', 'command'])\n","\n","    # Create worker components: learner, collector, evaluator, replay buffer, commander.\n","    tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))\n","    learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)\n","    collector = create_serial_collector(\n","        cfg.policy.collect.collector,\n","        env=collector_env,\n","        policy=policy.collect_mode,\n","        tb_logger=tb_logger,\n","        exp_name=cfg.exp_name\n","    )\n","    evaluator = InteractionSerialEvaluator(\n","        cfg.policy.eval.evaluator, evaluator_env, policy.eval_mode, tb_logger, exp_name=cfg.exp_name\n","    )\n","    replay_buffer = create_buffer(cfg.policy.other.replay_buffer, tb_logger=tb_logger, exp_name=cfg.exp_name)\n","    commander = BaseSerialCommander(\n","        cfg.policy.other.commander, learner, collector, evaluator, replay_buffer, policy.command_mode\n","    )\n","    reward_model = create_reward_model(cfg.reward_model, policy.collect_mode.get_attribute('device'), tb_logger)\n","\n","    # ==========\n","    # Main loop\n","    # ==========\n","    # Learner's before_run hook.\n","    learner.call_hook('before_run')\n","\n","    # Accumulate plenty of data at the beginning of training.\n","    if cfg.policy.get('random_collect_size', 0) > 0:\n","        random_collect(cfg.policy, policy, collector, collector_env, commander, replay_buffer)\n","    best_reward = -np.inf\n","    while True:\n","        collect_kwargs = commander.step()\n","        # Evaluate policy performance\n","        if evaluator.should_eval(learner.train_iter):\n","            stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)\n","            print(\"look here\")\n","            print(reward)\n","            # reward_mean = np.array([r['eval_episode_return'] for r in reward]).mean()\n","            reward_mean = np.array(reward['eval_episode_return']).mean()\n","            if reward_mean >= best_reward:\n","                save_reward_model(cfg.exp_name, reward_model, 'best')\n","                best_reward = reward_mean\n","            if stop:\n","                break\n","        new_data_count, target_new_data_count = 0, cfg.reward_model.get('target_new_data_count', 1)\n","        while new_data_count < target_new_data_count:\n","            new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs=collect_kwargs)\n","            new_data_count += len(new_data)\n","            # collect data for reward_model training\n","            reward_model.collect_data(new_data)\n","            replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)\n","        # update reward_model\n","        reward_model.train()\n","        reward_model.clear_data()\n","        # Learn policy from collected data\n","        for i in range(cfg.policy.learn.update_per_collect):\n","            # Learner will train ``update_per_collect`` times in one iteration.\n","            train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)\n","            if train_data is None:\n","                # It is possible that replay buffer's data count is too few to train ``update_per_collect`` times\n","                logging.warning(\n","                    \"Replay buffer's data can only train for {} steps. \".format(i) +\n","                    \"You can modify data collect config, e.g. increasing n_sample, n_episode.\"\n","                )\n","                break\n","            # update train_data reward using the augmented reward\n","            train_data_augmented = reward_model.estimate(train_data)\n","            learner.train(train_data_augmented, collector.envstep)\n","            if learner.policy.get_attribute('priority'):\n","                replay_buffer.update(learner.priority_info)\n","        if collector.envstep >= max_env_step or learner.train_iter >= max_train_iter:\n","            break\n","\n","    # Learner's after_run hook.\n","    learner.call_hook('after_run')\n","    save_reward_model(cfg.exp_name, reward_model, 'last')\n","    # evaluate\n","    # evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)\n","    return policy"],"metadata":{"id":"6uFe3QGYCiuv","executionInfo":{"status":"ok","timestamp":1709147414646,"user_tz":0,"elapsed":228,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Callable, List, Any\n","\n","from ding.policy import PolicyFactory\n","from ding.worker import IMetric, MetricSerialEvaluator\n","\n","\n","class AccMetric(IMetric):\n","\n","    def eval(self, inputs: Any, label: Any) -> dict:\n","        return {'Acc': (inputs['logit'].sum(dim=1) == label).sum().item() / label.shape[0]}\n","\n","    def reduce_mean(self, inputs: List[Any]) -> Any:\n","        s = 0\n","        for item in inputs:\n","            s += item['Acc']\n","        return {'Acc': s / len(inputs)}\n","\n","    def gt(self, metric1: Any, metric2: Any) -> bool:\n","        if metric2 is None:\n","            return True\n","        if isinstance(metric2, dict):\n","            m2 = metric2['Acc']\n","        else:\n","            m2 = metric2\n","        return metric1['Acc'] > m2\n","\n","\n","def mark_not_expert(ori_data: List[dict]) -> List[dict]:\n","    for i in range(len(ori_data)):\n","        # Set is_expert flag (expert 1, agent 0)\n","        ori_data[i]['is_expert'] = 0\n","    return ori_data\n","\n","\n","def mark_warm_up(ori_data: List[dict]) -> List[dict]:\n","    # for td3_vae\n","    for i in range(len(ori_data)):\n","        ori_data[i]['warm_up'] = True\n","    return ori_data\n","\n","\n","def random_collect( #粘过来是为了用这个\n","        policy_cfg: 'EasyDict',  # noqa\n","        policy: 'Policy',  # noqa\n","        collector: 'ISerialCollector',  # noqa\n","        collector_env: 'BaseEnvManager',  # noqa\n","        commander: 'BaseSerialCommander',  # noqa\n","        replay_buffer: 'IBuffer',  # noqa\n","        postprocess_data_fn: Optional[Callable] = None\n",") -> None:  # noqa\n","    assert policy_cfg.random_collect_size > 0\n","    if policy_cfg.get('transition_with_policy_data', False):\n","        collector.reset_policy(policy.collect_mode)\n","    else:\n","        action_space = collector_env.action_space\n","        random_policy = PolicyFactory.get_random_policy(policy.collect_mode, action_space=action_space)\n","        collector.reset_policy(random_policy)\n","    collect_kwargs = commander.step()\n","    if policy_cfg.collect.collector.type == 'episode':\n","        new_data = collector.collect(n_episode=policy_cfg.random_collect_size, policy_kwargs=collect_kwargs)\n","    else:\n","        new_data = collector.collect(\n","            n_sample=policy_cfg.random_collect_size,\n","            random_collect=True,\n","            record_random_collect=False,\n","            policy_kwargs=collect_kwargs\n","        )  # 'record_random_collect=False' means random collect without output log\n","    if postprocess_data_fn is not None:\n","        new_data = postprocess_data_fn(new_data)\n","    replay_buffer.push(new_data, cur_collector_envstep=0)\n","    collector.reset_policy(policy.collect_mode)"],"metadata":{"id":"IYU59z9T-qCX","executionInfo":{"status":"ok","timestamp":1709147423029,"user_tz":0,"elapsed":227,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# GAIL SAC bipedalwalker"],"metadata":{"id":"A-BJCDydsbbp"}},{"cell_type":"code","source":["# dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py\n","from easydict import EasyDict\n","\n","obs_shape = 24\n","act_shape = 4\n","bipedalwalker_sac_gail_default_config = dict(\n","    exp_name='bipedalwalker_sac_gail_seed0',\n","    env=dict(\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        # (bool) Scale output action into legal range.\n","        act_scale=True,\n","        n_evaluator_episode=5,\n","        stop_value=300,\n","        rew_clip=True,\n","        # The path to save the game replay\n","        replay_path=None,\n","    ),\n","    reward_model=dict(\n","        type='gail',\n","        input_size=obs_shape + act_shape,\n","        hidden_size=64,\n","        batch_size=64,\n","        learning_rate=1e-3,\n","        update_per_collect=100,\n","        # Users should add their own model path here. Model path should lead to a model.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is ``exp_name/ckpt/ckpt_best.pth.tar``.\n","        # expert_model_path='model_path_placeholder',\n","        expert_model_path='bipedalwalker_sac_config0/ckpt/ckpt_best.pth.tar',\n","        # Path where to store the reward model\n","        reward_model_path='bipedalwalker_sac_gail_seed0/reward_model/ckpt/ckpt_best.pth.tar',\n","        # Users should add their own data path here. Data path should lead to a file to store data or load the stored data.\n","        # Absolute path is recommended.\n","        # In DI-engine, it is usually located in ``exp_name`` directory\n","        data_path='bipedalwalker_sac_gail_seed0',\n","        collect_count=100000,\n","    ),\n","    policy=dict(\n","        cuda=False,\n","        priority=False,\n","        random_collect_size=1000,\n","        model=dict(\n","            obs_shape=obs_shape,\n","            action_shape=act_shape,\n","            twin_critic=True,\n","            action_space='reparameterization',\n","            actor_head_hidden_size=128,\n","            critic_head_hidden_size=128,\n","        ),\n","        learn=dict(\n","            update_per_collect=1,\n","            batch_size=128,\n","            learning_rate_q=0.001,\n","            learning_rate_policy=0.001,\n","            learning_rate_alpha=0.0003,\n","            ignore_done=True,\n","            target_theta=0.005,\n","            discount_factor=0.99,\n","            auto_alpha=True,\n","            value_network=False,\n","        ),\n","        collect=dict(\n","            n_sample=128,\n","            unroll_len=1,\n","        ),\n","        other=dict(replay_buffer=dict(replay_buffer_size=100000, ), ),\n","    ),\n",")\n","bipedalwalker_sac_gail_default_config = EasyDict(bipedalwalker_sac_gail_default_config)\n","main_config = bipedalwalker_sac_gail_default_config\n","\n","bipedalwalker_sac_gail_create_config = dict(\n","    env=dict(\n","        type='bipedalwalker',\n","        import_names=['dizoo.box2d.bipedalwalker.envs.bipedalwalker_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(\n","        type='sac',\n","        import_names=['ding.policy.sac'],\n","    ),\n","    replay_buffer=dict(type='naive', ),\n",")\n","bipedalwalker_sac_gail_create_config = EasyDict(bipedalwalker_sac_gail_create_config)\n","create_config = bipedalwalker_sac_gail_create_config\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06iKIDOZAMez","executionInfo":{"status":"ok","timestamp":1709148175019,"user_tz":0,"elapsed":345,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"7abef607-65c5-467a-9088-c425a0bd709c"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["# dizoo.box2d.bipedalwalker.config bipedalwalker_sac_config\n","from easydict import EasyDict\n","\n","bipedalwalker_sac_config = dict(\n","    exp_name='bipedalwalker_sac_config0',\n","    env=dict(\n","        env_id='BipedalWalker-v3',\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        # (bool) Scale output action into legal range.\n","        act_scale=True,\n","        n_evaluator_episode=5,\n","        rew_clip=True,\n","    ),\n","    policy=dict(\n","        cuda=True,\n","        random_collect_size=10000,\n","        model=dict(\n","            obs_shape=24,\n","            action_shape=4,\n","            twin_critic=True,\n","            action_space='reparameterization',\n","            actor_head_hidden_size=128,\n","            critic_head_hidden_size=128,\n","        ),\n","        learn=dict(\n","            update_per_collect=64,\n","            batch_size=256,\n","            learning_rate_q=0.0003,\n","            learning_rate_policy=0.0003,\n","            learning_rate_alpha=0.0003,\n","            target_theta=0.005,\n","            discount_factor=0.99,\n","            auto_alpha=True,\n","            learner=dict(hook=dict(log_show_after_iter=1000, ))\n","        ),\n","        collect=dict(n_sample=64, ),\n","        other=dict(replay_buffer=dict(replay_buffer_size=300000, ), ),\n","    ),\n",")\n","bipedalwalker_sac_config = EasyDict(bipedalwalker_sac_config)\n","expert_main_config = bipedalwalker_sac_config\n","bipedalwalker_sac_create_config = dict(\n","    env=dict(\n","        type='bipedalwalker',\n","        import_names=['dizoo.box2d.bipedalwalker.envs.bipedalwalker_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(type='sac', ),\n","    replay_buffer=dict(type='naive', ),\n",")\n","bipedalwalker_sac_create_config = EasyDict(bipedalwalker_sac_create_config)\n","expert_create_config = bipedalwalker_sac_create_config"],"metadata":{"id":"7lWZ2B9THt0Z","executionInfo":{"status":"ok","timestamp":1709148177140,"user_tz":0,"elapsed":218,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["#from ding.entry import serial_pipeline_gail\n","#from dizoo.box2d.bipedalwalker.config import bipedalwalker_sac_config , bipedalwalker_sac_create_config\n","#expert_main_config = bipedalwalker_sac_config\n","#expert_create_config = bipedalwalker_sac_create_config\n","serial_pipeline_gail(\n","    [main_config, create_config],\n","    [expert_main_config, expert_create_config],\n","    seed=0,\n","    collect_data=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZoW5SImAJHQ","executionInfo":{"status":"ok","timestamp":1709172346509,"user_tz":0,"elapsed":4164945,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"95f3248c-911e-405f-e6d2-7e9e83918adc"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function SampleSerialCollector.__del__ at 0x784d49501f30>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/worker/collector/sample_serial_collector.py\", line 212, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/worker/collector/sample_serial_collector.py\", line 201, in close\n","    self._env.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/subprocess_env_manager.py\", line 635, in close\n","    p.send(['close', None, None])\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 206, in send\n","    self._send_bytes(_ForkingPickler.dumps(obj))\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 411, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","Exception ignored in: <function InteractionSerialEvaluator.__del__ at 0x784d49518550>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/worker/collector/interaction_serial_evaluator.py\", line 160, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/worker/collector/interaction_serial_evaluator.py\", line 149, in close\n","    self._env.close()\n","  File \"/usr/local/lib/python3.10/dist-packages/ding/envs/env_manager/subprocess_env_manager.py\", line 635, in close\n","    p.send(['close', None, None])\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 206, in send\n","    self._send_bytes(_ForkingPickler.dumps(obj))\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 411, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","WARNING:collector_logger:File './default_experiment/log/collector/collector_logger.txt' has already been added to logger <Logger collector_logger (INFO)>, so this configuration will be ignored.\n","INFO:buffer_logger:In the past 60.0 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.2 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n"]},{"output_type":"stream","name":"stdout","text":["Collect demo data successfully\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: DI-engine DRL Policy\n","ContinuousQAC(\n","  (actor_encoder): Identity()\n","  (critic_encoder): Identity()\n","  (actor_head): Sequential(\n","    (0): Linear(in_features=24, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): ReparameterizationHead(\n","      (main): Sequential(\n","        (0): Linear(in_features=128, out_features=128, bias=True)\n","        (1): ReLU()\n","      )\n","      (mu): Linear(in_features=128, out_features=4, bias=True)\n","      (log_sigma_layer): Linear(in_features=128, out_features=4, bias=True)\n","    )\n","  )\n","  (critic_head): ModuleList(\n","    (0-1): 2 x Sequential(\n","      (0): Linear(in_features=28, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): RegressionHead(\n","        (main): Sequential(\n","          (0): Linear(in_features=128, out_features=128, bias=True)\n","          (1): ReLU()\n","        )\n","        (last): Linear(in_features=128, out_features=1, bias=True)\n","      )\n","    )\n","  )\n","  (actor): ModuleList(\n","    (0): Identity()\n","    (1): Sequential(\n","      (0): Linear(in_features=24, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): ReparameterizationHead(\n","        (main): Sequential(\n","          (0): Linear(in_features=128, out_features=128, bias=True)\n","          (1): ReLU()\n","        )\n","        (mu): Linear(in_features=128, out_features=4, bias=True)\n","        (log_sigma_layer): Linear(in_features=128, out_features=4, bias=True)\n","      )\n","    )\n","  )\n","  (critic): ModuleList(\n","    (0): Identity()\n","    (1): ModuleList(\n","      (0-1): 2 x Sequential(\n","        (0): Linear(in_features=28, out_features=128, bias=True)\n","        (1): ReLU()\n","        (2): RegressionHead(\n","          (main): Sequential(\n","            (0): Linear(in_features=128, out_features=128, bias=True)\n","            (1): ReLU()\n","          )\n","          (last): Linear(in_features=128, out_features=1, bias=True)\n","        )\n","      )\n","    )\n","  )\n",")\n","INFO:buffer_logger:In the past 61.0 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -91.9686, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -91.9604, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -92.0540, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -91.9117, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -91.9868, current episode: 5\n","INFO:evaluator_logger:\n","+-------+------------+---------------------+---------------+---------------+\n","| Name  | train_iter | ckpt_name           | episode_count | envstep_count |\n","+-------+------------+---------------------+---------------+---------------+\n","| Value | 0.000000   | iteration_0.pth.tar | 5.000000      | 577.000000    |\n","+-------+------------+---------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 115.400000              | 0.478079      | 1206.912696         | 10.458516            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -91.976288  | 0.046125   | -91.911682 | -92.054001 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                 | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-91.96858215332031, -91.98682403564453, -91.91168212890625, -91.9603500366211, -92.05400085449219] | -91.976288               |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-91.96858215332031, -91.98682403564453, -91.91168212890625, -91.9603500366211, -92.05400085449219], 'eval_episode_return_mean': -91.97628784179688}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 0 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -0.648018       | 0.922292        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 0.580234           | 0.199940  | 0.973497     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.644131                | 1.024702             | -10.693316     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/iteration_0.pth.tar\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-------------+-------------+----------+--------------+\n","| Name  | pushed_in   | sampled_out | removed  | current_have |\n","+-------+-------------+-------------+----------+--------------+\n","| Value | 7912.000000 | 6912.000000 | 0.000000 | 7912.000000  |\n","+-------+-------------+-------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 11623\n","train_sample_count: 11623\n","avg_envstep_per_episode: 894.0769230769231\n","avg_sample_per_episode: 894.0769230769231\n","avg_envstep_per_sec: 1161.4432452075412\n","avg_train_sample_per_sec: 1161.4432452075412\n","avg_episode_per_sec: 1.299041743757897\n","reward_mean: -98.09959344970706\n","reward_std: 9.793638718755314\n","reward_max: -77.23376289029\n","reward_min: -111.34123937129478\n","total_envstep_count: 14042\n","total_train_sample_count: 13986\n","total_episode_count: 19\n","INFO:learner_logger:[RANK0]: === Training Iteration 100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -1.341119       | 1.393035        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 0.850371           | 0.194329  | 1.407637     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.716270                | 1.422239             | -11.000598     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 14\n","envstep_count: 10195\n","train_sample_count: 10195\n","avg_envstep_per_episode: 728.2142857142857\n","avg_sample_per_episode: 728.2142857142857\n","avg_envstep_per_sec: 1177.3096917466537\n","avg_train_sample_per_sec: 1177.3096917466537\n","avg_episode_per_sec: 1.616707766989029\n","reward_mean: -103.63480589326818\n","reward_std: 9.271418758308354\n","reward_max: -89.04134172863591\n","reward_min: -119.47120717107691\n","total_envstep_count: 26909\n","total_train_sample_count: 26869\n","total_episode_count: 33\n","INFO:learner_logger:[RANK0]: === Training Iteration 200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -1.507128       | 0.437727        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.024835           | 0.188581  | 0.477931     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.637247                | 0.518135             | -11.070438     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+--------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have |\n","+-------+--------------+--------------+----------+--------------+\n","| Value | 20352.000000 | 20224.000000 | 0.000000 | 28264.000000 |\n","+-------+--------------+--------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 19\n","envstep_count: 10521\n","train_sample_count: 10521\n","avg_envstep_per_episode: 553.7368421052631\n","avg_sample_per_episode: 553.7368421052631\n","avg_envstep_per_sec: 1180.25557768294\n","avg_train_sample_per_sec: 1180.25557768294\n","avg_episode_per_sec: 2.131437693753052\n","reward_mean: -106.02731096463899\n","reward_std: 9.591420274599688\n","reward_max: -82.28227718859459\n","reward_min: -119.99016799034304\n","total_envstep_count: 39773\n","total_train_sample_count: 39726\n","total_episode_count: 52\n","INFO:learner_logger:[RANK0]: === Training Iteration 300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -1.720560       | 0.262361        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.196558           | 0.183006  | 0.263397     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.618881                | 0.264432             | -11.238418     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+--------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have |\n","+-------+--------------+--------------+----------+--------------+\n","| Value | 20608.000000 | 20608.000000 | 0.000000 | 48872.000000 |\n","+-------+--------------+--------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 23\n","envstep_count: 15265\n","train_sample_count: 15265\n","avg_envstep_per_episode: 663.695652173913\n","avg_sample_per_episode: 663.695652173913\n","avg_envstep_per_sec: 1168.8759352048444\n","avg_train_sample_per_sec: 1168.8759352048444\n","avg_episode_per_sec: 1.7611625620511904\n","reward_mean: -109.7098071573447\n","reward_std: 18.855698837575943\n","reward_max: -85.94043938973996\n","reward_min: -171.97224488554343\n","total_envstep_count: 52725\n","total_train_sample_count: 52671\n","total_episode_count: 75\n","INFO:learner_logger:[RANK0]: === Training Iteration 400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -1.892017       | 0.091549        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.408424           | 0.177649  | 0.096641     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.701129                | 0.101732             | -11.577199     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 27\n","envstep_count: 12759\n","train_sample_count: 12759\n","avg_envstep_per_episode: 472.55555555555554\n","avg_sample_per_episode: 472.55555555555554\n","avg_envstep_per_sec: 1186.9383226003058\n","avg_train_sample_per_sec: 1186.9383226003058\n","avg_episode_per_sec: 2.5117434524812494\n","reward_mean: -103.77142396392\n","reward_std: 13.916979442090863\n","reward_max: -72.90610269105018\n","reward_min: -127.70245675358922\n","total_envstep_count: 65738\n","total_train_sample_count: 65670\n","total_episode_count: 102\n","INFO:learner_logger:[RANK0]: === Training Iteration 500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -2.012576       | 0.143323        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.603190           | 0.172400  | 0.140818     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.709339                | 0.138314             | -11.792627     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+--------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have |\n","+-------+--------------+--------------+----------+--------------+\n","| Value | 20480.000000 | 20608.000000 | 0.000000 | 69352.000000 |\n","+-------+--------------+--------------+----------+--------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 23\n","envstep_count: 12575\n","train_sample_count: 12575\n","avg_envstep_per_episode: 546.7391304347826\n","avg_sample_per_episode: 546.7391304347826\n","avg_envstep_per_sec: 1169.5148983137783\n","avg_train_sample_per_sec: 1169.5148983137783\n","avg_episode_per_sec: 2.13907297504707\n","reward_mean: -104.45506719272744\n","reward_std: 10.482086486822363\n","reward_max: -84.59398686915145\n","reward_min: -121.85794627433457\n","total_envstep_count: 78674\n","total_train_sample_count: 78629\n","total_episode_count: 125\n","INFO:learner_logger:[RANK0]: === Training Iteration 600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -2.229175       | 0.256653        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.776580           | 0.167309  | 0.254268     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.607485                | 0.251883             | -11.811694     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+--------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have |\n","+-------+--------------+--------------+----------+--------------+\n","| Value | 20352.000000 | 20224.000000 | 0.000000 | 89704.000000 |\n","+-------+--------------+--------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 12\n","envstep_count: 11601\n","train_sample_count: 11601\n","avg_envstep_per_episode: 966.75\n","avg_sample_per_episode: 966.75\n","avg_envstep_per_sec: 1179.6804330814236\n","avg_train_sample_per_sec: 1179.6804330814236\n","avg_episode_per_sec: 1.2202538744054034\n","reward_mean: -101.68032515856096\n","reward_std: 9.597861458698892\n","reward_max: -87.812793518506\n","reward_min: -122.88979736629129\n","total_envstep_count: 91533\n","total_train_sample_count: 91478\n","total_episode_count: 137\n","INFO:learner_logger:[RANK0]: === Training Iteration 700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -2.361139       | 0.157450        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 1.972113           | 0.162383  | 0.153550     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.671682                | 0.149650             | -12.125775     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 13150\n","train_sample_count: 13150\n","avg_envstep_per_episode: 1011.5384615384615\n","avg_sample_per_episode: 1011.5384615384615\n","avg_envstep_per_sec: 1183.8738324905457\n","avg_train_sample_per_sec: 1183.8738324905457\n","avg_episode_per_sec: 1.170369568241604\n","reward_mean: -98.41858524343054\n","reward_std: 11.401859625906775\n","reward_max: -79.32723686364226\n","reward_min: -120.57947038416643\n","total_envstep_count: 104361\n","total_train_sample_count: 104308\n","total_episode_count: 150\n","INFO:learner_logger:[RANK0]: === Training Iteration 800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -2.570989       | 0.104242        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 2.201092           | 0.157603  | 0.101616     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.661009                | 0.098989             | -12.305376     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.9 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 20096.000000 | 20096.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.9 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 35\n","envstep_count: 13125\n","train_sample_count: 13125\n","avg_envstep_per_episode: 375.0\n","avg_sample_per_episode: 375.0\n","avg_envstep_per_sec: 1164.6160732131934\n","avg_train_sample_per_sec: 1164.6160732131934\n","avg_episode_per_sec: 3.1056428619018495\n","reward_mean: -107.19982852473694\n","reward_std: 8.57539832406134\n","reward_max: -80.47041131631177\n","reward_min: -118.90668816366232\n","total_envstep_count: 117375\n","total_train_sample_count: 117305\n","total_episode_count: 185\n","INFO:learner_logger:[RANK0]: === Training Iteration 900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -2.818829       | 0.269807        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 2.382555           | 0.152966  | 0.262608     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.638528                | 0.255408             | -12.462119     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -109.6716, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -109.6364, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -109.6975, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -109.7038, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -109.7609, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 1000.000000 | iteration_1000.pth.tar | 5.000000      | 218.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 43.600000               | 0.179756      | 1212.757175         | 27.815532            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+-------------+-------------+\n","| Name  | reward_mean | reward_std | reward_max  | reward_min  |\n","+-------+-------------+------------+-------------+-------------+\n","| Value | -109.694041 | 0.041028   | -109.636360 | -109.760948 |\n","+-------+-------------+------------+-------------+-------------+\n","+-------+-----------------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                       | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-109.69754028320312, -109.67156219482422, -109.63636016845703, -109.70379638671875, -109.76094818115234] | -109.694041              |\n","+-------+-----------------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 21\n","envstep_count: 10687\n","train_sample_count: 10687\n","avg_envstep_per_episode: 508.9047619047619\n","avg_sample_per_episode: 508.9047619047619\n","avg_envstep_per_sec: 1133.1684624279685\n","avg_train_sample_per_sec: 1133.1684624279685\n","avg_episode_per_sec: 2.2266808001298157\n","reward_mean: -102.41167710804149\n","reward_std: 13.71901407601124\n","reward_max: -73.80007629981355\n","reward_min: -115.24376965935062\n","total_envstep_count: 130306\n","total_train_sample_count: 130248\n","total_episode_count: 206\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-109.69754028320312, -109.67156219482422, -109.63636016845703, -109.70379638671875, -109.76094818115234], 'eval_episode_return_mean': -109.6940414428711}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 1000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.002236       | 0.358714        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 2.582813           | 0.148443  | 0.338710     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.610696                | 0.318705             | -12.608319     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19712.000000 | 19712.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 35\n","envstep_count: 14845\n","train_sample_count: 14845\n","avg_envstep_per_episode: 424.14285714285717\n","avg_sample_per_episode: 424.14285714285717\n","avg_envstep_per_sec: 1146.7405077462106\n","avg_train_sample_per_sec: 1146.7405077462106\n","avg_episode_per_sec: 2.7036657306242753\n","reward_mean: -104.0724381526944\n","reward_std: 11.413140793085358\n","reward_max: -77.64635205038391\n","reward_min: -121.39187148585496\n","total_envstep_count: 143301\n","total_train_sample_count: 143253\n","total_episode_count: 241\n","INFO:learner_logger:[RANK0]: === Training Iteration 1100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.171941       | 0.343260        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 2.783748           | 0.144079  | 0.325152     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.594712                | 0.307043             | -12.774618     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19968.000000 | 19968.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 12\n","envstep_count: 11745\n","train_sample_count: 11745\n","avg_envstep_per_episode: 978.75\n","avg_sample_per_episode: 978.75\n","avg_envstep_per_sec: 1140.69186603686\n","avg_train_sample_per_sec: 1140.69186603686\n","avg_episode_per_sec: 1.1654578452483881\n","reward_mean: -91.09947506277103\n","reward_std: 14.790339528020173\n","reward_max: -73.53346067989708\n","reward_min: -112.22250179657402\n","total_envstep_count: 156146\n","total_train_sample_count: 156086\n","total_episode_count: 253\n","INFO:learner_logger:[RANK0]: === Training Iteration 1200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.300973       | 0.338907        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 2.928597           | 0.139860  | 0.322515     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.482590                | 0.306123             | -12.750174     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 89\n","envstep_count: 18970\n","train_sample_count: 18970\n","avg_envstep_per_episode: 213.14606741573033\n","avg_sample_per_episode: 213.14606741573033\n","avg_envstep_per_sec: 1114.0041099956343\n","avg_train_sample_per_sec: 1114.0041099956343\n","avg_episode_per_sec: 5.226482118587847\n","reward_mean: -110.6983509735628\n","reward_std: 8.322639228964707\n","reward_max: -79.33083271183567\n","reward_min: -125.57739509378506\n","total_envstep_count: 169323\n","total_train_sample_count: 169264\n","total_episode_count: 342\n","INFO:learner_logger:[RANK0]: === Training Iteration 1300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.527808       | 0.070149        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 3.215201           | 0.135784  | 0.069396     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.649485                | 0.068642             | -13.274942     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 10\n","envstep_count: 12954\n","train_sample_count: 12954\n","avg_envstep_per_episode: 1295.4\n","avg_sample_per_episode: 1295.4\n","avg_envstep_per_sec: 1164.0522303567498\n","avg_train_sample_per_sec: 1164.0522303567498\n","avg_episode_per_sec: 0.8986044699372779\n","reward_mean: -85.55150118227314\n","reward_std: 16.602645096934232\n","reward_max: -69.4657943030175\n","reward_min: -116.67484670312112\n","total_envstep_count: 182155\n","total_train_sample_count: 182106\n","total_episode_count: 352\n","INFO:learner_logger:[RANK0]: === Training Iteration 1400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.703368       | 0.105266        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 3.366359           | 0.131843  | 0.104529     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.519642                | 0.103792             | -13.207865     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19840.000000 | 19968.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 40\n","envstep_count: 14111\n","train_sample_count: 14111\n","avg_envstep_per_episode: 352.775\n","avg_sample_per_episode: 352.775\n","avg_envstep_per_sec: 1109.4447967724057\n","avg_train_sample_per_sec: 1109.4447967724057\n","avg_episode_per_sec: 3.144907651541083\n","reward_mean: -106.88298232276426\n","reward_std: 16.170109526874345\n","reward_max: -69.27132375663719\n","reward_min: -164.7421801203597\n","total_envstep_count: 195196\n","total_train_sample_count: 195145\n","total_episode_count: 392\n","INFO:learner_logger:[RANK0]: === Training Iteration 1500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -3.955254       | 0.086012        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 3.658777           | 0.128023  | 0.084587     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.609880                | 0.083162             | -13.584976     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 60\n","envstep_count: 6303\n","train_sample_count: 6303\n","avg_envstep_per_episode: 105.05\n","avg_sample_per_episode: 105.05\n","avg_envstep_per_sec: 1143.8739889786573\n","avg_train_sample_per_sec: 1143.8739889786573\n","avg_episode_per_sec: 10.888852822262326\n","reward_mean: -108.4788118237421\n","reward_std: 5.540901757950494\n","reward_max: -81.61362245792598\n","reward_min: -120.0871466801182\n","total_envstep_count: 208292\n","total_train_sample_count: 208232\n","total_episode_count: 452\n","INFO:learner_logger:[RANK0]: === Training Iteration 1600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -4.256132       | 0.191363        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 3.985291           | 0.124293  | 0.183488     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.407721                | 0.175613             | -13.358977     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19584.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.4 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 54\n","envstep_count: 16986\n","train_sample_count: 16986\n","avg_envstep_per_episode: 314.55555555555554\n","avg_sample_per_episode: 314.55555555555554\n","avg_envstep_per_sec: 1122.3233534908331\n","avg_train_sample_per_sec: 1122.3233534908331\n","avg_episode_per_sec: 3.5679654473392786\n","reward_mean: -103.28286555365348\n","reward_std: 11.115212908638913\n","reward_max: -69.39486709677544\n","reward_min: -121.84101645431109\n","total_envstep_count: 221367\n","total_train_sample_count: 221314\n","total_episode_count: 506\n","INFO:learner_logger:[RANK0]: === Training Iteration 1700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -4.496409       | 0.178943        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 4.240169           | 0.120669  | 0.172844     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.447850                | 0.166745             | -13.633308     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.4 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 61\n","envstep_count: 16275\n","train_sample_count: 16275\n","avg_envstep_per_episode: 266.8032786885246\n","avg_sample_per_episode: 266.8032786885246\n","avg_envstep_per_sec: 1125.3545115234824\n","avg_train_sample_per_sec: 1125.3545115234824\n","avg_episode_per_sec: 4.217918599258521\n","reward_mean: -102.89625191999798\n","reward_std: 9.73978175877536\n","reward_max: -62.173706646054605\n","reward_min: -120.66726757207637\n","total_envstep_count: 234491\n","total_train_sample_count: 234437\n","total_episode_count: 567\n","INFO:learner_logger:[RANK0]: === Training Iteration 1800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -4.999818       | 0.210921        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 4.753203           | 0.117162  | 0.207929     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.392981                | 0.204937             | -13.705920     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 62\n","envstep_count: 9687\n","train_sample_count: 9687\n","avg_envstep_per_episode: 156.24193548387098\n","avg_sample_per_episode: 156.24193548387098\n","avg_envstep_per_sec: 1153.1694305348333\n","avg_train_sample_per_sec: 1153.1694305348333\n","avg_episode_per_sec: 7.380665292986442\n","reward_mean: -105.30188351362446\n","reward_std: 8.264476891256244\n","reward_max: -94.5701705074571\n","reward_min: -128.58224103802505\n","total_envstep_count: 247678\n","total_train_sample_count: 247628\n","total_episode_count: 629\n","INFO:learner_logger:[RANK0]: === Training Iteration 1900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -5.650821       | 0.914023        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 5.286091           | 0.113790  | 0.868145     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.978139                | 0.822267             | -12.991312     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -98.2740, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -98.3183, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -98.1230, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -98.5690, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -98.6398, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 2000.000000 | iteration_2000.pth.tar | 5.000000      | 444.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 88.800000               | 0.595732      | 745.301910          | 8.393040             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -98.384813  | 0.191925   | -98.123024 | -98.639778 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                 | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-98.27397155761719, -98.63977813720703, -98.1230239868164, -98.31829071044922, -98.56900024414062] | -98.384813               |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 141\n","envstep_count: 16967\n","train_sample_count: 16967\n","avg_envstep_per_episode: 120.33333333333333\n","avg_sample_per_episode: 120.33333333333333\n","avg_envstep_per_sec: 1139.3507574825053\n","avg_train_sample_per_sec: 1139.3507574825053\n","avg_episode_per_sec: 9.468288843344919\n","reward_mean: -104.57167980427656\n","reward_std: 7.036189142740912\n","reward_max: -71.05578604296085\n","reward_min: -133.73165765962858\n","total_envstep_count: 261098\n","total_train_sample_count: 261059\n","total_episode_count: 770\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-98.27397155761719, -98.63977813720703, -98.1230239868164, -98.31829071044922, -98.56900024414062], 'eval_episode_return_mean': -98.38481292724609}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 2000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -5.995931       | 0.230782        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 5.747943           | 0.110577  | 0.228154     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.175226                | 0.225526             | -13.596184     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 158\n","envstep_count: 13569\n","train_sample_count: 13569\n","avg_envstep_per_episode: 85.87974683544304\n","avg_sample_per_episode: 85.87974683544304\n","avg_envstep_per_sec: 1115.7470151063762\n","avg_train_sample_per_sec: 1115.7470151063762\n","avg_episode_per_sec: 12.99196907559934\n","reward_mean: -105.06124906457511\n","reward_std: 4.739084071278206\n","reward_max: -94.78678483346039\n","reward_min: -123.47224726251824\n","total_envstep_count: 274680\n","total_train_sample_count: 274644\n","total_episode_count: 928\n","INFO:learner_logger:[RANK0]: === Training Iteration 2100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -6.697593       | 0.355987        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 6.454611           | 0.107466  | 0.348445     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.074678                | 0.340904             | -13.548477     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 134\n","envstep_count: 13577\n","train_sample_count: 13577\n","avg_envstep_per_episode: 101.32089552238806\n","avg_sample_per_episode: 101.32089552238806\n","avg_envstep_per_sec: 1121.5557033643656\n","avg_train_sample_per_sec: 1121.5557033643656\n","avg_episode_per_sec: 11.06934258310562\n","reward_mean: -105.2726809350005\n","reward_std: 5.397336308552276\n","reward_max: -93.26653845886436\n","reward_min: -123.9841070968757\n","total_envstep_count: 288222\n","total_train_sample_count: 288173\n","total_episode_count: 1062\n","INFO:learner_logger:[RANK0]: === Training Iteration 2200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -7.445584       | 0.264844        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 7.337801           | 0.104482  | 0.260408     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.192470                | 0.255973             | -13.985234     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 156\n","envstep_count: 13452\n","train_sample_count: 13452\n","avg_envstep_per_episode: 86.23076923076923\n","avg_sample_per_episode: 86.23076923076923\n","avg_envstep_per_sec: 1091.0678600008214\n","avg_train_sample_per_sec: 1091.0678600008214\n","avg_episode_per_sec: 12.652883300633967\n","reward_mean: -102.57414697660644\n","reward_std: 3.7050501648315453\n","reward_max: -94.05843674008486\n","reward_min: -120.3124555250096\n","total_envstep_count: 301596\n","total_train_sample_count: 301545\n","total_episode_count: 1218\n","INFO:learner_logger:[RANK0]: === Training Iteration 2300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -8.230201       | 0.600741        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 7.984507           | 0.101551  | 0.593600     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.198987                | 0.586459             | -14.176650     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 146\n","envstep_count: 13080\n","train_sample_count: 13080\n","avg_envstep_per_episode: 89.58904109589041\n","avg_sample_per_episode: 89.58904109589041\n","avg_envstep_per_sec: 1150.9717729174258\n","avg_train_sample_per_sec: 1150.9717729174258\n","avg_episode_per_sec: 12.847238443879524\n","reward_mean: -101.50726319834916\n","reward_std: 4.544980029801074\n","reward_max: -95.2739016243505\n","reward_min: -130.62677360359208\n","total_envstep_count: 315026\n","total_train_sample_count: 314993\n","total_episode_count: 1364\n","INFO:learner_logger:[RANK0]: === Training Iteration 2400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -9.169189       | 1.607605        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 8.712502           | 0.098703  | 1.562912     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.737465                | 1.518218             | -13.284298     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 101\n","envstep_count: 10816\n","train_sample_count: 10816\n","avg_envstep_per_episode: 107.08910891089108\n","avg_sample_per_episode: 107.08910891089108\n","avg_envstep_per_sec: 1140.6499465959837\n","avg_train_sample_per_sec: 1140.6499465959837\n","avg_episode_per_sec: 10.6514094495372\n","reward_mean: -108.08159771886326\n","reward_std: 6.372190381852088\n","reward_max: -98.04017541539277\n","reward_min: -137.13616024123152\n","total_envstep_count: 328298\n","total_train_sample_count: 328257\n","total_episode_count: 1465\n","INFO:learner_logger:[RANK0]: === Training Iteration 2500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -9.735177       | 2.378691        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 9.066263           | 0.096109  | 2.272836     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.154175                | 2.166980             | -12.070848     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 61.0 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 56\n","envstep_count: 14174\n","train_sample_count: 14174\n","avg_envstep_per_episode: 253.10714285714286\n","avg_sample_per_episode: 253.10714285714286\n","avg_envstep_per_sec: 1129.982635009794\n","avg_train_sample_per_sec: 1129.982635009794\n","avg_episode_per_sec: 4.464443880382987\n","reward_mean: -102.22693501979481\n","reward_std: 11.267822000396697\n","reward_max: -72.51333308531288\n","reward_min: -163.563072128332\n","total_envstep_count: 341377\n","total_train_sample_count: 341311\n","total_episode_count: 1521\n","INFO:learner_logger:[RANK0]: === Training Iteration 2600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -9.315758       | 0.476415        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 9.167888           | 0.093631  | 0.492043     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.873540                | 0.507671             | -13.909113     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 61.0 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 131\n","envstep_count: 14995\n","train_sample_count: 14995\n","avg_envstep_per_episode: 114.46564885496183\n","avg_sample_per_episode: 114.46564885496183\n","avg_envstep_per_sec: 1133.5145104265148\n","avg_train_sample_per_sec: 1133.5145104265148\n","avg_episode_per_sec: 9.902660944706465\n","reward_mean: -100.37776169066937\n","reward_std: 5.645706322301932\n","reward_max: -76.6401653461528\n","reward_min: -130.4921825826913\n","total_envstep_count: 354903\n","total_train_sample_count: 354850\n","total_episode_count: 1652\n","INFO:learner_logger:[RANK0]: === Training Iteration 2700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -10.142374      | 0.820731        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 9.811660           | 0.091082  | 0.816458     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.580642                | 0.812185             | -13.369741     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 137\n","envstep_count: 13222\n","train_sample_count: 13222\n","avg_envstep_per_episode: 96.51094890510949\n","avg_sample_per_episode: 96.51094890510949\n","avg_envstep_per_sec: 1155.3091239551084\n","avg_train_sample_per_sec: 1155.3091239551084\n","avg_episode_per_sec: 11.970757070174699\n","reward_mean: -102.7078165686197\n","reward_std: 6.5887464513406675\n","reward_max: -93.4141712650787\n","reward_min: -133.86516072428498\n","total_envstep_count: 368341\n","total_train_sample_count: 368296\n","total_episode_count: 1789\n","INFO:learner_logger:[RANK0]: === Training Iteration 2800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -10.225297      | 1.533731        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 9.996771           | 0.088719  | 1.517890     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -6.003497                | 1.502050             | -14.540693     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 110\n","envstep_count: 13781\n","train_sample_count: 13781\n","avg_envstep_per_episode: 125.28181818181818\n","avg_sample_per_episode: 125.28181818181818\n","avg_envstep_per_sec: 1143.402112852756\n","avg_train_sample_per_sec: 1143.402112852756\n","avg_episode_per_sec: 9.126640477019315\n","reward_mean: -103.9134014487775\n","reward_std: 8.124638672740597\n","reward_max: -93.26524578394788\n","reward_min: -147.8594561613006\n","total_envstep_count: 381735\n","total_train_sample_count: 381677\n","total_episode_count: 1899\n","INFO:learner_logger:[RANK0]: === Training Iteration 2900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -10.465850      | 0.813678        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 10.212631          | 0.086385  | 0.799389     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.257151                | 0.785099             | -12.873232     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -94.8041, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -94.7215, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -92.5861, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -93.7035, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -95.6777, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 3000.000000 | iteration_3000.pth.tar | 5.000000      | 589.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 117.800000              | 0.488829      | 1204.920583         | 10.228528            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -94.298592  | 1.060491   | -92.586098 | -95.677750 |\n","+-------+-------------+------------+------------+------------+\n","+-------+------------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                  | eval_episode_return_mean |\n","+-------+------------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-94.80413818359375, -95.67774963378906, -93.70352172851562, -94.72145080566406, -92.58609771728516] | -94.298592               |\n","+-------+------------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 122\n","envstep_count: 13283\n","train_sample_count: 13283\n","avg_envstep_per_episode: 108.87704918032787\n","avg_sample_per_episode: 108.87704918032787\n","avg_envstep_per_sec: 1144.3289957370441\n","avg_train_sample_per_sec: 1144.3289957370441\n","avg_episode_per_sec: 10.510286643071549\n","reward_mean: -99.62170876048185\n","reward_std: 5.772380475951123\n","reward_max: -91.08812480435644\n","reward_min: -127.36509203956555\n","total_envstep_count: 395159\n","total_train_sample_count: 395120\n","total_episode_count: 2021\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-94.80413818359375, -95.67774963378906, -93.70352172851562, -94.72145080566406, -92.58609771728516], 'eval_episode_return_mean': -94.29859161376953}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 3000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -11.142679      | 1.418617        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 10.855374          | 0.084159  | 1.411263     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.404909                | 1.403910             | -13.376165     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 117\n","envstep_count: 13679\n","train_sample_count: 13679\n","avg_envstep_per_episode: 116.91452991452991\n","avg_sample_per_episode: 116.91452991452991\n","avg_envstep_per_sec: 1153.1822713844763\n","avg_train_sample_per_sec: 1153.1822713844763\n","avg_episode_per_sec: 9.863464123984482\n","reward_mean: -100.00382828771393\n","reward_std: 6.738150115607027\n","reward_max: -90.4798213237181\n","reward_min: -129.6104495146107\n","total_envstep_count: 408715\n","total_train_sample_count: 408671\n","total_episode_count: 2138\n","INFO:learner_logger:[RANK0]: === Training Iteration 3100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -12.062970      | 4.416496        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 11.037125          | 0.081975  | 4.338667     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.057511                | 4.260838             | -12.649473     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18944.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 113\n","envstep_count: 13254\n","train_sample_count: 13254\n","avg_envstep_per_episode: 117.29203539823008\n","avg_sample_per_episode: 117.29203539823008\n","avg_envstep_per_sec: 1173.0771699144198\n","avg_train_sample_per_sec: 1173.0771699144198\n","avg_episode_per_sec: 10.001336969996185\n","reward_mean: -104.54011518124173\n","reward_std: 6.6260355610766295\n","reward_max: -92.6835593855492\n","reward_min: -121.31687186840176\n","total_envstep_count: 422094\n","total_train_sample_count: 422053\n","total_episode_count: 2251\n","INFO:learner_logger:[RANK0]: === Training Iteration 3200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -11.357420      | 0.675721        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 11.216635          | 0.079950  | 0.693832     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.544222                | 0.711944             | -14.004733     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19712.000000 | 19584.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 111\n","envstep_count: 13314\n","train_sample_count: 13314\n","avg_envstep_per_episode: 119.94594594594595\n","avg_sample_per_episode: 119.94594594594595\n","avg_envstep_per_sec: 1160.6303081179672\n","avg_train_sample_per_sec: 1160.6303081179672\n","avg_episode_per_sec: 9.676277918063269\n","reward_mean: -94.89452302849213\n","reward_std: 6.8286220191044364\n","reward_max: -82.0451206743475\n","reward_min: -129.00294398354677\n","total_envstep_count: 435474\n","total_train_sample_count: 435415\n","total_episode_count: 2362\n","INFO:learner_logger:[RANK0]: === Training Iteration 3300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -12.621847      | 1.022187        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 12.309355          | 0.077908  | 1.018908     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -5.109028                | 1.015629             | -13.038123     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 123\n","envstep_count: 13462\n","train_sample_count: 13462\n","avg_envstep_per_episode: 109.44715447154472\n","avg_sample_per_episode: 109.44715447154472\n","avg_envstep_per_sec: 1136.2422465353457\n","avg_train_sample_per_sec: 1136.2422465353457\n","avg_episode_per_sec: 10.381651784567488\n","reward_mean: -95.76664137709955\n","reward_std: 6.3425581260861446\n","reward_max: -81.45970735721119\n","reward_min: -123.31275204972084\n","total_envstep_count: 448939\n","total_train_sample_count: 448877\n","total_episode_count: 2485\n","INFO:learner_logger:[RANK0]: === Training Iteration 3400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -13.593954      | 1.934689        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 13.194922          | 0.076025  | 1.918458     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -4.639982                | 1.902226             | -11.955114     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.5 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.5 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 119\n","envstep_count: 12826\n","train_sample_count: 12826\n","avg_envstep_per_episode: 107.78151260504201\n","avg_sample_per_episode: 107.78151260504201\n","avg_envstep_per_sec: 1195.2791148339768\n","avg_train_sample_per_sec: 1195.2791148339768\n","avg_episode_per_sec: 11.089834294810792\n","reward_mean: -98.00122330818988\n","reward_std: 6.73494161848674\n","reward_max: -77.72200705874998\n","reward_min: -124.78304023334074\n","total_envstep_count: 462495\n","total_train_sample_count: 462455\n","total_episode_count: 2604\n","INFO:learner_logger:[RANK0]: === Training Iteration 3500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -14.405561      | 5.689393        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 13.285845          | 0.074272  | 5.618316     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -4.450080                | 5.547239             | -11.569582     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19584.000000 | 19584.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 113\n","envstep_count: 13127\n","train_sample_count: 13127\n","avg_envstep_per_episode: 116.16814159292035\n","avg_sample_per_episode: 116.16814159292035\n","avg_envstep_per_sec: 1164.708601771746\n","avg_train_sample_per_sec: 1164.708601771746\n","avg_episode_per_sec: 10.026058657744139\n","reward_mean: -103.37635623363074\n","reward_std: 11.923739987596349\n","reward_max: -86.17065967923303\n","reward_min: -166.19309459592034\n","total_envstep_count: 475898\n","total_train_sample_count: 475854\n","total_episode_count: 2717\n","INFO:learner_logger:[RANK0]: === Training Iteration 3600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -14.457479      | 2.825881        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 13.870172          | 0.072689  | 2.877462     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.603835                | 2.929044             | -9.446692      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 101\n","envstep_count: 13040\n","train_sample_count: 13040\n","avg_envstep_per_episode: 129.1089108910891\n","avg_sample_per_episode: 129.1089108910891\n","avg_envstep_per_sec: 1157.9934794650715\n","avg_train_sample_per_sec: 1157.9934794650715\n","avg_episode_per_sec: 8.96912127499787\n","reward_mean: -107.44480194880056\n","reward_std: 13.391282890664487\n","reward_max: -85.70751907624894\n","reward_min: -171.8773212159\n","total_envstep_count: 489060\n","total_train_sample_count: 489022\n","total_episode_count: 2818\n","INFO:learner_logger:[RANK0]: === Training Iteration 3700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -14.419085      | 4.427959        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 13.846227          | 0.071121  | 4.338803     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -4.762249                | 4.249647             | -12.586974     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 78\n","envstep_count: 14368\n","train_sample_count: 14368\n","avg_envstep_per_episode: 184.2051282051282\n","avg_sample_per_episode: 184.2051282051282\n","avg_envstep_per_sec: 1198.5195543465431\n","avg_train_sample_per_sec: 1198.5195543465431\n","avg_episode_per_sec: 6.506439674208684\n","reward_mean: -104.97534654157904\n","reward_std: 16.152987488750906\n","reward_max: -71.49819611742052\n","reward_min: -142.17276388901976\n","total_envstep_count: 502296\n","total_train_sample_count: 502238\n","total_episode_count: 2896\n","INFO:learner_logger:[RANK0]: === Training Iteration 3800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -15.051715      | 4.499780        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 13.945573          | 0.069658  | 4.476660     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -4.200418                | 4.453539             | -11.189257     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19584.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 122\n","envstep_count: 13844\n","train_sample_count: 13844\n","avg_envstep_per_episode: 113.47540983606558\n","avg_sample_per_episode: 113.47540983606558\n","avg_envstep_per_sec: 1167.7545109120806\n","avg_train_sample_per_sec: 1167.7545109120806\n","avg_episode_per_sec: 10.290815539675949\n","reward_mean: -102.5669361733526\n","reward_std: 14.98256721416119\n","reward_max: -70.3047517079433\n","reward_min: -130.63122295159064\n","total_envstep_count: 515844\n","total_train_sample_count: 515778\n","total_episode_count: 3018\n","INFO:learner_logger:[RANK0]: === Training Iteration 3900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -15.173652      | 2.327512        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 14.783234          | 0.068204  | 2.319761     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.518313                | 2.312010             | -9.446875      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -76.4657, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -73.7402, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -68.1790, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -65.8780, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -59.0174, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 4000.000000 | iteration_4000.pth.tar | 5.000000      | 1023.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 204.600000              | 0.778081      | 1314.772839         | 6.426065             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -68.656081  | 6.126789   | -59.017448 | -76.465721 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                 | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-68.17903137207031, -76.4657211303711, -65.87797546386719, -59.01744842529297, -73.74022674560547] | -68.656081               |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n","INFO:collector_logger:collect end:\n","episode_count: 103\n","envstep_count: 13110\n","train_sample_count: 13110\n","avg_envstep_per_episode: 127.28155339805825\n","avg_sample_per_episode: 127.28155339805825\n","avg_envstep_per_sec: 1133.2042127624427\n","avg_train_sample_per_sec: 1133.2042127624427\n","avg_episode_per_sec: 8.903129970597377\n","reward_mean: -95.48324873861633\n","reward_std: 11.943535124171877\n","reward_max: -59.00209678745999\n","reward_min: -130.10987032232907\n","total_envstep_count: 529400\n","total_train_sample_count: 529336\n","total_episode_count: 3121\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-68.17903137207031, -76.4657211303711, -65.87797546386719, -59.01744842529297, -73.74022674560547], 'eval_episode_return_mean': -68.65608062744141}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 4000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -15.842712      | 2.406435        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 15.261459          | 0.066749  | 2.392792     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -4.174333                | 2.379148             | -11.298319     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18816.000000 | 18816.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 102\n","envstep_count: 13331\n","train_sample_count: 13331\n","avg_envstep_per_episode: 130.69607843137254\n","avg_sample_per_episode: 130.69607843137254\n","avg_envstep_per_sec: 1147.6846557099548\n","avg_train_sample_per_sec: 1147.6846557099548\n","avg_episode_per_sec: 8.781324347942043\n","reward_mean: -85.41394559166656\n","reward_std: 14.633094822138238\n","reward_max: -25.816783158853\n","reward_min: -124.11011205183668\n","total_envstep_count: 542702\n","total_train_sample_count: 542651\n","total_episode_count: 3223\n","INFO:learner_logger:[RANK0]: === Training Iteration 4100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -16.003832      | 2.084512        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 15.629213          | 0.065197  | 2.033369     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.638488                | 1.982226             | -9.933360      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19712.000000 | 19840.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 111\n","envstep_count: 13502\n","train_sample_count: 13502\n","avg_envstep_per_episode: 121.63963963963964\n","avg_sample_per_episode: 121.63963963963964\n","avg_envstep_per_sec: 1168.3383633987937\n","avg_train_sample_per_sec: 1168.3383633987937\n","avg_episode_per_sec: 9.604914704285743\n","reward_mean: -88.24107015588396\n","reward_std: 15.714931036474214\n","reward_max: -37.97583241855665\n","reward_min: -131.57794807024595\n","total_envstep_count: 556061\n","total_train_sample_count: 556009\n","total_episode_count: 3334\n","INFO:learner_logger:[RANK0]: === Training Iteration 4200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -16.990906      | 3.633858        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 16.467491          | 0.063782  | 3.608730     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.677030                | 3.583601             | -10.119535     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 77\n","envstep_count: 11843\n","train_sample_count: 11843\n","avg_envstep_per_episode: 153.80519480519482\n","avg_sample_per_episode: 153.80519480519482\n","avg_envstep_per_sec: 1148.32600330041\n","avg_train_sample_per_sec: 1148.32600330041\n","avg_episode_per_sec: 7.466106751172133\n","reward_mean: -96.3609773573573\n","reward_std: 17.4322700482746\n","reward_max: -32.321382240984136\n","reward_min: -114.34138792580026\n","total_envstep_count: 569134\n","total_train_sample_count: 569100\n","total_episode_count: 3411\n","INFO:learner_logger:[RANK0]: === Training Iteration 4300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -16.869785      | 4.149118        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 15.990878          | 0.062665  | 4.132681     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.162830                | 4.116245             | -8.760036      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18816.000000 | 18688.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 67\n","envstep_count: 13707\n","train_sample_count: 13707\n","avg_envstep_per_episode: 204.5820895522388\n","avg_sample_per_episode: 204.5820895522388\n","avg_envstep_per_sec: 1171.3642669372543\n","avg_train_sample_per_sec: 1171.3642669372543\n","avg_episode_per_sec: 5.725644260946673\n","reward_mean: -85.76560793152197\n","reward_std: 14.44119901375964\n","reward_max: -21.06293271751329\n","reward_min: -117.16674667671819\n","total_envstep_count: 582385\n","total_train_sample_count: 582343\n","total_episode_count: 3478\n","INFO:learner_logger:[RANK0]: === Training Iteration 4400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -17.963159      | 2.811587        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 17.489584          | 0.061225  | 2.726301     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.674984                | 2.641014             | -10.264022     |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 84\n","envstep_count: 14042\n","train_sample_count: 14042\n","avg_envstep_per_episode: 167.16666666666666\n","avg_sample_per_episode: 167.16666666666666\n","avg_envstep_per_sec: 1168.4855122315953\n","avg_train_sample_per_sec: 1168.4855122315953\n","avg_episode_per_sec: 6.989943243658597\n","reward_mean: -92.38419051578411\n","reward_std: 13.567383819950468\n","reward_max: -24.441222902559588\n","reward_min: -116.21633827597951\n","total_envstep_count: 595658\n","total_train_sample_count: 595617\n","total_episode_count: 3562\n","INFO:learner_logger:[RANK0]: === Training Iteration 4500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -20.302713      | 4.633554        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 19.466226          | 0.060086  | 4.585360     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -3.137754                | 4.537166             | -8.822354      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 67\n","envstep_count: 11001\n","train_sample_count: 11001\n","avg_envstep_per_episode: 164.19402985074626\n","avg_sample_per_episode: 164.19402985074626\n","avg_envstep_per_sec: 1157.4466559806153\n","avg_train_sample_per_sec: 1157.4466559806153\n","avg_episode_per_sec: 7.0492615171985475\n","reward_mean: -106.68413410712465\n","reward_std: 18.110574417241544\n","reward_max: -75.3848097818271\n","reward_min: -165.87802703618576\n","total_envstep_count: 608868\n","total_train_sample_count: 608794\n","total_episode_count: 3629\n","INFO:learner_logger:[RANK0]: === Training Iteration 4600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -23.134327      | 9.376306        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 19.868253          | 0.059101  | 9.378686     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.776536                | 9.381067             | -2.196401      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 55\n","envstep_count: 14006\n","train_sample_count: 14006\n","avg_envstep_per_episode: 254.65454545454546\n","avg_sample_per_episode: 254.65454545454546\n","avg_envstep_per_sec: 1174.8101701826067\n","avg_train_sample_per_sec: 1174.8101701826067\n","avg_episode_per_sec: 4.613348519209151\n","reward_mean: -92.61192513851371\n","reward_std: 16.621869401694735\n","reward_max: -52.09992008978812\n","reward_min: -127.34645568217368\n","total_envstep_count: 621950\n","total_train_sample_count: 621888\n","total_episode_count: 3684\n","INFO:learner_logger:[RANK0]: === Training Iteration 4700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -21.415160      | 3.313941        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 20.995919          | 0.058361  | 3.243881     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -2.305213                | 3.173820             | -6.548820      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19584.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 50\n","envstep_count: 12314\n","train_sample_count: 12314\n","avg_envstep_per_episode: 246.28\n","avg_sample_per_episode: 246.28\n","avg_envstep_per_sec: 1160.5895098508647\n","avg_train_sample_per_sec: 1160.5895098508647\n","avg_episode_per_sec: 4.712479737903463\n","reward_mean: -73.5594206243993\n","reward_std: 39.88955797561094\n","reward_max: 44.79039169496289\n","reward_min: -126.25683467478679\n","total_envstep_count: 635027\n","total_train_sample_count: 634970\n","total_episode_count: 3734\n","INFO:learner_logger:[RANK0]: === Training Iteration 4800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -22.715786      | 7.920890        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 21.220301          | 0.057480  | 7.955837     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -1.552557                | 7.990783             | -4.434565      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 59\n","envstep_count: 9837\n","train_sample_count: 9837\n","avg_envstep_per_episode: 166.72881355932202\n","avg_sample_per_episode: 166.72881355932202\n","avg_envstep_per_sec: 1133.739418570299\n","avg_train_sample_per_sec: 1133.739418570299\n","avg_episode_per_sec: 6.799900955133439\n","reward_mean: -90.54003828103166\n","reward_std: 10.054717560774725\n","reward_max: -51.35029688724076\n","reward_min: -103.64624051894108\n","total_envstep_count: 648150\n","total_train_sample_count: 648071\n","total_episode_count: 3793\n","INFO:learner_logger:[RANK0]: === Training Iteration 4900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -23.117258      | 7.108882        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 21.291778          | 0.056762  | 7.135105     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.642407                | 7.161327             | -1.843035      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -84.9531, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -79.4469, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -84.9531, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -50.7425, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -79.4469, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -39.1867, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -84.9531, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -22.8989, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 5000.000000 | iteration_5000.pth.tar | 5.000000      | 2945.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 589.000000              | 2.270155      | 1297.268139         | 2.202493             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -55.445614  | 23.632661  | -22.898918 | -84.953102 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                 | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-84.9531021118164, -39.186683654785156, -50.74246597290039, -79.4468994140625, -22.89891815185547] | -55.445614               |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n","INFO:collector_logger:collect end:\n","episode_count: 33\n","envstep_count: 11691\n","train_sample_count: 11691\n","avg_envstep_per_episode: 354.27272727272725\n","avg_sample_per_episode: 354.27272727272725\n","avg_envstep_per_sec: 1152.052802683301\n","avg_train_sample_per_sec: 1152.052802683301\n","avg_episode_per_sec: 3.2518811469120634\n","reward_mean: -81.46693090269581\n","reward_std: 19.678668064542215\n","reward_max: -17.37641671181902\n","reward_min: -118.10567011198899\n","total_envstep_count: 661150\n","total_train_sample_count: 661090\n","total_episode_count: 3826\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-84.9531021118164, -39.186683654785156, -50.74246597290039, -79.4468994140625, -22.89891815185547], 'eval_episode_return_mean': -55.445613861083984}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 5000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -23.559436      | 7.543699        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 22.268861          | 0.056063  | 7.480381     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -1.871092                | 7.417062             | -5.390801      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18560.000000 | 18560.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 85\n","envstep_count: 19634\n","train_sample_count: 19634\n","avg_envstep_per_episode: 230.98823529411766\n","avg_sample_per_episode: 230.98823529411766\n","avg_envstep_per_sec: 1165.4415045429537\n","avg_train_sample_per_sec: 1165.4415045429537\n","avg_episode_per_sec: 5.045458280847054\n","reward_mean: -85.49253484730451\n","reward_std: 19.701499063092392\n","reward_max: -25.107505150761227\n","reward_min: -134.66020282715002\n","total_envstep_count: 674466\n","total_train_sample_count: 674420\n","total_episode_count: 3911\n","INFO:learner_logger:[RANK0]: === Training Iteration 5100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -24.257208      | 4.776146        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 23.726360          | 0.055550  | 4.786413     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -2.334363                | 4.796679             | -6.747079      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 72\n","envstep_count: 11192\n","train_sample_count: 11192\n","avg_envstep_per_episode: 155.44444444444446\n","avg_sample_per_episode: 155.44444444444446\n","avg_envstep_per_sec: 1128.4069215298534\n","avg_train_sample_per_sec: 1128.4069215298534\n","avg_episode_per_sec: 7.2592296595916235\n","reward_mean: -101.93022317116258\n","reward_std: 22.394476698261894\n","reward_max: -8.675769918983093\n","reward_min: -136.05046709601518\n","total_envstep_count: 687686\n","total_train_sample_count: 687612\n","total_episode_count: 3983\n","INFO:learner_logger:[RANK0]: === Training Iteration 5200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -26.174034      | 10.717255       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 23.744191          | 0.054856  | 10.574874    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.236131                | 10.432493            | -0.685524      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.7 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 64\n","envstep_count: 15215\n","train_sample_count: 15215\n","avg_envstep_per_episode: 237.734375\n","avg_sample_per_episode: 237.734375\n","avg_envstep_per_sec: 1155.5226401109985\n","avg_train_sample_per_sec: 1155.5226401109985\n","avg_episode_per_sec: 4.8605618775618735\n","reward_mean: -86.63820789878052\n","reward_std: 38.650226648161386\n","reward_max: 56.11897861500202\n","reward_min: -136.90004979490905\n","total_envstep_count: 700934\n","total_train_sample_count: 700891\n","total_episode_count: 4047\n","INFO:learner_logger:[RANK0]: === Training Iteration 5300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -26.844493      | 7.141418        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 26.113679          | 0.054009  | 7.022733     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -2.473855                | 6.904048             | -7.219818      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.7 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 78\n","envstep_count: 13689\n","train_sample_count: 13689\n","avg_envstep_per_episode: 175.5\n","avg_sample_per_episode: 175.5\n","avg_envstep_per_sec: 1200.679227396221\n","avg_train_sample_per_sec: 1200.679227396221\n","avg_episode_per_sec: 6.841477079180746\n","reward_mean: -90.68536261813472\n","reward_std: 30.64220206434234\n","reward_max: 7.444445085634101\n","reward_min: -130.09265351605694\n","total_envstep_count: 714167\n","total_train_sample_count: 714116\n","total_episode_count: 4125\n","INFO:learner_logger:[RANK0]: === Training Iteration 5400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -29.025959      | 6.931678        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 28.271380          | 0.053311  | 6.890147     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.893477                 | 6.848617             | 2.619353       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 67\n","envstep_count: 9680\n","train_sample_count: 9680\n","avg_envstep_per_episode: 144.47761194029852\n","avg_sample_per_episode: 144.47761194029852\n","avg_envstep_per_sec: 1157.0068364756123\n","avg_train_sample_per_sec: 1157.0068364756123\n","avg_episode_per_sec: 8.008208475605995\n","reward_mean: -91.79625900443098\n","reward_std: 21.468708212589807\n","reward_max: -27.460576295439168\n","reward_min: -129.16979273797463\n","total_envstep_count: 727373\n","total_train_sample_count: 727316\n","total_episode_count: 4192\n","INFO:learner_logger:[RANK0]: === Training Iteration 5500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -30.676164      | 7.716606        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 30.034205          | 0.052738  | 7.783945     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.909388                | 7.851285             | -2.675616      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 64\n","envstep_count: 14644\n","train_sample_count: 14644\n","avg_envstep_per_episode: 228.8125\n","avg_sample_per_episode: 228.8125\n","avg_envstep_per_sec: 1153.9336232106477\n","avg_train_sample_per_sec: 1153.9336232106477\n","avg_episode_per_sec: 5.043140664127386\n","reward_mean: -81.10837420534816\n","reward_std: 51.4885288644819\n","reward_max: 99.98347534757227\n","reward_min: -134.21355216031873\n","total_envstep_count: 740589\n","total_train_sample_count: 740552\n","total_episode_count: 4256\n","INFO:learner_logger:[RANK0]: === Training Iteration 5600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -33.167017      | 13.249448       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 30.035683          | 0.052395  | 13.166460    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.345951                 | 13.083472            | 3.969724       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 18944.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 60\n","envstep_count: 15066\n","train_sample_count: 15066\n","avg_envstep_per_episode: 251.1\n","avg_sample_per_episode: 251.1\n","avg_envstep_per_sec: 1153.9895968845499\n","avg_train_sample_per_sec: 1153.9895968845499\n","avg_episode_per_sec: 4.595737144104141\n","reward_mean: -84.4830364441553\n","reward_std: 28.873906938901214\n","reward_max: -2.9786935487109787\n","reward_min: -133.27221310098213\n","total_envstep_count: 753761\n","total_train_sample_count: 753714\n","total_episode_count: 4316\n","INFO:learner_logger:[RANK0]: === Training Iteration 5700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -32.902093      | 8.301307        | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 31.740561          | 0.052385  | 8.276143     |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.334753                 | 8.250980             | 0.987293       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 85\n","envstep_count: 12358\n","train_sample_count: 12358\n","avg_envstep_per_episode: 145.38823529411764\n","avg_sample_per_episode: 145.38823529411764\n","avg_envstep_per_sec: 1161.542886817639\n","avg_train_sample_per_sec: 1161.542886817639\n","avg_episode_per_sec: 7.989249504733721\n","reward_mean: -101.05175374640558\n","reward_std: 19.440263514847988\n","reward_max: -37.57459216113828\n","reward_min: -129.346723475073\n","total_envstep_count: 767069\n","total_train_sample_count: 767016\n","total_episode_count: 4401\n","INFO:learner_logger:[RANK0]: === Training Iteration 5800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -34.176859      | 10.810444       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 30.786907          | 0.052239  | 11.036876    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -1.704638                | 11.263308            | -5.031710      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 53\n","envstep_count: 13931\n","train_sample_count: 13931\n","avg_envstep_per_episode: 262.8490566037736\n","avg_sample_per_episode: 262.8490566037736\n","avg_envstep_per_sec: 1096.3988441763581\n","avg_train_sample_per_sec: 1096.3988441763581\n","avg_episode_per_sec: 4.171210877994902\n","reward_mean: -83.27750024479968\n","reward_std: 39.350251576615726\n","reward_max: 44.0390557822829\n","reward_min: -166.96947233980723\n","total_envstep_count: 780196\n","total_train_sample_count: 780147\n","total_episode_count: 4454\n","INFO:learner_logger:[RANK0]: === Training Iteration 5900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -34.852527      | 12.751208       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 33.212982          | 0.051879  | 12.924018    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.285040                | 13.096828            | -0.843393      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -100.2628, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -100.7227, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -108.1851, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -108.2827, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -109.0549, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 6000.000000 | iteration_6000.pth.tar | 5.000000      | 470.000000    |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 94.000000               | 0.404651      | 1161.494931         | 12.356329            |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+-------------+-------------+\n","| Name  | reward_mean | reward_std | reward_max  | reward_min  |\n","+-------+-------------+------------+-------------+-------------+\n","| Value | -105.301637 | 3.940675   | -100.262794 | -109.054893 |\n","+-------+-------------+------------+-------------+-------------+\n","+-------+----------------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                      | eval_episode_return_mean |\n","+-------+----------------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-108.18505096435547, -100.72270202636719, -109.05489349365234, -108.28274536132812, -100.2627944946289] | -105.301637              |\n","+-------+----------------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 94\n","envstep_count: 13752\n","train_sample_count: 13752\n","avg_envstep_per_episode: 146.29787234042553\n","avg_sample_per_episode: 146.29787234042553\n","avg_envstep_per_sec: 1157.5295354532755\n","avg_train_sample_per_sec: 1157.5295354532755\n","avg_episode_per_sec: 7.912141967176257\n","reward_mean: -91.55308992565085\n","reward_std: 16.536967583184605\n","reward_max: -38.60470280900321\n","reward_min: -121.63553296662681\n","total_envstep_count: 793558\n","total_train_sample_count: 793515\n","total_episode_count: 4548\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-108.18505096435547, -100.72270202636719, -109.05489349365234, -108.28274536132812, -100.2627944946289], 'eval_episode_return_mean': -105.3016372680664}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 6000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -36.801580      | 13.735232       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 33.520890          | 0.052192  | 14.009861    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -1.935026                | 14.284489            | -5.713753      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 53\n","envstep_count: 10122\n","train_sample_count: 10122\n","avg_envstep_per_episode: 190.9811320754717\n","avg_sample_per_episode: 190.9811320754717\n","avg_envstep_per_sec: 1162.390826076153\n","avg_train_sample_per_sec: 1162.390826076153\n","avg_episode_per_sec: 6.086417089709159\n","reward_mean: -68.17885564576642\n","reward_std: 104.5937760304454\n","reward_max: 282.63539277692536\n","reward_min: -120.26768225056088\n","total_envstep_count: 806598\n","total_train_sample_count: 806565\n","total_episode_count: 4601\n","INFO:learner_logger:[RANK0]: === Training Iteration 6100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -37.868626      | 11.411815       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 36.801362          | 0.051981  | 11.393496    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -1.522148                | 11.375178            | -4.500697      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 19\n","envstep_count: 11782\n","train_sample_count: 11782\n","avg_envstep_per_episode: 620.1052631578947\n","avg_sample_per_episode: 620.1052631578947\n","avg_envstep_per_sec: 1201.2353695904058\n","avg_train_sample_per_sec: 1201.2353695904058\n","avg_episode_per_sec: 1.9371475150413944\n","reward_mean: 44.38820097940277\n","reward_std: 115.25049506054792\n","reward_max: 283.25365335633785\n","reward_min: -91.94864564683661\n","total_envstep_count: 819556\n","total_train_sample_count: 819499\n","total_episode_count: 4620\n","INFO:learner_logger:[RANK0]: === Training Iteration 6200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -42.682406      | 13.358333       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 40.223254          | 0.051508  | 13.105440    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.193651                 | 12.852547            | 3.540658       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.6 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18944.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 42\n","envstep_count: 16375\n","train_sample_count: 16375\n","avg_envstep_per_episode: 389.8809523809524\n","avg_sample_per_episode: 389.8809523809524\n","avg_envstep_per_sec: 1167.0728763806599\n","avg_train_sample_per_sec: 1167.0728763806599\n","avg_episode_per_sec: 2.9934082936175703\n","reward_mean: -41.84745969183854\n","reward_std: 90.25688954565832\n","reward_max: 287.7498491132981\n","reward_min: -119.91557999803281\n","total_envstep_count: 832569\n","total_train_sample_count: 832546\n","total_episode_count: 4662\n","INFO:learner_logger:[RANK0]: === Training Iteration 6300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -44.099155      | 18.154072       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 41.537451          | 0.051783  | 18.083484    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 2.760792                 | 18.012896            | 8.175454       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.5 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 47\n","envstep_count: 12499\n","train_sample_count: 12499\n","avg_envstep_per_episode: 265.93617021276594\n","avg_sample_per_episode: 265.93617021276594\n","avg_envstep_per_sec: 1141.7771108768418\n","avg_train_sample_per_sec: 1141.7771108768418\n","avg_episode_per_sec: 4.2934254109298\n","reward_mean: -76.14644135549052\n","reward_std: 48.6251411860803\n","reward_max: 150.31188269560926\n","reward_min: -135.81835357239063\n","total_envstep_count: 845755\n","total_train_sample_count: 845717\n","total_episode_count: 4709\n","INFO:learner_logger:[RANK0]: === Training Iteration 6400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -46.558623      | 14.711626       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 45.095997          | 0.051826  | 14.696913    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 2.040052                 | 14.682201            | 6.039179       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 37\n","envstep_count: 13674\n","train_sample_count: 13674\n","avg_envstep_per_episode: 369.56756756756755\n","avg_sample_per_episode: 369.56756756756755\n","avg_envstep_per_sec: 1168.6792862039626\n","avg_train_sample_per_sec: 1168.6792862039626\n","avg_episode_per_sec: 3.162288546844129\n","reward_mean: -41.90437385494162\n","reward_std: 74.89756404086667\n","reward_max: 278.8672183505823\n","reward_min: -116.50830790278937\n","total_envstep_count: 858821\n","total_train_sample_count: 858751\n","total_episode_count: 4746\n","INFO:learner_logger:[RANK0]: === Training Iteration 6500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -50.107826      | 24.408369       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 46.621498          | 0.052195  | 23.684599    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.953560                 | 22.960831            | 2.815779       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19584.000000 | 19584.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 60\n","envstep_count: 13921\n","train_sample_count: 13921\n","avg_envstep_per_episode: 232.01666666666668\n","avg_sample_per_episode: 232.01666666666668\n","avg_envstep_per_sec: 1164.4056990005506\n","avg_train_sample_per_sec: 1164.4056990005506\n","avg_episode_per_sec: 5.0186295481670165\n","reward_mean: -81.23268742915356\n","reward_std: 45.81996456879768\n","reward_max: 85.94268126318335\n","reward_min: -127.66622524781121\n","total_envstep_count: 872033\n","total_train_sample_count: 872000\n","total_episode_count: 4806\n","INFO:learner_logger:[RANK0]: === Training Iteration 6600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -55.651673      | 20.135429       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 51.895718          | 0.052436  | 20.228005    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 2.194641                 | 20.320582            | 6.470957       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 107\n","envstep_count: 12481\n","train_sample_count: 12481\n","avg_envstep_per_episode: 116.64485981308411\n","avg_sample_per_episode: 116.64485981308411\n","avg_envstep_per_sec: 1132.107404516167\n","avg_train_sample_per_sec: 1132.107404516167\n","avg_episode_per_sec: 9.705591882319514\n","reward_mean: -110.17789745803151\n","reward_std: 10.239345538689037\n","reward_max: -60.68675725361709\n","reward_min: -134.64013679489244\n","total_envstep_count: 885561\n","total_train_sample_count: 885505\n","total_episode_count: 4913\n","INFO:learner_logger:[RANK0]: === Training Iteration 6700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -53.827762      | 19.512348       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 51.577601          | 0.053094  | 19.547652    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.168432                 | 19.582956            | 0.494613       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 20\n","envstep_count: 11654\n","train_sample_count: 11654\n","avg_envstep_per_episode: 582.7\n","avg_sample_per_episode: 582.7\n","avg_envstep_per_sec: 1145.6375386472475\n","avg_train_sample_per_sec: 1145.6375386472475\n","avg_episode_per_sec: 1.9660846724682473\n","reward_mean: 25.341256178996908\n","reward_std: 133.89773319764274\n","reward_max: 280.9703791124306\n","reward_min: -125.98317498971554\n","total_envstep_count: 898456\n","total_train_sample_count: 898407\n","total_episode_count: 4933\n","INFO:learner_logger:[RANK0]: === Training Iteration 6800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -58.641241      | 15.422479       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 57.532879          | 0.053613  | 15.822912    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.636782                | 16.223344            | -1.863170      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 14\n","envstep_count: 13043\n","train_sample_count: 13043\n","avg_envstep_per_episode: 931.6428571428571\n","avg_sample_per_episode: 931.6428571428571\n","avg_envstep_per_sec: 1160.9382061012839\n","avg_train_sample_per_sec: 1160.9382061012839\n","avg_episode_per_sec: 1.2461193655921163\n","reward_mean: 173.93243298712142\n","reward_std: 145.31119816626787\n","reward_max: 286.64661907138884\n","reward_min: -53.87283192532581\n","total_envstep_count: 911356\n","total_train_sample_count: 911322\n","total_episode_count: 4947\n","INFO:learner_logger:[RANK0]: === Training Iteration 6900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -59.004118      | 15.272502       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 57.994981          | 0.053617  | 15.423982    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.865584                | 15.575461            | -2.532508      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -93.1909, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -36.3298, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -93.1909, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 6.6367, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -36.3298, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -93.1909, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -36.3298, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 69.0771, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -93.1909, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 6.6367, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -36.3298, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 277.5970, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 7000.000000 | iteration_7000.pth.tar | 5.000000      | 7509.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 1501.800000             | 5.894942      | 1273.803820         | 0.848185             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 44.758002   | 127.956058 | 277.596954 | -93.190880 |\n","+-------+-------------+------------+------------+------------+\n","+-------+----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                | eval_episode_return_mean |\n","+-------+----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [6.636654853820801, -93.19087982177734, -36.329830169677734, 69.07711029052734, 277.5969543457031] | 44.758002                |\n","+-------+----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n","INFO:collector_logger:collect end:\n","episode_count: 34\n","envstep_count: 14373\n","train_sample_count: 14373\n","avg_envstep_per_episode: 422.7352941176471\n","avg_sample_per_episode: 422.7352941176471\n","avg_envstep_per_sec: 1161.0224699272148\n","avg_train_sample_per_sec: 1161.0224699272148\n","avg_episode_per_sec: 2.7464526527186606\n","reward_mean: -39.59090839610734\n","reward_std: 85.11430176971155\n","reward_max: 280.40673916369815\n","reward_min: -127.79688307698133\n","total_envstep_count: 924359\n","total_train_sample_count: 924303\n","total_episode_count: 4981\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [6.636654853820801, -93.19087982177734, -36.329830169677734, 69.07711029052734, 277.5969543457031], 'eval_episode_return_mean': 44.75800189971924}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 7000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -61.210390      | 21.597351       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 59.269734          | 0.053522  | 21.148008    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.527582                | 20.698664            | -1.544539      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 17152.000000 | 17152.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 38\n","envstep_count: 13414\n","train_sample_count: 13414\n","avg_envstep_per_episode: 353.0\n","avg_sample_per_episode: 353.0\n","avg_envstep_per_sec: 1134.6767312874283\n","avg_train_sample_per_sec: 1134.6767312874283\n","avg_episode_per_sec: 3.2143816750352077\n","reward_mean: -49.158583549052075\n","reward_std: 50.834092154114074\n","reward_max: 90.46818677556729\n","reward_min: -135.6715775324222\n","total_envstep_count: 937404\n","total_train_sample_count: 937365\n","total_episode_count: 5019\n","INFO:learner_logger:[RANK0]: === Training Iteration 7100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -61.416308      | 16.984716       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 60.325841          | 0.053195  | 17.107353    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.288816                 | 17.229993            | 0.847332       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.7 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18944.000000 | 18944.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.7 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 27\n","envstep_count: 12293\n","train_sample_count: 12293\n","avg_envstep_per_episode: 455.2962962962963\n","avg_sample_per_episode: 455.2962962962963\n","avg_envstep_per_sec: 1148.7072474776696\n","avg_train_sample_per_sec: 1148.7072474776696\n","avg_episode_per_sec: 2.5229883414867875\n","reward_mean: -8.862654591024798\n","reward_std: 96.4072531263715\n","reward_max: 283.9109322488141\n","reward_min: -100.46671746809191\n","total_envstep_count: 950382\n","total_train_sample_count: 950330\n","total_episode_count: 5046\n","INFO:learner_logger:[RANK0]: === Training Iteration 7200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -65.391629      | 25.145595       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 63.721623          | 0.053068  | 25.592555    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.278349                 | 26.039516            | 0.817279       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 80\n","envstep_count: 12095\n","train_sample_count: 12095\n","avg_envstep_per_episode: 151.1875\n","avg_sample_per_episode: 151.1875\n","avg_envstep_per_sec: 1112.4155956852928\n","avg_train_sample_per_sec: 1112.4155956852928\n","avg_episode_per_sec: 7.357854291428145\n","reward_mean: -93.00310908645409\n","reward_std: 43.774319104306535\n","reward_max: 98.27147282510484\n","reward_min: -126.59448848719632\n","total_envstep_count: 963514\n","total_train_sample_count: 963465\n","total_episode_count: 5126\n","INFO:learner_logger:[RANK0]: === Training Iteration 7300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -64.568480      | 25.182919       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 62.146856          | 0.054127  | 25.331963    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.217244                 | 25.481008            | 3.550084       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18688.000000 | 18816.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 26\n","envstep_count: 14274\n","train_sample_count: 14274\n","avg_envstep_per_episode: 549.0\n","avg_sample_per_episode: 549.0\n","avg_envstep_per_sec: 1164.4171169544045\n","avg_train_sample_per_sec: 1164.4171169544045\n","avg_episode_per_sec: 2.120978355108205\n","reward_mean: 19.813840998463704\n","reward_std: 112.58030308200226\n","reward_max: 290.2819660864126\n","reward_min: -103.01967382075078\n","total_envstep_count: 976540\n","total_train_sample_count: 976491\n","total_episode_count: 5152\n","INFO:learner_logger:[RANK0]: === Training Iteration 7400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -69.550444      | 19.453899       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 68.365614          | 0.054284  | 19.367603    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.302780                 | 19.281307            | 0.882274       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 37\n","envstep_count: 13461\n","train_sample_count: 13461\n","avg_envstep_per_episode: 363.81081081081084\n","avg_sample_per_episode: 363.81081081081084\n","avg_envstep_per_sec: 1155.0847555718503\n","avg_train_sample_per_sec: 1155.0847555718503\n","avg_episode_per_sec: 3.1749599551414054\n","reward_mean: -31.897430221923436\n","reward_std: 65.70655892229428\n","reward_max: 149.03398538024496\n","reward_min: -134.61721100093985\n","total_envstep_count: 989559\n","total_train_sample_count: 989520\n","total_episode_count: 5189\n","INFO:learner_logger:[RANK0]: === Training Iteration 7500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -73.082567      | 15.643532       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 72.630352          | 0.054475  | 15.562761    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.646245                | 15.481989            | -1.880460      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 45\n","envstep_count: 13219\n","train_sample_count: 13219\n","avg_envstep_per_episode: 293.75555555555553\n","avg_sample_per_episode: 293.75555555555553\n","avg_envstep_per_sec: 1195.775633633662\n","avg_train_sample_per_sec: 1195.775633633662\n","avg_episode_per_sec: 4.070648575044617\n","reward_mean: -47.59343694065604\n","reward_std: 87.22520816370324\n","reward_max: 183.3462406347174\n","reward_min: -110.71071578623976\n","total_envstep_count: 1002704\n","total_train_sample_count: 1002643\n","total_episode_count: 5234\n","INFO:learner_logger:[RANK0]: === Training Iteration 7600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -77.705059      | 33.329628       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 71.809217          | 0.054453  | 33.831977    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.981584                 | 34.334326            | 2.856818       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 30\n","envstep_count: 9229\n","train_sample_count: 9229\n","avg_envstep_per_episode: 307.6333333333333\n","avg_sample_per_episode: 307.6333333333333\n","avg_envstep_per_sec: 1171.0526616800562\n","avg_train_sample_per_sec: 1171.0526616800562\n","avg_episode_per_sec: 3.806650758522233\n","reward_mean: -101.56144657191187\n","reward_std: 30.772558292647325\n","reward_max: -23.621592631461127\n","reward_min: -140.89708866701136\n","total_envstep_count: 1015756\n","total_train_sample_count: 1015696\n","total_episode_count: 5264\n","INFO:learner_logger:[RANK0]: === Training Iteration 7700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -78.128634      | 27.595051       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 71.883919          | 0.056021  | 27.192291    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 4.005648                 | 26.789532            | 11.548075      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19712.000000 | 19712.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 44\n","envstep_count: 15070\n","train_sample_count: 15070\n","avg_envstep_per_episode: 342.5\n","avg_sample_per_episode: 342.5\n","avg_envstep_per_sec: 1200.6730078558696\n","avg_train_sample_per_sec: 1200.6730078558696\n","avg_episode_per_sec: 3.505614621476992\n","reward_mean: -68.77311430975983\n","reward_std: 72.23910091521059\n","reward_max: 282.6899154027866\n","reward_min: -137.3879916084328\n","total_envstep_count: 1028922\n","total_train_sample_count: 1028878\n","total_episode_count: 5308\n","INFO:learner_logger:[RANK0]: === Training Iteration 7800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -73.610875      | 22.346251       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 72.036101          | 0.057340  | 22.412297    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.241586                 | 22.478342            | 0.690677       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 21\n","envstep_count: 12392\n","train_sample_count: 12392\n","avg_envstep_per_episode: 590.0952380952381\n","avg_sample_per_episode: 590.0952380952381\n","avg_envstep_per_sec: 1169.6458553664052\n","avg_train_sample_per_sec: 1169.6458553664052\n","avg_episode_per_sec: 1.9821306457952312\n","reward_mean: 39.00406547569574\n","reward_std: 134.38767335452044\n","reward_max: 289.2741383877916\n","reward_min: -111.20757605140656\n","total_envstep_count: 1041882\n","total_train_sample_count: 1041830\n","total_episode_count: 5329\n","INFO:learner_logger:[RANK0]: === Training Iteration 7900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -76.354446      | 27.079118       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 73.214109          | 0.057418  | 27.165398    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.470676                 | 27.251680            | 4.202514       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 35.6059, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 291.5372, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 288.4014, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 288.4362, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 288.6523, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 8000.000000 | iteration_8000.pth.tar | 5.000000      | 5671.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 1134.200000             | 4.352005      | 1303.077688         | 1.148896             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 238.526597  | 101.467196 | 291.537170 | 35.605946  |\n","+-------+-------------+------------+------------+------------+\n","+-------+--------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                              | eval_episode_return_mean |\n","+-------+--------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [35.6059455871582, 288.65228271484375, 288.4013977050781, 291.53717041015625, 288.4361877441406] | 238.526597               |\n","+-------+--------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n","INFO:collector_logger:collect end:\n","episode_count: 20\n","envstep_count: 10712\n","train_sample_count: 10712\n","avg_envstep_per_episode: 535.6\n","avg_sample_per_episode: 535.6\n","avg_envstep_per_sec: 1193.9512522587097\n","avg_train_sample_per_sec: 1193.9512522587097\n","avg_episode_per_sec: 2.229184563589824\n","reward_mean: -16.670902872989473\n","reward_std: 65.6215211815866\n","reward_max: 116.925055936826\n","reward_min: -130.80845255316268\n","total_envstep_count: 1054875\n","total_train_sample_count: 1054814\n","total_episode_count: 5349\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [35.6059455871582, 288.65228271484375, 288.4013977050781, 291.53717041015625, 288.4361877441406], 'eval_episode_return_mean': 238.5265968322754}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 8000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -77.495364      | 22.831909       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 76.512214          | 0.058165  | 22.970328    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.100299                 | 23.108747            | 3.130032       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.4 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 17280.000000 | 17280.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.4 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 15488\n","train_sample_count: 15488\n","avg_envstep_per_episode: 1191.3846153846155\n","avg_sample_per_episode: 1191.3846153846155\n","avg_envstep_per_sec: 1147.0643585044386\n","avg_train_sample_per_sec: 1147.0643585044386\n","avg_episode_per_sec: 0.9627993711620417\n","reward_mean: 285.44655165563086\n","reward_std: 4.596144126567157\n","reward_max: 290.1944723860464\n","reward_min: 273.859879651745\n","total_envstep_count: 1067763\n","total_train_sample_count: 1067710\n","total_episode_count: 5362\n","INFO:learner_logger:[RANK0]: === Training Iteration 8100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -81.685231      | 21.915002       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 80.557284          | 0.058879  | 21.914537    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.440171                | 21.914072            | -1.246669      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 10305\n","train_sample_count: 10305\n","avg_envstep_per_episode: 792.6923076923077\n","avg_sample_per_episode: 792.6923076923077\n","avg_envstep_per_sec: 1234.860883097767\n","avg_train_sample_per_sec: 1234.860883097767\n","avg_episode_per_sec: 1.557806063102472\n","reward_mean: 121.00488216029383\n","reward_std: 150.10981393678625\n","reward_max: 292.7639680950144\n","reward_min: -105.2990472418548\n","total_envstep_count: 1080707\n","total_train_sample_count: 1080655\n","total_episode_count: 5375\n","INFO:learner_logger:[RANK0]: === Training Iteration 8200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -83.633569      | 21.786886       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 82.887439          | 0.058671  | 21.352739    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.188228                 | 20.918591            | 0.533779       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19968.000000 | 20096.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 18\n","envstep_count: 14547\n","train_sample_count: 14547\n","avg_envstep_per_episode: 808.1666666666666\n","avg_sample_per_episode: 808.1666666666666\n","avg_envstep_per_sec: 1248.3780455733834\n","avg_train_sample_per_sec: 1248.3780455733834\n","avg_episode_per_sec: 1.5447037066282328\n","reward_mean: 118.18790365114523\n","reward_std: 164.62271953731934\n","reward_max: 292.15228783303206\n","reward_min: -115.7008915607923\n","total_envstep_count: 1093660\n","total_train_sample_count: 1093586\n","total_episode_count: 5393\n","INFO:learner_logger:[RANK0]: === Training Iteration 8300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -87.826118      | 26.398826       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 85.157632          | 0.059519  | 26.673143    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 1.563728                 | 26.947459            | 4.413023       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19584.000000 | 19584.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 47\n","envstep_count: 14577\n","train_sample_count: 14577\n","avg_envstep_per_episode: 310.1489361702128\n","avg_sample_per_episode: 310.1489361702128\n","avg_envstep_per_sec: 1182.1252367260247\n","avg_train_sample_per_sec: 1182.1252367260247\n","avg_episode_per_sec: 3.8114760325254275\n","reward_mean: -72.81572652220343\n","reward_std: 46.531747579174365\n","reward_max: 91.12345885377178\n","reward_min: -127.1571460703453\n","total_envstep_count: 1106909\n","total_train_sample_count: 1106851\n","total_episode_count: 5440\n","INFO:learner_logger:[RANK0]: === Training Iteration 8400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -84.804173      | 18.983535       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 84.422145          | 0.060666  | 19.043368    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.996376                | 19.103201            | -2.792144      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 23\n","envstep_count: 12885\n","train_sample_count: 12885\n","avg_envstep_per_episode: 560.2173913043479\n","avg_sample_per_episode: 560.2173913043479\n","avg_envstep_per_sec: 1191.9661355178212\n","avg_train_sample_per_sec: 1191.9661355178212\n","avg_episode_per_sec: 2.127684991611167\n","reward_mean: 41.34229257458668\n","reward_std: 129.92394320038028\n","reward_max: 296.4778869099138\n","reward_min: -130.26036793083262\n","total_envstep_count: 1119929\n","total_train_sample_count: 1119880\n","total_episode_count: 5463\n","INFO:learner_logger:[RANK0]: === Training Iteration 8500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -91.084731      | 24.061431       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 88.886634          | 0.061099  | 23.694055    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.024045                 | 23.326678            | 0.067212       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 28\n","envstep_count: 9057\n","train_sample_count: 9057\n","avg_envstep_per_episode: 323.4642857142857\n","avg_sample_per_episode: 323.4642857142857\n","avg_envstep_per_sec: 1192.7795959919792\n","avg_train_sample_per_sec: 1192.7795959919792\n","avg_episode_per_sec: 3.687515588801526\n","reward_mean: -54.803991200343866\n","reward_std: 51.986454980401675\n","reward_max: 60.07988820576992\n","reward_min: -109.68127142559922\n","total_envstep_count: 1132878\n","total_train_sample_count: 1132841\n","total_episode_count: 5491\n","INFO:learner_logger:[RANK0]: === Training Iteration 8600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -92.946880      | 27.650668       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 91.371815          | 0.061308  | 27.884378    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.758359                 | 28.118089            | 2.117273       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 18\n","envstep_count: 14326\n","train_sample_count: 14326\n","avg_envstep_per_episode: 795.8888888888889\n","avg_sample_per_episode: 795.8888888888889\n","avg_envstep_per_sec: 1178.1066470402568\n","avg_train_sample_per_sec: 1178.1066470402568\n","avg_episode_per_sec: 1.480240098193817\n","reward_mean: 125.50267921350284\n","reward_std: 155.1403822669115\n","reward_max: 300.41254621755155\n","reward_min: -109.69774468084549\n","total_envstep_count: 1145786\n","total_train_sample_count: 1145743\n","total_episode_count: 5509\n","INFO:learner_logger:[RANK0]: === Training Iteration 8700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -94.841058      | 22.017579       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 93.053836          | 0.061404  | 22.134957    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.507211                 | 22.252335            | 1.415333       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 11\n","envstep_count: 14827\n","train_sample_count: 14827\n","avg_envstep_per_episode: 1347.909090909091\n","avg_sample_per_episode: 1347.909090909091\n","avg_envstep_per_sec: 1185.8768791880432\n","avg_train_sample_per_sec: 1185.8768791880432\n","avg_episode_per_sec: 0.8797899555586751\n","reward_mean: 104.76066284354893\n","reward_std: 125.02671196729399\n","reward_max: 278.4651643587709\n","reward_min: -107.89079178927071\n","total_envstep_count: 1158668\n","total_train_sample_count: 1158586\n","total_episode_count: 5520\n","INFO:learner_logger:[RANK0]: === Training Iteration 8800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -99.925885      | 25.476797       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 94.054890          | 0.062049  | 25.144635    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.985781                 | 24.812472            | 2.740973       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19456.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 10993\n","train_sample_count: 10993\n","avg_envstep_per_episode: 845.6153846153846\n","avg_sample_per_episode: 845.6153846153846\n","avg_envstep_per_sec: 1170.6558329931463\n","avg_train_sample_per_sec: 1170.6558329931463\n","avg_episode_per_sec: 1.3843833192859913\n","reward_mean: -4.9194963696367875\n","reward_std: 92.82383278763517\n","reward_max: 199.58685876513522\n","reward_min: -124.45006372951715\n","total_envstep_count: 1171524\n","total_train_sample_count: 1171483\n","total_episode_count: 5533\n","INFO:learner_logger:[RANK0]: === Training Iteration 8900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -96.304126      | 30.436067       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 93.783388          | 0.063432  | 29.553938    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.596986                 | 28.671811            | 1.646334       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.3 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: -81.8547, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: -84.4878, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: -84.2390, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: -78.7249, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: -82.0258, current episode: 5\n","INFO:evaluator_logger:\n","+-------+-------------+------------------------+---------------+---------------+\n","| Name  | train_iter  | ckpt_name              | episode_count | envstep_count |\n","+-------+-------------+------------------------+---------------+---------------+\n","| Value | 9000.000000 | iteration_9000.pth.tar | 5.000000      | 8000.000000   |\n","+-------+-------------+------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 1600.000000             | 6.880306      | 1162.739022         | 0.726712             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | -82.266441  | 2.078237   | -78.724930 | -84.487808 |\n","+-------+-------------+------------+------------+------------+\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                                 | eval_episode_return_mean |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [-81.85468292236328, -84.48780822753906, -84.23902893066406, -78.72492980957031, -82.0257568359375] | -82.266441               |\n","+-------+-----------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:collector_logger:collect end:\n","episode_count: 22\n","envstep_count: 14792\n","train_sample_count: 14792\n","avg_envstep_per_episode: 672.3636363636364\n","avg_sample_per_episode: 672.3636363636364\n","avg_envstep_per_sec: 1172.5811874818314\n","avg_train_sample_per_sec: 1172.5811874818314\n","avg_episode_per_sec: 1.7439687753245194\n","reward_mean: 54.689464503130424\n","reward_std: 144.6011226767805\n","reward_max: 292.9993755369996\n","reward_min: -110.811289011082\n","total_envstep_count: 1184534\n","total_train_sample_count: 1184467\n","total_episode_count: 5555\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [-81.85468292236328, -84.48780822753906, -84.23902893066406, -78.72492980957031, -82.0257568359375], 'eval_episode_return_mean': -82.26644134521484}\n"]},{"output_type":"stream","name":"stderr","text":["INFO:learner_logger:[RANK0]: === Training Iteration 9000 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -99.850839      | 26.323276       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 92.108272          | 0.064726  | 26.003213    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.376771                 | 25.683150            | 1.031877       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 17152.000000 | 17152.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 9\n","envstep_count: 13066\n","train_sample_count: 13066\n","avg_envstep_per_episode: 1451.7777777777778\n","avg_sample_per_episode: 1451.7777777777778\n","avg_envstep_per_sec: 1197.6540751349278\n","avg_train_sample_per_sec: 1197.6540751349278\n","avg_episode_per_sec: 0.8249568862861127\n","reward_mean: 182.5165132772168\n","reward_std: 111.91443174140926\n","reward_max: 262.1850661341291\n","reward_min: -116.5685050561838\n","total_envstep_count: 1197381\n","total_train_sample_count: 1197325\n","total_episode_count: 5564\n","INFO:learner_logger:[RANK0]: === Training Iteration 9100 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -97.747624      | 24.653558       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 95.633360          | 0.065388  | 24.214880    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.267361                 | 23.776203            | 0.729348       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 9\n","envstep_count: 12559\n","train_sample_count: 12559\n","avg_envstep_per_episode: 1395.4444444444443\n","avg_sample_per_episode: 1395.4444444444443\n","avg_envstep_per_sec: 1165.4407085674088\n","avg_train_sample_per_sec: 1165.4407085674088\n","avg_episode_per_sec: 0.8351752828335599\n","reward_mean: 182.867887852882\n","reward_std: 162.35816799593687\n","reward_max: 297.8977129844629\n","reward_min: -117.35050469488748\n","total_envstep_count: 1210206\n","total_train_sample_count: 1210172\n","total_episode_count: 5573\n","INFO:learner_logger:[RANK0]: === Training Iteration 9200 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -104.278489     | 21.571347       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 97.581785          | 0.066066  | 21.847357    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.388858                 | 22.123366            | 1.056623       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19072.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 17\n","envstep_count: 14813\n","train_sample_count: 14813\n","avg_envstep_per_episode: 871.3529411764706\n","avg_sample_per_episode: 871.3529411764706\n","avg_envstep_per_sec: 1163.344558195637\n","avg_train_sample_per_sec: 1163.344558195637\n","avg_episode_per_sec: 1.3351014304547242\n","reward_mean: 20.869228081356027\n","reward_std: 133.2749578035532\n","reward_max: 292.95029650174973\n","reward_min: -113.48137980543744\n","total_envstep_count: 1223092\n","total_train_sample_count: 1223033\n","total_episode_count: 5590\n","INFO:learner_logger:[RANK0]: === Training Iteration 9300 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -99.538519      | 22.022146       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 98.411665          | 0.067564  | 21.747679    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.546594                 | 21.473212            | 1.473187       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19328.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 10\n","envstep_count: 9510\n","train_sample_count: 9510\n","avg_envstep_per_episode: 951.0\n","avg_sample_per_episode: 951.0\n","avg_envstep_per_sec: 1143.420252103814\n","avg_train_sample_per_sec: 1143.420252103814\n","avg_episode_per_sec: 1.2023346499514342\n","reward_mean: 47.13826871835234\n","reward_std: 139.03376106171288\n","reward_max: 297.08388984872\n","reward_min: -105.93596094348763\n","total_envstep_count: 1235924\n","total_train_sample_count: 1235887\n","total_episode_count: 5600\n","INFO:learner_logger:[RANK0]: === Training Iteration 9400 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -96.328960      | 19.531575       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 95.568613          | 0.068688  | 18.629285    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.023464                | 17.726995            | -0.062834      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 11\n","envstep_count: 11918\n","train_sample_count: 11918\n","avg_envstep_per_episode: 1083.4545454545455\n","avg_sample_per_episode: 1083.4545454545455\n","avg_envstep_per_sec: 1142.7368220665307\n","avg_train_sample_per_sec: 1142.7368220665307\n","avg_episode_per_sec: 1.054715979420359\n","reward_mean: 187.69099662656237\n","reward_std: 127.27066962905216\n","reward_max: 300.489689226037\n","reward_min: -19.655291398305934\n","total_envstep_count: 1248808\n","total_train_sample_count: 1248765\n","total_episode_count: 5611\n","INFO:learner_logger:[RANK0]: === Training Iteration 9500 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -99.217499      | 13.945289       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 99.117180          | 0.069529  | 13.816131    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.782378                 | 13.686974            | 2.085904       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19072.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 14\n","envstep_count: 16187\n","train_sample_count: 16187\n","avg_envstep_per_episode: 1156.2142857142858\n","avg_sample_per_episode: 1156.2142857142858\n","avg_envstep_per_sec: 1183.9620271794329\n","avg_train_sample_per_sec: 1183.9620271794329\n","avg_episode_per_sec: 1.0239987879478631\n","reward_mean: 248.6404384775032\n","reward_std: 116.97757026236417\n","reward_max: 300.7212843183818\n","reward_min: -137.66580980576495\n","total_envstep_count: 1261714\n","total_train_sample_count: 1261688\n","total_episode_count: 5625\n","INFO:learner_logger:[RANK0]: === Training Iteration 9600 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -103.879587     | 14.552987       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 103.785887         | 0.069993  | 14.105203    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.216981                 | 13.657419            | 0.577023       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19200.000000 | 19200.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 11\n","envstep_count: 10789\n","train_sample_count: 10789\n","avg_envstep_per_episode: 980.8181818181819\n","avg_sample_per_episode: 980.8181818181819\n","avg_envstep_per_sec: 1203.470978166021\n","avg_train_sample_per_sec: 1203.470978166021\n","avg_episode_per_sec: 1.227007207324704\n","reward_mean: 264.5838793798172\n","reward_std: 107.2306573432217\n","reward_max: 300.1449013887948\n","reward_min: -74.49135653413553\n","total_envstep_count: 1274616\n","total_train_sample_count: 1274557\n","total_episode_count: 5636\n","INFO:learner_logger:[RANK0]: === Training Iteration 9700 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -106.796970     | 13.903197       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 107.027604         | 0.070318  | 13.657901    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | 0.126125                 | 13.412606            | 0.334844       |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 14\n","envstep_count: 14591\n","train_sample_count: 14591\n","avg_envstep_per_episode: 1042.2142857142858\n","avg_sample_per_episode: 1042.2142857142858\n","avg_envstep_per_sec: 1182.6110845428952\n","avg_train_sample_per_sec: 1182.6110845428952\n","avg_episode_per_sec: 1.1347101078473396\n","reward_mean: 298.8327886553222\n","reward_std: 1.125954268324446\n","reward_max: 300.7093453270157\n","reward_min: 297.1501253012989\n","total_envstep_count: 1287526\n","total_train_sample_count: 1287468\n","total_episode_count: 5650\n","INFO:learner_logger:[RANK0]: === Training Iteration 9800 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -112.943769     | 10.750488       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 113.477071         | 0.070029  | 10.381254    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.419167                | 10.012020            | -1.114414      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 18944.000000 | 18944.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+-----------+-------------+----------+--------------+\n","| Name  | pushed_in | sampled_out | removed  | current_have |\n","+-------+-----------+-------------+----------+--------------+\n","| Value | 0.000000  | 0.000000    | 0.000000 | 0.000000     |\n","+-------+-----------+-------------+----------+--------------+\n","\n","\n","INFO:collector_logger:collect end:\n","episode_count: 13\n","envstep_count: 12657\n","train_sample_count: 12657\n","avg_envstep_per_episode: 973.6153846153846\n","avg_sample_per_episode: 973.6153846153846\n","avg_envstep_per_sec: 1194.9306016454468\n","avg_train_sample_per_sec: 1194.9306016454468\n","avg_episode_per_sec: 1.2273127772292651\n","reward_mean: 247.22261997826888\n","reward_std: 101.66762124464698\n","reward_max: 302.2074389403607\n","reward_min: -20.79937879667453\n","total_envstep_count: 1300434\n","total_train_sample_count: 1300365\n","total_episode_count: 5663\n","INFO:learner_logger:[RANK0]: === Training Iteration 9900 Result ===\n","INFO:learner_logger:\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Name  | value_lossalpha_loss_avg | policy_loss_avg | critic_loss_avg | cur_lr_q_avg |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","| Value | 0.000000                 | -116.066595     | 13.231975       | 0.001000     |\n","+-------+--------------------------+-----------------+-----------------+--------------+\n","+-------+--------------+--------------------+-----------+--------------+\n","| Name  | cur_lr_p_avg | target_q_value_avg | alpha_avg | td_error_avg |\n","+-------+--------------+--------------------+-----------+--------------+\n","| Value | 0.001000     | 116.360733         | 0.069788  | 13.002864    |\n","+-------+--------------+--------------------+-----------+--------------+\n","+-------+--------------------------+----------------------+----------------+\n","| Name  | transformed_log_prob_avg | twin_critic_loss_avg | alpha_loss_avg |\n","+-------+--------------------------+----------------------+----------------+\n","| Value | -0.011409                | 12.773755            | -0.030360      |\n","+-------+--------------------------+----------------------+----------------+\n","\n","INFO:buffer_logger:In the past 60.1 seconds, buffer statistics is as follows:\n","INFO:buffer_logger:\n","+-------+--------------+--------------+----------+---------------+\n","| Name  | pushed_in    | sampled_out  | removed  | current_have  |\n","+-------+--------------+--------------+----------+---------------+\n","| Value | 19328.000000 | 19456.000000 | 0.000000 | 100000.000000 |\n","+-------+--------------+--------------+----------+---------------+\n","\n","\n","INFO:evaluator_logger:[EVALUATOR]env 2 finish episode, final reward: 306.4040, current episode: 1\n","INFO:evaluator_logger:[EVALUATOR]env 4 finish episode, final reward: 303.9179, current episode: 2\n","INFO:evaluator_logger:[EVALUATOR]env 1 finish episode, final reward: 304.8934, current episode: 3\n","INFO:evaluator_logger:[EVALUATOR]env 3 finish episode, final reward: 305.0522, current episode: 4\n","INFO:evaluator_logger:[EVALUATOR]env 0 finish episode, final reward: 305.2849, current episode: 5\n","INFO:evaluator_logger:\n","+-------+--------------+-------------------------+---------------+---------------+\n","| Name  | train_iter   | ckpt_name               | episode_count | envstep_count |\n","+-------+--------------+-------------------------+---------------+---------------+\n","| Value | 10000.000000 | iteration_10000.pth.tar | 5.000000      | 5274.000000   |\n","+-------+--------------+-------------------------+---------------+---------------+\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Name  | avg_envstep_per_episode | evaluate_time | avg_envstep_per_sec | avg_time_per_episode |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","| Value | 1054.800000             | 3.856196      | 1367.669012         | 1.296615             |\n","+-------+-------------------------+---------------+---------------------+----------------------+\n","+-------+-------------+------------+------------+------------+\n","| Name  | reward_mean | reward_std | reward_max | reward_min |\n","+-------+-------------+------------+------------+------------+\n","| Value | 305.110480  | 0.797039   | 306.403992 | 303.917908 |\n","+-------+-------------+------------+------------+------------+\n","+-------+---------------------------------------------------------------------------------------------------+--------------------------+\n","| Name  | eval_episode_return                                                                               | eval_episode_return_mean |\n","+-------+---------------------------------------------------------------------------------------------------+--------------------------+\n","| Value | [305.284912109375, 304.89337158203125, 306.40399169921875, 305.0522155761719, 303.91790771484375] | 305.110480               |\n","+-------+---------------------------------------------------------------------------------------------------+--------------------------+\n","\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar\n","INFO:evaluator_logger:[DI-engine serial pipeline] Current episode_return: 305.1105 is greater than stop_value: 300, so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details.\n","INFO:learner_logger:[RANK0]: learner save ckpt in ./bipedalwalker_sac_gail_seed0_240229_005622/ckpt/iteration_10000.pth.tar\n"]},{"output_type":"stream","name":"stdout","text":["look here\n","{'eval_episode_return': [305.284912109375, 304.89337158203125, 306.40399169921875, 305.0522155761719, 303.91790771484375], 'eval_episode_return_mean': 305.1104797363281}\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar\n","Saved reward model ckpt in bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_last.pth.tar\n"]},{"output_type":"execute_result","data":{"text/plain":["DI-engine DRL Policy\n","ContinuousQAC(\n","  (actor_encoder): Identity()\n","  (critic_encoder): Identity()\n","  (actor_head): Sequential(\n","    (0): Linear(in_features=24, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): ReparameterizationHead(\n","      (main): Sequential(\n","        (0): Linear(in_features=128, out_features=128, bias=True)\n","        (1): ReLU()\n","      )\n","      (mu): Linear(in_features=128, out_features=4, bias=True)\n","      (log_sigma_layer): Linear(in_features=128, out_features=4, bias=True)\n","    )\n","  )\n","  (critic_head): ModuleList(\n","    (0-1): 2 x Sequential(\n","      (0): Linear(in_features=28, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): RegressionHead(\n","        (main): Sequential(\n","          (0): Linear(in_features=128, out_features=128, bias=True)\n","          (1): ReLU()\n","        )\n","        (last): Linear(in_features=128, out_features=1, bias=True)\n","      )\n","    )\n","  )\n","  (actor): ModuleList(\n","    (0): Identity()\n","    (1): Sequential(\n","      (0): Linear(in_features=24, out_features=128, bias=True)\n","      (1): ReLU()\n","      (2): ReparameterizationHead(\n","        (main): Sequential(\n","          (0): Linear(in_features=128, out_features=128, bias=True)\n","          (1): ReLU()\n","        )\n","        (mu): Linear(in_features=128, out_features=4, bias=True)\n","        (log_sigma_layer): Linear(in_features=128, out_features=4, bias=True)\n","      )\n","    )\n","  )\n","  (critic): ModuleList(\n","    (0): Identity()\n","    (1): ModuleList(\n","      (0-1): 2 x Sequential(\n","        (0): Linear(in_features=28, out_features=128, bias=True)\n","        (1): ReLU()\n","        (2): RegressionHead(\n","          (main): Sequential(\n","            (0): Linear(in_features=128, out_features=128, bias=True)\n","            (1): ReLU()\n","          )\n","          (last): Linear(in_features=128, out_features=1, bias=True)\n","        )\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["!zip -r bipedalwalker_gail_sac_0229.zip bipedalwalker_sac_gail_seed0_240229_005622"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbPHF-hurf4s","executionInfo":{"status":"ok","timestamp":1709173590226,"user_tz":0,"elapsed":2721,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"26f45ee6-b8d4-4f49-9548-18f4fa33ff94"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["  adding: bipedalwalker_sac_gail_seed0_240229_005622/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/total_config.py (deflated 72%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/collector/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/collector/collector_logger.txt (deflated 79%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/buffer/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/buffer/buffer_logger.txt (deflated 97%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/buffer/buffer_tb_logger/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/buffer/buffer_tb_logger/events.out.tfevents.1709172398.61bfd91e8b4a (deflated 75%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/buffer/buffer_tb_logger/events.out.tfevents.1709168367.61bfd91e8b4a (deflated 78%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/learner/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/learner/learner_logger.txt (deflated 94%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/evaluator/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/evaluator/evaluator_logger.txt (deflated 91%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/serial/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/log/serial/events.out.tfevents.1709168306.61bfd91e8b4a (deflated 75%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_best.pth.tar (deflated 17%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/reward_model/ckpt/ckpt_last.pth.tar (deflated 17%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ (stored 0%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/ckpt/iteration_0.pth.tar (deflated 12%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/ckpt/ckpt_best.pth.tar (deflated 16%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/ckpt/iteration_10000.pth.tar (deflated 16%)\n","  adding: bipedalwalker_sac_gail_seed0_240229_005622/formatted_total_config.py (deflated 70%)\n"]}]},{"cell_type":"code","source":["# 需要练一个SAC的bipedalwalk模型出来！"],"metadata":{"id":"OF6cRRA6K-KF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 这是ppo的\n","from easydict import EasyDict\n","\n","bipedalwalker_ppo_config = dict(\n","    exp_name='bipedalwalker_ppo_seed0',\n","    env=dict(\n","        env_id='BipedalWalker-v3',\n","        collector_env_num=8,\n","        evaluator_env_num=5,\n","        # (bool) Scale output action into legal range.\n","        act_scale=True,\n","        n_evaluator_episode=5,\n","        stop_value=300,\n","        rew_clip=True,\n","        # The path to save the game replay\n","        # replay_path='./bipedalwalker_ppo_seed0/video',\n","    ),\n","    policy=dict(\n","        cuda=False,\n","        load_path=\"./bipedalwalker_ppo_seed0/ckpt/ckpt_best.pth.tar\",\n","        action_space='continuous',\n","        model=dict(\n","            action_space='continuous',\n","            obs_shape=24,\n","            action_shape=4,\n","        ),\n","        learn=dict(\n","            epoch_per_collect=10,\n","            batch_size=64,\n","            learning_rate=0.001,\n","            value_weight=0.5,\n","            entropy_weight=0.01,\n","            clip_ratio=0.2,\n","            adv_norm=True,\n","        ),\n","        collect=dict(\n","            n_sample=2048,\n","            unroll_len=1,\n","            discount_factor=0.99,\n","            gae_lambda=0.95,\n","        ),\n","    ),\n",")\n","bipedalwalker_ppo_config = EasyDict(bipedalwalker_ppo_config)\n","#main_config = bipedalwalker_ppo_config\n","bipedalwalker_ppo_create_config = dict(\n","    env=dict(\n","        type='bipedalwalker',\n","        import_names=['dizoo.box2d.bipedalwalker.envs.bipedalwalker_env'],\n","    ),\n","    env_manager=dict(type='subprocess'),\n","    policy=dict(type='ppo'),\n",")\n","bipedalwalker_ppo_create_config = EasyDict(bipedalwalker_ppo_create_config)\n","#create_config = bipedalwalker_ppo_create_config"],"metadata":{"id":"6PEdoUhllP5l","executionInfo":{"status":"ok","timestamp":1709157142077,"user_tz":0,"elapsed":312,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":["#from dizoo.box2d.bipedalwalker.config import bipedalwalker_ppo_config , bipedalwalker_ppo_create_config\n","from ding.config import compile_config\n","cfg = compile_config(bipedalwalker_ppo_config, create_cfg=bipedalwalker_ppo_create_config, auto=True)"],"metadata":{"id":"vxLbnbtNbdjD","executionInfo":{"status":"ok","timestamp":1709155112876,"user_tz":0,"elapsed":2462,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["from ding.envs import DingEnvWrapper, BaseEnvManagerV2\n","\n","collector_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"BipedalWalker-v3\")) for _ in range(cfg.env.collector_env_num)],\n","    cfg=cfg.env.manager\n",")\n","evaluator_env = BaseEnvManagerV2(\n","    env_fn=[lambda: DingEnvWrapper(gym.make(\"BipedalWalker-v3\")) for _ in range(cfg.env.evaluator_env_num)],\n","    cfg=cfg.env.manager\n",")"],"metadata":{"id":"Mcjqr-xYbdeG","executionInfo":{"status":"ok","timestamp":1709155130574,"user_tz":0,"elapsed":226,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# from ding.model import DQN\n","from ding.policy import PPOPolicy\n","from ding.data import DequeBuffer\n","\n","#model = SAC(**cfg.policy.model)\n","buffer_ = DequeBuffer(size=cfg.policy.other.replay_buffer.replay_buffer_size)\n","policy = PPOPolicy(cfg.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdCJbd1kbdT3","executionInfo":{"status":"ok","timestamp":1709155228170,"user_tz":0,"elapsed":253,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"47863f39-f260-4c7a-dd29-3e6dacfb6ae8"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["\n","!mkdir /content/bipedalwalker_sac_config0/ckpt\n"],"metadata":{"id":"d4yVVC9FgTjs","executionInfo":{"status":"ok","timestamp":1709153951920,"user_tz":0,"elapsed":240,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["from ding.framework import task\n","from ding.framework.context import OnlineRLContext\n","from ding.framework.middleware import OffPolicyLearner, StepCollector, interaction_evaluator, data_pusher, eps_greedy_handler, CkptSaver\n","\n","import logging\n","logging.getLogger().setLevel(logging.INFO)\n","\n","with task.start(async_mode=False, ctx=OnlineRLContext()):\n","    # Evaluating, we place it on the first place to get the score of the random model as a benchmark value\n","    task.use(interaction_evaluator(cfg, policy.eval_mode, evaluator_env))\n","    #task.use(eps_greedy_handler(cfg))  # Decay probability of explore-exploit\n","    task.use(StepCollector(cfg, policy.collect_mode, collector_env))  # Collect environmental data\n","    task.use(data_pusher(cfg, buffer_))  # Push data to buffer\n","    task.use(OffPolicyLearner(cfg, policy.learn_mode, buffer_))  # Train the model\n","    task.use(CkptSaver(policy, cfg.exp_name, train_freq=100))  # Save the model\n","    # In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\n","    task.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"id":"5k7iyqxfca5W","executionInfo":{"status":"error","timestamp":1709155237666,"user_tz":0,"elapsed":2450,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"e39c2660-8b62-446a-bc93-64e093c06ef1"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","INFO:root:Evaluation: Train Iter(0) Env Step(0) Episode Return(-91.949) \n"]},{"output_type":"error","ename":"AttributeError","evalue":"'EasyDict' object has no attribute 'update_per_collect'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-00f6257b2c46>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCkptSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# In the evaluation process, if the model is found to have exceeded the convergence value, it will end early here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, max_step)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_middleware\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;31m# Sync should be called before backward, otherwise it is possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# that some generators have not been pushed to backward_stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mruntime_handler\u001b[0;34m(task, async_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mruntime_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fn, ctx)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx)\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mruntime_handler\u001b[0;34m(task, async_mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mruntime_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/task.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fn, ctx)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ding/framework/middleware/learner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_output_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_per_collect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'update_per_collect'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"av2GLzXkcbG4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SAC agent"],"metadata":{"id":"7gFLKflTmRcb"}},{"cell_type":"code","source":["!git clone https://github.com/opendilab/huggingface_ding.git\n","!pip3 install -e ./huggingface_ding/"],"metadata":{"id":"Z7f5YPo5Bee1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://huggingface.co/OpenDILabCommunity/BipedalWalker-v3-SAC"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5MMuG0eYlpT","executionInfo":{"status":"ok","timestamp":1709152044518,"user_tz":0,"elapsed":1965,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"5e373994-e7ce-45ef-cbb3-91600c6b0474"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Cloning into 'BipedalWalker-v3-SAC'...\n","remote: Enumerating objects: 28, done.\u001b[K\n","remote: Total 28 (delta 0), reused 0 (delta 0), pack-reused 28\u001b[K\n","Unpacking objects: 100% (28/28), 267.23 KiB | 4.69 MiB/s, done.\n"]}]},{"cell_type":"code","source":["from ding.bonus import SACAgent\n","from ding.config import Config\n","from easydict import EasyDict\n","import torch\n","\n","# Pull model from Hugggingface hub\n","policy_state_dict = torch.load(\"/content/BipedalWalker-v3-SAC/pytorch_model.bin\", map_location=torch.device(\"cpu\"))\n","cfg = EasyDict(Config.file_to_dict(\"/content/BipedalWalker-v3-SAC/policy_config.py\").cfg_dict)\n","\n","# Instantiate the agent\n","agent = SACAgent(\n","    env_id=\"BipedalWalker-v3\",\n","    exp_name=\"BipedalWalker-v3-SAC\",\n","    cfg=cfg.exp_config,\n","    policy_state_dict=policy_state_dict\n",")\n","# Continue training\n","agent.train(step=5000)\n","# Render the new agent performance\n","#agent.deploy(enable_save_replay=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["bde63b395e214fd3be471faea7427b33","462e36fb06bd4544ad82a5b201d64142","c884797cbd0840fabc6b6e7409f25c44","281ba6c596894037aee7301b6ceb693b","7208ca175e0b448dad3b7f8ca68f62f3","040ea7f7b5204a5fba87f1c6b2f9999e","139cef0b0cd74470bbdb762a940cb18d","df3c108ceb824d82b88e0bad160e193c"]},"id":"oFz-WfRoYzsZ","executionInfo":{"status":"ok","timestamp":1709152199097,"user_tz":0,"elapsed":32280,"user":{"displayName":"Haochen Liu","userId":"03082085296391564384"}},"outputId":"d94ce1bd-89a2-4d0a-d846-1bcd72fe956c"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:htuj1689) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde63b395e214fd3be471faea7427b33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>adv_max</td><td>▁▄▆▅█</td></tr><tr><td>adv_mean</td><td>▃▁█▇▇</td></tr><tr><td>approx_kl</td><td>▆▅█▁▂</td></tr><tr><td>clipfrac</td><td>█▅█▁▃</td></tr><tr><td>cur_lr</td><td>▁▁▁▁▁</td></tr><tr><td>entropy_loss</td><td>▁▂▅██</td></tr><tr><td>env step</td><td>▁</td></tr><tr><td>episode return mean</td><td>▁</td></tr><tr><td>episode return std</td><td>▁</td></tr><tr><td>policy_loss</td><td>█▅▇▁▃</td></tr><tr><td>train iter</td><td>▁</td></tr><tr><td>value_loss</td><td>█▅▁▁▁</td></tr><tr><td>value_max</td><td>█▁▄▇▆</td></tr><tr><td>value_mean</td><td>▆▁▃█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>adv_max</td><td>2.99238</td></tr><tr><td>adv_mean</td><td>0.0</td></tr><tr><td>approx_kl</td><td>0.40581</td></tr><tr><td>clipfrac</td><td>0.72402</td></tr><tr><td>cur_lr</td><td>0.001</td></tr><tr><td>entropy_loss</td><td>0.54375</td></tr><tr><td>env step</td><td>1027</td></tr><tr><td>episode return mean</td><td>191.83281</td></tr><tr><td>episode return std</td><td>168.70515</td></tr><tr><td>policy_loss</td><td>-0.02645</td></tr><tr><td>train iter</td><td>160</td></tr><tr><td>value_loss</td><td>0.21358</td></tr><tr><td>value_max</td><td>6.55879</td></tr><tr><td>value_mean</td><td>5.61543</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">gallant-jazz-1</strong> at: <a href='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-PPO/runs/htuj1689?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd' target=\"_blank\">https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-PPO/runs/htuj1689?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240228_202529-htuj1689/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:htuj1689). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240228_202934-hzlsjuqh</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927/runs/hzlsjuqh?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd' target=\"_blank\">vocal-dragon-1</a></strong> to <a href='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd' target=\"_blank\">https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927/runs/hzlsjuqh?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd' target=\"_blank\">https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927/runs/hzlsjuqh?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Do NOT share these links with anyone. They can be used to claim your runs."]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","INFO:root:Evaluation: Train Iter(0) Env Step(0) Episode Return(315.905) \n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:268: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","INFO:root:Training: Train Iter(0)\tEnv Step(10007)\tLoss(-28.857)\n","INFO:root:Exceeded maximum number of env_step(10007), program is terminated\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainingReturn(wandb_url='https://wandb.ai/anony-mouse-586513020729122229/BipedalWalker-v3-SAC_240228_202927?apiKey=6b60cd7681f9349f2cc972ab9211dc5aab0e7bfd')"]},"metadata":{},"execution_count":52}]}]}